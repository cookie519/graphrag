2025/02/18 03:45:30 routes.go:1259: INFO server config env="map[CUDA_VISIBLE_DEVICES:0,1 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/scratch/gpfs/jx0800/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-02-18T03:45:30.836-05:00 level=INFO source=images.go:757 msg="total blobs: 21"
time=2025-02-18T03:45:30.836-05:00 level=INFO source=images.go:764 msg="total unused blobs removed: 0"
[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.

[GIN-debug] [WARNING] Running in "debug" mode. Switch to "release" mode in production.
 - using env:	export GIN_MODE=release
 - using code:	gin.SetMode(gin.ReleaseMode)

[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)
[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)
[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)
[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)
[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)
[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)
[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)
[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)
[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)
[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)
[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)
[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)
[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)
[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)
[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)
[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)
[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)
[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)
[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)
[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)
[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)
[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)
[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)
[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)
time=2025-02-18T03:45:30.837-05:00 level=INFO source=routes.go:1310 msg="Listening on 127.0.0.1:11434 (version 0.5.4)"
time=2025-02-18T03:45:30.840-05:00 level=INFO source=routes.go:1339 msg="Dynamic LLM libraries" runners="[cpu_avx2 cuda_v11_avx cuda_v12_avx rocm_avx cpu cpu_avx]"
time=2025-02-18T03:45:30.840-05:00 level=INFO source=gpu.go:226 msg="looking for compatible GPUs"
time=2025-02-18T03:45:31.856-05:00 level=INFO source=types.go:131 msg="inference compute" id=GPU-63a9e174-72a6-45d3-2492-80fdda2549b9 library=cuda variant=v12 compute=8.0 driver=12.8 name="NVIDIA A100-SXM4-80GB" total="79.3 GiB" available="78.8 GiB"
time=2025-02-18T03:45:31.856-05:00 level=INFO source=types.go:131 msg="inference compute" id=GPU-6d0c49ae-6fd7-9f12-0b4b-296f29a8e8e0 library=cuda variant=v12 compute=8.0 driver=12.8 name="NVIDIA A100-SXM4-80GB" total="79.3 GiB" available="78.8 GiB"

config_path: /scratch/gpfs/jx0800/data/graphrag/settings.yaml
Logging enabled at /scratch/gpfs/jx0800/data/graphrag/logs/indexing-engine.log
Running standard indexing.
üöÄ create_base_text_units
                                                  id  ... n_tokens
0  e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a303...  ...      701
1  826236e9b53b61f612c6be568d84689b52b02eaab67617...  ...      849
2  bcfc6a23d78c876f7cc226f911e6ce8375526d89093e88...  ...      926
3  332527ce3f7779776fcfae343ec912a32806bb50e27d75...  ...      819

[4 rows x 4 columns]
üöÄ create_final_documents
                                                  id  ...                       
text_unit_ids
0  60fc9c195dd446cea71026da21b6a6a4741d6bba079baa...  ...  
[e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...
1  9e68811b4cd4f15f8ecc47c135b57630624ac5217352bc...  ...  
[bcfc6a23d78c876f7cc226f911e6ce8375526d89093e8...
2  deffe383fea4e5e43d969b23593899dfe4ee8f4533fc02...  ...  
[332527ce3f7779776fcfae343ec912a32806bb50e27d7...
3  88eb9b650c09283b91b10a3bc047224790471845d07fff...  ...  
[826236e9b53b61f612c6be568d84689b52b02eaab6761...

[4 rows x 5 columns]
llm_config: api_key='eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExMjIwNjUyMzA1MjU3MzA4NzcxMSIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTg5NTczNDQxMywidXVpZCI6IjNmMGNjOTczLTI2ODEtNDY3Yi04ZjJiLWNhZDFlZmE2MThjMCIsIm5hbWUiOiJ0ZXN0IiwiZXhwaXJlc19hdCI6IjIwMzAtMDEtMjdUMDg6NTM6MzMrMDAwMCJ9.G6SMzeru88oakNW_MmeQIlWy6WviW1TE_vcfeVgUmHw' auth_type=<AuthType.APIKey: 'api_key'> type="openai_chat" model='llama3.3' encoding_model='cl100k_base' max_tokens=20000 temperature=0.6 top_p=0.9 n=1 frequency_penalty=0.0 presence_penalty=0.0 request_timeout=180.0 api_base='http://localhost:11434/v1' api_version=None deployment_name=None organization=None proxy=None audience=None model_supports_json=True tokens_per_minute=50000 requests_per_minute=1000 max_retries=10 max_retry_wait=10.0 sleep_on_rate_limit_recommendation=True concurrent_requests=25 responses=None parallelization_stagger=0.3 parallelization_num_threads=50 async_mode=<AsyncType.Threaded: 'threaded'>
configuration: max_retries=10 max_json_retries=3 max_retry_wait=10.0 max_concurrency=25 tokens_per_minute=50000 requests_per_minute=1000 requests_burst_mode=True json_strategy=<JsonStrategy.VALID: 'valid'> azure=False api_key='eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExMjIwNjUyMzA1MjU3MzA4NzcxMSIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTg5NTczNDQxMywidXVpZCI6IjNmMGNjOTczLTI2ODEtNDY3Yi04ZjJiLWNhZDFlZmE2MThjMCIsIm5hbWUiOiJ0ZXN0IiwiZXhwaXJlc19hdCI6IjIwMzAtMDEtMjdUMDg6NTM6MzMrMDAwMCJ9.G6SMzeru88oakNW_MmeQIlWy6WviW1TE_vcfeVgUmHw' track_stream_usage=False organization=None timeout=180.0 model='llama3.3' encoding='cl100k_base' chat_parameters={'frequency_penalty': 0.0, 'max_tokens': 20000, 'n': 1, 'presence_penalty': 0.0, 'temperature': 0.6, 'top_p': 0.9} embeddings_parameters={} sleep_on_rate_limit_recommendation=False base_url='http://localhost:11434/v1'
client: <openai.AsyncOpenAI object at 0x14a8d3ead0d0>
cache: <graphrag.index.llm.load_llm.GraphRagLLMCache object at 0x14a8d143f210>
events: <graphrag.index.llm.load_llm.GraphRagLLMEvents object at 0x14a8d437b510>
llm_config: api_key='eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExMjIwNjUyMzA1MjU3MzA4NzcxMSIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTg5NTczNDQxMywidXVpZCI6IjNmMGNjOTczLTI2ODEtNDY3Yi04ZjJiLWNhZDFlZmE2MThjMCIsIm5hbWUiOiJ0ZXN0IiwiZXhwaXJlc19hdCI6IjIwMzAtMDEtMjdUMDg6NTM6MzMrMDAwMCJ9.G6SMzeru88oakNW_MmeQIlWy6WviW1TE_vcfeVgUmHw' auth_type=<AuthType.APIKey: 'api_key'> type="openai_chat" model='llama3.3' encoding_model='cl100k_base' max_tokens=20000 temperature=0.6 top_p=0.9 n=1 frequency_penalty=0.0 presence_penalty=0.0 request_timeout=180.0 api_base='http://localhost:11434/v1' api_version=None deployment_name=None organization=None proxy=None audience=None model_supports_json=True tokens_per_minute=50000 requests_per_minute=1000 max_retries=10 max_retry_wait=10.0 sleep_on_rate_limit_recommendation=True concurrent_requests=25 responses=None parallelization_stagger=0.3 parallelization_num_threads=50 async_mode=<AsyncType.Threaded: 'threaded'>
llm_config: api_key='eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExMjIwNjUyMzA1MjU3MzA4NzcxMSIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTg5NTczNDQxMywidXVpZCI6IjNmMGNjOTczLTI2ODEtNDY3Yi04ZjJiLWNhZDFlZmE2MThjMCIsIm5hbWUiOiJ0ZXN0IiwiZXhwaXJlc19hdCI6IjIwMzAtMDEtMjdUMDg6NTM6MzMrMDAwMCJ9.G6SMzeru88oakNW_MmeQIlWy6WviW1TE_vcfeVgUmHw' auth_type=<AuthType.APIKey: 'api_key'> type="openai_chat" model='llama3.3' encoding_model='cl100k_base' max_tokens=20000 temperature=0.6 top_p=0.9 n=1 frequency_penalty=0.0 presence_penalty=0.0 request_timeout=180.0 api_base='http://localhost:11434/v1' api_version=None deployment_name=None organization=None proxy=None audience=None model_supports_json=True tokens_per_minute=50000 requests_per_minute=1000 max_retries=10 max_retry_wait=10.0 sleep_on_rate_limit_recommendation=True concurrent_requests=25 responses=None parallelization_stagger=0.3 parallelization_num_threads=50 async_mode=<AsyncType.Threaded: 'threaded'>
llm_config: api_key='eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExMjIwNjUyMzA1MjU3MzA4NzcxMSIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTg5NTczNDQxMywidXVpZCI6IjNmMGNjOTczLTI2ODEtNDY3Yi04ZjJiLWNhZDFlZmE2MThjMCIsIm5hbWUiOiJ0ZXN0IiwiZXhwaXJlc19hdCI6IjIwMzAtMDEtMjdUMDg6NTM6MzMrMDAwMCJ9.G6SMzeru88oakNW_MmeQIlWy6WviW1TE_vcfeVgUmHw' auth_type=<AuthType.APIKey: 'api_key'> type="openai_chat" model='llama3.3' encoding_model='cl100k_base' max_tokens=20000 temperature=0.6 top_p=0.9 n=1 frequency_penalty=0.0 presence_penalty=0.0 request_timeout=180.0 api_base='http://localhost:11434/v1' api_version=None deployment_name=None organization=None proxy=None audience=None model_supports_json=True tokens_per_minute=50000 requests_per_minute=1000 max_retries=10 max_retry_wait=10.0 sleep_on_rate_limit_recommendation=True concurrent_requests=25 responses=None parallelization_stagger=0.3 parallelization_num_threads=50 async_mode=<AsyncType.Threaded: 'threaded'>
graph: Graph with 0 nodes and 0 edges
graph: Graph with 0 nodes and 0 edges
graph: Graph with 0 nodes and 0 edges
graph: Graph with 0 nodes and 0 edges
‚ùå extract_graph
None
‚†ã GraphRAG Indexer 
‚îú‚îÄ‚îÄ Loading Input (InputFileType.text) - 4 files loaded (0 filtered) ‚îÅ 100% ‚Ä¶ 0‚Ä¶
‚îú‚îÄ‚îÄ create_base_text_units ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100% 0:00:00 0:00:00
‚îú‚îÄ‚îÄ create_final_documents ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100% 0:00:00 0:00:00‚ùå Errors occurred during the pipeline run, see logs for more details.
