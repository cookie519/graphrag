INFO 03-17 23:12:20 __init__.py:190] Automatically detected platform cuda.
INFO:__main__:batch size: 1000
INFO:__main__:num: 400
INFO:__main__:start_idx: 3200000
INFO:__main__:stop_idx: 3429515
INFO:__main__:Loading LLM
INFO 03-17 23:12:35 config.py:542] This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 03-17 23:12:35 config.py:1401] Defaulting to use mp for distributed inference
WARNING 03-17 23:12:35 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 03-17 23:12:35 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 03-17 23:12:35 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='/scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 03-17 23:12:35 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:12:36 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
/home/jx0800/.conda/envs/graphrag/lib/python3.11/site-packages/vllm/executor/mp_distributed_executor.py:110: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 03-17 23:12:37 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:12:37 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:12:40 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:12:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-17 23:12:40 utils.py:950] Found nccl from library libnccl.so.2
INFO 03-17 23:12:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-17 23:12:43 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/jx0800/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:12:43 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/jx0800/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 03-17 23:12:43 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_e38a4090'), local_subscribe_port=45123, remote_subscribe_port=None)
INFO 03-17 23:12:44 model_runner.py:1110] Starting to load model /scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct...
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:12:44 model_runner.py:1110] Starting to load model /scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:17,  5.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:10<00:10,  5.07s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.48s/it]
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:13:00 model_runner.py:1115] Loading model weights took 7.5123 GB
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  3.78s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.05s/it]

INFO 03-17 23:13:00 model_runner.py:1115] Loading model weights took 7.5123 GB
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:13:09 worker.py:267] Memory profiling takes 8.70 seconds
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:13:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:13:09 worker.py:267] model weights take 7.51GiB; non_torch_memory takes 1.58GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 61.94GiB.
INFO 03-17 23:13:09 worker.py:267] Memory profiling takes 9.01 seconds
INFO 03-17 23:13:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
INFO 03-17 23:13:09 worker.py:267] model weights take 7.51GiB; non_torch_memory takes 1.83GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 60.72GiB.
INFO 03-17 23:13:10 executor_base.py:110] # CUDA blocks: 62178, # CPU blocks: 4096
INFO 03-17 23:13:10 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 7.59x
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:13:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-17 23:13:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|â–Ž         | 1/35 [00:00<00:28,  1.19it/s]Capturing CUDA graph shapes:   6%|â–Œ         | 2/35 [00:01<00:26,  1.24it/s]Capturing CUDA graph shapes:   9%|â–Š         | 3/35 [00:02<00:24,  1.32it/s]Capturing CUDA graph shapes:  11%|â–ˆâ–        | 4/35 [00:03<00:23,  1.33it/s]Capturing CUDA graph shapes:  14%|â–ˆâ–        | 5/35 [00:03<00:22,  1.35it/s]Capturing CUDA graph shapes:  17%|â–ˆâ–‹        | 6/35 [00:04<00:21,  1.34it/s]Capturing CUDA graph shapes:  20%|â–ˆâ–ˆ        | 7/35 [00:05<00:20,  1.33it/s]Capturing CUDA graph shapes:  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:06<00:20,  1.34it/s]Capturing CUDA graph shapes:  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:06<00:19,  1.34it/s]Capturing CUDA graph shapes:  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:07<00:18,  1.34it/s]Capturing CUDA graph shapes:  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:08<00:17,  1.34it/s]Capturing CUDA graph shapes:  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:09<00:17,  1.35it/s]Capturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:09<00:16,  1.33it/s]Capturing CUDA graph shapes:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:10<00:15,  1.34it/s]Capturing CUDA graph shapes:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:11<00:14,  1.35it/s]Capturing CUDA graph shapes:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:11<00:14,  1.36it/s]Capturing CUDA graph shapes:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:12<00:13,  1.35it/s]Capturing CUDA graph shapes:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:13<00:12,  1.34it/s]Capturing CUDA graph shapes:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:14<00:12,  1.32it/s]Capturing CUDA graph shapes:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:15<00:11,  1.30it/s]Capturing CUDA graph shapes:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:15<00:10,  1.33it/s]Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:16<00:09,  1.34it/s]Capturing CUDA graph shapes:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:17<00:09,  1.33it/s]Capturing CUDA graph shapes:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:18<00:08,  1.34it/s]Capturing CUDA graph shapes:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:18<00:07,  1.34it/s]Capturing CUDA graph shapes:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:19<00:06,  1.34it/s]Capturing CUDA graph shapes:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:20<00:05,  1.35it/s]Capturing CUDA graph shapes:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:20<00:05,  1.34it/s]Capturing CUDA graph shapes:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:21<00:04,  1.32it/s]Capturing CUDA graph shapes:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:22<00:03,  1.31it/s]Capturing CUDA graph shapes:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:23<00:03,  1.31it/s]Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:24<00:02,  1.31it/s]Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:24<00:01,  1.33it/s]Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:25<00:00,  1.34it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:26<00:00,  1.10it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:26<00:00,  1.30it/s]
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:13:40 custom_all_reduce.py:226] Registering 2275 cuda graph addresses
INFO 03-17 23:13:40 custom_all_reduce.py:226] Registering 2275 cuda graph addresses
INFO 03-17 23:13:40 model_runner.py:1562] Graph capturing finished in 27 secs, took 0.25 GiB
[1;36m(VllmWorkerProcess pid=1248024)[0;0m INFO 03-17 23:13:40 model_runner.py:1562] Graph capturing finished in 27 secs, took 0.25 GiB
INFO 03-17 23:13:40 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 39.66 seconds
INFO:__main__:Processing batch to idx 999:
INFO 03-17 23:13:40 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO:__main__:Processing batch to idx 1999:
INFO:__main__:Processing batch to idx 2999:
INFO:__main__:Processing batch to idx 3999:
INFO:__main__:Processing batch to idx 4999:
INFO:__main__:Processing batch to idx 5999:
INFO:__main__:Processing batch to idx 6999:
INFO:__main__:Processing batch to idx 7999:
INFO:__main__:Processing batch to idx 8999:
INFO:__main__:Processing batch to idx 9999:
INFO:__main__:Processing batch to idx 10999:
INFO:__main__:Processing batch to idx 11999:
INFO:__main__:Processing batch to idx 12999:
INFO:__main__:Processing batch to idx 13999:
INFO:__main__:Processing batch to idx 14999:
INFO:__main__:Processing batch to idx 15999:
INFO:__main__:Processing batch to idx 16999:
INFO:__main__:Processing batch to idx 17999:
INFO:__main__:Processing batch to idx 18999:
INFO:__main__:Processing batch to idx 19999:
INFO:__main__:Processing batch to idx 20999:
INFO:__main__:Processing batch to idx 21999:
INFO:__main__:Processing batch to idx 22999:
INFO:__main__:Processing batch to idx 23999:
INFO:__main__:Processing batch to idx 24999:
INFO:__main__:Processing batch to idx 25999:
INFO:__main__:Processing batch to idx 26999:
INFO:__main__:Processing batch to idx 27999:
INFO:__main__:Processing batch to idx 28999:
INFO:__main__:Processing batch to idx 29999:
INFO:__main__:Processing batch to idx 30999:
INFO:__main__:Processing batch to idx 31999:
INFO:__main__:Processing batch to idx 32999:
INFO:__main__:Processing batch to idx 33999:
INFO:__main__:Processing batch to idx 34999:
INFO:__main__:Processing batch to idx 35999:
INFO:__main__:Processing batch to idx 36999:
INFO:__main__:Processing batch to idx 37999:
INFO:__main__:Processing batch to idx 38999:
INFO:__main__:Processing batch to idx 39999:
INFO:__main__:Processing batch to idx 40999:
INFO:__main__:Processing batch to idx 41999:
INFO:__main__:Processing batch to idx 42999:
INFO:__main__:Processing batch to idx 43999:
INFO:__main__:Processing batch to idx 44999:
INFO:__main__:Processing batch to idx 45999:
INFO:__main__:Processing batch to idx 46999:
INFO:__main__:Processing batch to idx 47999:
INFO:__main__:Processing batch to idx 48999:
INFO:__main__:Processing batch to idx 49999:
INFO:__main__:Processing batch to idx 50999:
INFO:__main__:Processing batch to idx 51999:
INFO:__main__:Processing batch to idx 52999:
INFO:__main__:Processing batch to idx 53999:
INFO:__main__:Processing batch to idx 54999:
INFO:__main__:Processing batch to idx 55999:
INFO:__main__:Processing batch to idx 56999:
INFO:__main__:Processing batch to idx 57999:
INFO:__main__:Processing batch to idx 58999:
INFO:__main__:Processing batch to idx 59999:
INFO:__main__:Processing batch to idx 60999:
INFO:__main__:Processing batch to idx 61999:
INFO:__main__:Processing batch to idx 62999:
INFO:__main__:Processing batch to idx 63999:
INFO:__main__:Processing batch to idx 64999:
INFO:__main__:Processing batch to idx 65999:
INFO:__main__:Processing batch to idx 66999:
INFO:__main__:Processing batch to idx 67999:
INFO:__main__:Processing batch to idx 68999:
INFO:__main__:Processing batch to idx 69999:
INFO:__main__:Processing batch to idx 70999:
INFO:__main__:Processing batch to idx 71999:
INFO:__main__:Processing batch to idx 72999:
INFO:__main__:Processing batch to idx 73999:
INFO:__main__:Processing batch to idx 74999:
INFO:__main__:Processing batch to idx 75999:
INFO:__main__:Processing batch to idx 76999:
INFO:__main__:Processing batch to idx 77999:
INFO:__main__:Processing batch to idx 78999:
INFO:__main__:Processing batch to idx 79999:
INFO:__main__:Processing batch to idx 80999:
INFO:__main__:Processing batch to idx 81999:
INFO:__main__:Processing batch to idx 82999:
INFO:__main__:Processing batch to idx 83999:
INFO:__main__:Processing batch to idx 84999:
INFO:__main__:Processing batch to idx 85999:
INFO:__main__:Processing batch to idx 86999:
INFO:__main__:Processing batch to idx 87999:
INFO:__main__:Processing batch to idx 88999:
INFO:__main__:Processing batch to idx 89999:
INFO:__main__:Processing batch to idx 90999:
INFO:__main__:Processing batch to idx 91999:
INFO:__main__:Processing batch to idx 92999:
INFO:__main__:Processing batch to idx 93999:
INFO:__main__:Processing batch to idx 94999:
INFO:__main__:Processing batch to idx 95999:
INFO:__main__:Processing batch to idx 96999:
INFO:__main__:Processing batch to idx 97999:
INFO:__main__:Processing batch to idx 98999:
INFO:__main__:Processing batch to idx 99999:
INFO:__main__:Processing batch to idx 100999:
INFO:__main__:Processing batch to idx 101999:
INFO:__main__:Processing batch to idx 102999:
INFO:__main__:Processing batch to idx 103999:
INFO:__main__:Processing batch to idx 104999:
INFO:__main__:Processing batch to idx 105999:
INFO:__main__:Processing batch to idx 106999:
INFO:__main__:Processing batch to idx 107999:
INFO:__main__:Processing batch to idx 108999:
INFO:__main__:Processing batch to idx 109999:
INFO:__main__:Processing batch to idx 110999:
INFO:__main__:Processing batch to idx 111999:
INFO:__main__:Processing batch to idx 112999:
INFO:__main__:Processing batch to idx 113999:
INFO:__main__:Processing batch to idx 114999:
INFO:__main__:Processing batch to idx 115999:
INFO:__main__:Processing batch to idx 116999:
INFO:__main__:Processing batch to idx 117999:
INFO:__main__:Processing batch to idx 118999:
INFO:__main__:Processing batch to idx 119999:
INFO:__main__:Processing batch to idx 120999:
INFO:__main__:Processing batch to idx 121999:
INFO:__main__:Processing batch to idx 122999:
INFO:__main__:Processing batch to idx 123999:
INFO:__main__:Processing batch to idx 124999:
INFO:__main__:Processing batch to idx 125999:
INFO:__main__:Processing batch to idx 126999:
INFO:__main__:Processing batch to idx 127999:
INFO:__main__:Processing batch to idx 128999:
INFO:__main__:Processing batch to idx 129999:
INFO:__main__:Processing batch to idx 130999:
INFO:__main__:Processing batch to idx 131999:
INFO:__main__:Processing batch to idx 132999:
INFO:__main__:Processing batch to idx 133999:
INFO:__main__:Processing batch to idx 134999:
INFO:__main__:Processing batch to idx 135999:
INFO:__main__:Processing batch to idx 136999:
INFO:__main__:Processing batch to idx 137999:
INFO:__main__:Processing batch to idx 138999:
INFO:__main__:Processing batch to idx 139999:
INFO:__main__:Processing batch to idx 140999:
INFO:__main__:Processing batch to idx 141999:
INFO:__main__:Processing batch to idx 142999:
INFO:__main__:Processing batch to idx 143999:
INFO:__main__:Processing batch to idx 144999:
INFO:__main__:Processing batch to idx 145999:
INFO:__main__:Processing batch to idx 146999:
INFO:__main__:Processing batch to idx 147999:
INFO:__main__:Processing batch to idx 148999:
INFO:__main__:Processing batch to idx 149999:
INFO:__main__:Processing batch to idx 150999:
INFO:__main__:Processing batch to idx 151999:
INFO:__main__:Processing batch to idx 152999:
INFO:__main__:Processing batch to idx 153999:
INFO:__main__:Processing batch to idx 154999:
INFO:__main__:Processing batch to idx 155999:
INFO:__main__:Processing batch to idx 156999:
INFO:__main__:Processing batch to idx 157999:
INFO:__main__:Processing batch to idx 158999:
INFO:__main__:Processing batch to idx 159999:
INFO:__main__:Processing batch to idx 160999:
INFO:__main__:Processing batch to idx 161999:
INFO:__main__:Processing batch to idx 162999:
INFO:__main__:Processing batch to idx 163999:
INFO:__main__:Processing batch to idx 164999:
INFO:__main__:Processing batch to idx 165999:
INFO:__main__:Processing batch to idx 166999:
INFO:__main__:Processing batch to idx 167999:
INFO:__main__:Processing batch to idx 168999:
INFO:__main__:Processing batch to idx 169999:
INFO:__main__:Processing batch to idx 170999:
INFO:__main__:Processing batch to idx 171999:
INFO:__main__:Processing batch to idx 172999:
INFO:__main__:Processing batch to idx 173999:
INFO:__main__:Processing batch to idx 174999:
INFO:__main__:Processing batch to idx 175999:
INFO:__main__:Processing batch to idx 176999:
INFO:__main__:Processing batch to idx 177999:
INFO:__main__:Processing batch to idx 178999:
INFO:__main__:Processing batch to idx 179999:
INFO:__main__:Processing batch to idx 180999:
INFO:__main__:Processing batch to idx 181999:
INFO:__main__:Processing batch to idx 182999:
INFO:__main__:Processing batch to idx 183999:
INFO:__main__:Processing batch to idx 184999:
INFO:__main__:Processing batch to idx 185999:
INFO:__main__:Processing batch to idx 186999:
INFO:__main__:Processing batch to idx 187999:
INFO:__main__:Processing batch to idx 188999:
INFO:__main__:Processing batch to idx 189999:
INFO:__main__:Processing batch to idx 190999:
INFO:__main__:Processing batch to idx 191999:
INFO:__main__:Processing batch to idx 192999:
INFO:__main__:Processing batch to idx 193999:
INFO:__main__:Processing batch to idx 194999:
INFO:__main__:Processing batch to idx 195999:
INFO:__main__:Processing batch to idx 196999:
INFO:__main__:Processing batch to idx 197999:
INFO:__main__:Processing batch to idx 198999:
INFO:__main__:Processing batch to idx 199999:
INFO:__main__:Processing batch to idx 200999:
INFO:__main__:Processing batch to idx 201999:
INFO:__main__:Processing batch to idx 202999:
INFO:__main__:Processing batch to idx 203999:
INFO:__main__:Processing batch to idx 204999:
INFO:__main__:Processing batch to idx 205999:
INFO:__main__:Processing batch to idx 206999:
INFO:__main__:Processing batch to idx 207999:
INFO:__main__:Processing batch to idx 208999:
INFO:__main__:Processing batch to idx 209999:
INFO:__main__:Processing batch to idx 210999:
INFO:__main__:Processing batch to idx 211999:
INFO:__main__:Processing batch to idx 212999:
INFO:__main__:Processing batch to idx 213999:
INFO:__main__:Processing batch to idx 214999:
INFO:__main__:Processing batch to idx 215999:
INFO:__main__:Processing batch to idx 216999:
INFO:__main__:Processing batch to idx 217999:
INFO:__main__:Processing batch to idx 218999:
INFO:__main__:Processing batch to idx 219999:
INFO:__main__:Processing batch to idx 220999:
INFO:__main__:Processing batch to idx 221999:
INFO:__main__:Processing batch to idx 222999:
INFO:__main__:Processing batch to idx 223999:
INFO:__main__:Processing batch to idx 224999:
INFO:__main__:Processing batch to idx 225999:
INFO:__main__:Processing batch to idx 226999:
INFO:__main__:Processing batch to idx 227999:
INFO:__main__:Processing batch to idx 228999:
INFO:__main__:Processing batch to idx 229514:
ERROR 03-18 01:55:08 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 1248024 died, exit code: -15
INFO 03-18 01:55:08 multiproc_worker_utils.py:128] Killing local vLLM worker processes
[rank0]:[W318 01:55:08.103698970 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/jx0800/.conda/envs/graphrag/lib/python3.11/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
