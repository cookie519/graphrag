INFO 03-17 16:29:47 __init__.py:190] Automatically detected platform cuda.
INFO:__main__:batch size: 500
INFO:__main__:num: 1
INFO:__main__:start_idx: 0
INFO:__main__:stop_idx: 500
INFO:__main__:Loading LLM
INFO 03-17 16:29:57 config.py:542] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 03-17 16:29:57 config.py:1401] Defaulting to use mp for distributed inference
WARNING 03-17 16:29:57 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 03-17 16:29:57 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 03-17 16:29:57 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='/scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 03-17 16:29:57 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:29:57 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
/home/jx0800/.conda/envs/graphrag/lib/python3.11/site-packages/vllm/executor/mp_distributed_executor.py:110: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 03-17 16:29:58 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:29:58 cuda.py:230] Using Flash Attention backend.
INFO 03-17 16:30:00 utils.py:950] Found nccl from library libnccl.so.2
INFO 03-17 16:30:00 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:30:00 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:30:00 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-17 16:30:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/jx0800/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:30:04 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/jx0800/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 03-17 16:30:04 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_da755f04'), local_subscribe_port=34013, remote_subscribe_port=None)
INFO 03-17 16:30:04 model_runner.py:1110] Starting to load model /scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct...
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:30:04 model_runner.py:1110] Starting to load model /scratch/gpfs/JHA/models/Llama-3.1-8B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.35s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.74s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:10<00:03,  3.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.03s/it]

[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:30:20 model_runner.py:1115] Loading model weights took 7.5123 GB
INFO 03-17 16:30:20 model_runner.py:1115] Loading model weights took 7.5123 GB
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:30:30 worker.py:267] Memory profiling takes 8.89 seconds
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:30:30 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:30:30 worker.py:267] model weights take 7.51GiB; non_torch_memory takes 1.58GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 61.94GiB.
INFO 03-17 16:30:30 worker.py:267] Memory profiling takes 9.39 seconds
INFO 03-17 16:30:30 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
INFO 03-17 16:30:30 worker.py:267] model weights take 7.51GiB; non_torch_memory takes 1.83GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 60.72GiB.
INFO 03-17 16:30:30 executor_base.py:110] # CUDA blocks: 62178, # CPU blocks: 4096
INFO 03-17 16:30:30 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 7.59x
INFO 03-17 16:30:35 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:30:35 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:28,  1.19it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:25,  1.27it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:23,  1.35it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:03<00:22,  1.35it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:21,  1.38it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:21,  1.36it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:05<00:20,  1.38it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.38it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:18,  1.38it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:07<00:18,  1.37it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:08<00:17,  1.38it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:16,  1.36it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:09<00:16,  1.37it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:10<00:15,  1.37it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:11<00:14,  1.36it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:11<00:14,  1.34it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:12<00:13,  1.35it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:13<00:12,  1.35it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:14<00:11,  1.35it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:14<00:10,  1.37it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:15<00:10,  1.36it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:16<00:09,  1.35it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:16<00:08,  1.36it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:17<00:08,  1.37it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:18<00:07,  1.37it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:19<00:06,  1.36it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:19<00:05,  1.36it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:20<00:05,  1.35it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:21<00:04,  1.33it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:21<00:03,  1.42it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:22<00:02,  1.39it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:23<00:02,  1.49it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:24<00:01,  1.37it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:24<00:00,  1.37it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:26<00:00,  1.04it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:26<00:00,  1.33it/s]
INFO 03-17 16:31:01 custom_all_reduce.py:226] Registering 2275 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:31:01 custom_all_reduce.py:226] Registering 2275 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1224403)[0;0m INFO 03-17 16:31:01 model_runner.py:1562] Graph capturing finished in 26 secs, took 0.25 GiB
INFO 03-17 16:31:01 model_runner.py:1562] Graph capturing finished in 26 secs, took 0.25 GiB
INFO 03-17 16:31:01 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 40.91 seconds
INFO:__main__:Processing batch to idx 499:
INFO 03-17 16:31:02 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
ERROR 03-17 16:31:28 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 1224403 died, exit code: -15
INFO 03-17 16:31:28 multiproc_worker_utils.py:128] Killing local vLLM worker processes
[rank0]:[W317 16:31:29.686761274 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
