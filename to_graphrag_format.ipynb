{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ccf355-01bd-4cbe-b05a-75918f80ba46",
   "metadata": {},
   "source": [
    "## Injected KG to graphrag format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9fdf74-4e6b-4b57-8f0f-6ac60b675056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Read the CSV file\n",
    "root = '/projects/JHA/shared/graph/pubmed'\n",
    "graph_path = os.path.join(root, 'injections_train.csv')\n",
    "output_path = os.path.join(root, 'injected')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# read the CSV file\n",
    "df = pd.read_csv(graph_path)\n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.MultiDiGraph()  # Use DiGraph for a directed graph. Use Graph() for an undirected graph.\n",
    "\n",
    "def add_edge_if_not_exists(graph, u, v, key=None, **attr):\n",
    "    # Check if an edge exists between u and v\n",
    "    if graph.has_edge(u, v):\n",
    "        # Get all edges between u and v\n",
    "        edges_data = graph.get_edge_data(u, v)\n",
    "        # Iterate through all edges between u and v\n",
    "        for edge_key, edge_attrs in edges_data.items():\n",
    "            # Check if the attributes match\n",
    "            if edge_attrs == attr:\n",
    "                #print(f\"Edge ({u}, {v}) with attributes {attr} already exists. Skipping.\")\n",
    "                return\n",
    "    # If no matching edge is found, add the new edge\n",
    "    graph.add_edge(u, v, key=key, **attr)\n",
    "    #print(f\"Edge ({u}, {v}) with attributes {attr} added.\")\n",
    "\n",
    "# Iterate over the rows in the CSV file\n",
    "for index, row in df.iterrows():\n",
    "    head = row['root']\n",
    "    relation = row['relation']\n",
    "    tail = row['tail']\n",
    "    # Add an edge to the graph with the relation as an edge attribute\n",
    "    add_edge_if_not_exists(G, head, tail, rel=relation)\n",
    "\n",
    "# Prepare entities dataframe\n",
    "nodes_data = []\n",
    "for human_readable_id, node in enumerate(G.nodes()):\n",
    "    node_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each node.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"title\": node,                        # Use the node name for the title.\n",
    "        \"description\": node,                  # Use the node name for the description.\n",
    "        \"degree\": G.degree(node)              # Calculate the node's degree.\n",
    "    }\n",
    "    nodes_data.append(node_entry)\n",
    "\n",
    "entities_df = pd.DataFrame(nodes_data)\n",
    "\n",
    "# Process edges to create the relationships DataFrame.\n",
    "edges_data = []\n",
    "for human_readable_id, (source, target, data) in enumerate(G.edges(data=True)):\n",
    "    combined_degree = G.degree(source) + G.degree(target)\n",
    "    rel = data.get(\"rel\", \"\")\n",
    "    if rel == 'isa': \n",
    "        rel = 'is a'\n",
    "    edge_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each edge.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"source\": source,                     # Source node (using the node name).\n",
    "        \"target\": target,                      # Target node (using the node name).\n",
    "        \"combined_degree\": combined_degree, \n",
    "        \"description\": f\"{source} {rel} {target}\"\n",
    "        # 'rel' attribute is available in data if needed: data.get('rel')\n",
    "    }\n",
    "    edges_data.append(edge_entry)\n",
    "\n",
    "relationships_df = pd.DataFrame(edges_data)\n",
    "\n",
    "# Save DataFrames to parquet files\n",
    "entities_df.to_parquet(os.path.join(output_path, 'entities.parquet'), index=False)\n",
    "relationships_df.to_parquet(os.path.join(output_path, 'relationships.parquet'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6d2326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "/home/jx0800/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/e5042dce39060cc34bc223455f25cf1d26db4655/modeling_hf_nomic_bert.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4534\n",
      "0\n",
      "Total embeddings generated: 4534\n",
      "\n",
      "Entities have been embedded and saved to LanceDB successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-01T02:23:39Z WARN  lance::dataset::write::insert] No existing dataset at /projects/JHA/shared/graph/pubmed/injected/lancedb/default-entity-description.lance.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "\n",
    "entities_df = pd.read_parquet(os.path.join(output_path, 'entities.parquet'))\n",
    "sentences = list(entities_df['description'])\n",
    "print(len(sentences))\n",
    "final_embeddings = []\n",
    "batch_size = 50000\n",
    "for i in range(0, len(sentences)+1, batch_size):\n",
    "    print(i)\n",
    "    if i+batch_size<=len(sentences)+1:\n",
    "        batch = sentences[i:i+batch_size]\n",
    "    else:\n",
    "        batch = sentences[i:]\n",
    "    response = model.encode(batch)\n",
    "    final_embeddings.extend(response)\n",
    "\n",
    "import json\n",
    "print(f\"Total embeddings generated: {len(final_embeddings)}\")\n",
    "\n",
    "entities_df['vector'] = final_embeddings\n",
    "\n",
    "# Create a new column \"attributes\" as a dictionary with the title\n",
    "entities_df['attributes'] = entities_df['title'].apply(lambda title: json.dumps({\"title\": title}))\n",
    "print()\n",
    "final_entities_df = entities_df[['id', 'description', 'vector', 'attributes']].rename(columns={'description': 'text'})\n",
    "\n",
    "# Connect to (or create) a LanceDB database and save the DataFrame.\n",
    "db = lancedb.connect(os.path.join(output_path, 'lancedb'))\n",
    "table = db.create_table(\"default-entity-description.lance\", final_entities_df, mode=\"overwrite\")\n",
    "\n",
    "print(\"Entities have been embedded and saved to LanceDB successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fa53e-21eb-45d5-8899-595cb9ec47ff",
   "metadata": {},
   "source": [
    "## injected KG (lim2) to graphrag format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54dfc616-58a5-4629-ba5b-3e3ce92eed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Read the CSV file\n",
    "root = '/projects/JHA/shared/graph/pubmed'\n",
    "graph_path = os.path.join(root, '32k_lim2_rel32.csv')\n",
    "output_path = os.path.join(root, 'injected_32k')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# read the CSV file\n",
    "df = pd.read_csv(graph_path)\n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.MultiDiGraph()  # Use DiGraph for a directed graph. Use Graph() for an undirected graph.\n",
    "\n",
    "def add_edge_if_not_exists(graph, u, v, key=None, **attr):\n",
    "    # Check if an edge exists between u and v\n",
    "    if graph.has_edge(u, v):\n",
    "        # Get all edges between u and v\n",
    "        edges_data = graph.get_edge_data(u, v)\n",
    "        # Iterate through all edges between u and v\n",
    "        for edge_key, edge_attrs in edges_data.items():\n",
    "            # Check if the attributes match\n",
    "            if edge_attrs == attr:\n",
    "                #print(f\"Edge ({u}, {v}) with attributes {attr} already exists. Skipping.\")\n",
    "                return\n",
    "    # If no matching edge is found, add the new edge\n",
    "    graph.add_edge(u, v, key=key, **attr)\n",
    "    #print(f\"Edge ({u}, {v}) with attributes {attr} added.\")\n",
    "\n",
    "# Iterate over the rows in the CSV file\n",
    "for index, row in df.iterrows():\n",
    "    head = row['root']\n",
    "    relation = row['relation']\n",
    "    tail = row['tail']\n",
    "    # Add an edge to the graph with the relation as an edge attribute\n",
    "    add_edge_if_not_exists(G, head, tail, rel=relation)\n",
    "\n",
    "# Prepare entities dataframe\n",
    "nodes_data = []\n",
    "for human_readable_id, node in enumerate(G.nodes()):\n",
    "    node_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each node.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"title\": node,                        # Use the node name for the title.\n",
    "        \"description\": node,                  # Use the node name for the description.\n",
    "        \"degree\": G.degree(node)              # Calculate the node's degree.\n",
    "    }\n",
    "    nodes_data.append(node_entry)\n",
    "\n",
    "entities_df = pd.DataFrame(nodes_data)\n",
    "\n",
    "# Process edges to create the relationships DataFrame.\n",
    "edges_data = []\n",
    "for human_readable_id, (source, target, data) in enumerate(G.edges(data=True)):\n",
    "    combined_degree = G.degree(source) + G.degree(target)\n",
    "    rel = data.get(\"rel\", \"\")\n",
    "    if rel == 'isa': \n",
    "        rel = 'is a'\n",
    "    edge_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each edge.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"source\": source,                     # Source node (using the node name).\n",
    "        \"target\": target,                      # Target node (using the node name).\n",
    "        \"combined_degree\": combined_degree, \n",
    "        \"description\": f\"{source} {rel} {target}\"\n",
    "        # 'rel' attribute is available in data if needed: data.get('rel')\n",
    "    }\n",
    "    edges_data.append(edge_entry)\n",
    "\n",
    "relationships_df = pd.DataFrame(edges_data)\n",
    "\n",
    "# Save DataFrames to parquet files\n",
    "entities_df.to_parquet(os.path.join(output_path, 'entities.parquet'), index=False)\n",
    "relationships_df.to_parquet(os.path.join(output_path, 'relationships.parquet'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d8a632-317c-4ecb-848f-071c07374469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "/home/jx0800/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/e5042dce39060cc34bc223455f25cf1d26db4655/modeling_hf_nomic_bert.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43970\n",
      "0\n",
      "Total embeddings generated: 43970\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-18T18:09:37Z WARN  lance::dataset::write::insert] No existing dataset at /projects/JHA/shared/graph/pubmed/injected_32k/lancedb/default-entity-description.lance.lance, it will be created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities have been embedded and saved to LanceDB successfully.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "\n",
    "entities_df = pd.read_parquet(os.path.join(output_path, 'entities.parquet'))\n",
    "sentences = list(entities_df['description'])\n",
    "print(len(sentences))\n",
    "final_embeddings = []\n",
    "batch_size = 50000\n",
    "for i in range(0, len(sentences)+1, batch_size):\n",
    "    print(i)\n",
    "    if i+batch_size<=len(sentences)+1:\n",
    "        batch = sentences[i:i+batch_size]\n",
    "    else:\n",
    "        batch = sentences[i:]\n",
    "    response = model.encode(batch)\n",
    "    final_embeddings.extend(response)\n",
    "\n",
    "import json\n",
    "print(f\"Total embeddings generated: {len(final_embeddings)}\")\n",
    "\n",
    "entities_df['vector'] = final_embeddings\n",
    "\n",
    "# Create a new column \"attributes\" as a dictionary with the title\n",
    "entities_df['attributes'] = entities_df['title'].apply(lambda title: json.dumps({\"title\": title}))\n",
    "print()\n",
    "final_entities_df = entities_df[['id', 'description', 'vector', 'attributes']].rename(columns={'description': 'text'})\n",
    "\n",
    "# Connect to (or create) a LanceDB database and save the DataFrame.\n",
    "db = lancedb.connect(os.path.join(output_path, 'lancedb'))\n",
    "table = db.create_table(\"default-entity-description\", final_entities_df, mode=\"overwrite\")\n",
    "\n",
    "print(\"Entities have been embedded and saved to LanceDB successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f7ddf-ec59-4d61-b17b-29a2177b3d85",
   "metadata": {},
   "source": [
    "## expanded KG (lim2) to graphrag format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "784cc964-63eb-4fa9-a418-b985c4c3b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Read the CSV file\n",
    "root = '/projects/JHA/shared/graph/pubmed'\n",
    "graph_path = os.path.join(root, 'expanded_true_triples.csv')\n",
    "output_path = os.path.join(root, 'expanded_32k')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# read the CSV file\n",
    "df = pd.read_csv(graph_path, na_values=['NULL'], keep_default_na=False)\n",
    "root_degree = df['root'].value_counts()\n",
    "tail_degree = df['tail'].value_counts()\n",
    "\n",
    "all_entities = set(root_degree.index).union(set(tail_degree.index))\n",
    "combined_degree = {entity: root_degree.get(entity, 0) + tail_degree.get(entity, 0) for entity in all_entities}\n",
    "sorted_combined_degree = dict(sorted(combined_degree.items(), key=lambda item: item[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7653c66-c050-46f8-8ea2-7344eba4ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the rows in the CSV file\n",
    "# Prepare entities dataframe\n",
    "nodes_data = []\n",
    "for index, (head, degree) in enumerate(sorted_combined_degree.items()):  \n",
    "    node_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each node.\n",
    "        \"human_readable_id\": index, # A sequential human readable id.\n",
    "        \"title\": head,                        # Use the node name for the title.\n",
    "        \"description\": head,                  # Use the node name for the description.\n",
    "        \"degree\": degree              # Calculate the node's degree.\n",
    "    }\n",
    "    nodes_data.append(node_entry)\n",
    "\n",
    "entities_df = pd.DataFrame(nodes_data)\n",
    "# Save DataFrames to parquet files\n",
    "entities_df.to_parquet(os.path.join(output_path, 'entities.parquet'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbc6fe1d-d277-4b12-8b32-6ef054f224ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process edges to create the relationships DataFrame.\n",
    "edges_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    head = row['root']\n",
    "    relation = row['relation']\n",
    "    tail = row['tail']\n",
    "    \n",
    "    if relation == 'isa': \n",
    "        relation = 'is a'\n",
    "    edge_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each edge.\n",
    "        \"human_readable_id\": index, # A sequential human readable id.\n",
    "        \"source\": head,                     # Source node (using the node name).\n",
    "        \"target\": tail,                      # Target node (using the node name).\n",
    "        \"combined_degree\": sorted_combined_degree[head]+sorted_combined_degree[tail], \n",
    "        \"description\": f\"{head} {relation} {tail}\"\n",
    "        # 'rel' attribute is available in data if needed: data.get('rel')\n",
    "    }\n",
    "    edges_data.append(edge_entry)\n",
    "\n",
    "relationships_df = pd.DataFrame(edges_data)\n",
    "\n",
    "# Save DataFrames to parquet files\n",
    "relationships_df.to_parquet(os.path.join(output_path, 'relationships.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aac99d62-0559-491a-87b4-59db54edc8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "/home/jx0800/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/e5042dce39060cc34bc223455f25cf1d26db4655/modeling_hf_nomic_bert.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389476\n",
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "Total embeddings generated: 389476\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-19T21:26:13Z WARN  lance::dataset::write::insert] No existing dataset at /projects/JHA/shared/graph/pubmed/expanded_32k/lancedb/default-entity-description.lance, it will be created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities have been embedded and saved to LanceDB successfully.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "\n",
    "entities_df = pd.read_parquet(os.path.join(output_path, 'entities.parquet'))\n",
    "sentences = list(entities_df['description'])\n",
    "print(len(sentences))\n",
    "final_embeddings = []\n",
    "batch_size = 50000\n",
    "for i in range(0, len(sentences)+1, batch_size):\n",
    "    print(i)\n",
    "    if i+batch_size<=len(sentences)+1:\n",
    "        batch = sentences[i:i+batch_size]\n",
    "    else:\n",
    "        batch = sentences[i:]\n",
    "    response = model.encode(batch)\n",
    "    final_embeddings.extend(response)\n",
    "\n",
    "import json\n",
    "print(f\"Total embeddings generated: {len(final_embeddings)}\")\n",
    "\n",
    "entities_df['vector'] = final_embeddings\n",
    "\n",
    "# Create a new column \"attributes\" as a dictionary with the title\n",
    "entities_df['attributes'] = entities_df['title'].apply(lambda title: json.dumps({\"title\": title}))\n",
    "print()\n",
    "final_entities_df = entities_df[['id', 'description', 'vector', 'attributes']].rename(columns={'description': 'text'})\n",
    "\n",
    "# Connect to (or create) a LanceDB database and save the DataFrame.\n",
    "db = lancedb.connect(os.path.join(output_path, 'lancedb'))\n",
    "table = db.create_table(\"default-entity-description\", final_entities_df, mode=\"overwrite\")\n",
    "\n",
    "print(\"Entities have been embedded and saved to LanceDB successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2fcd4-5c84-48f2-b200-67925781ec2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983c4f0-d147-4b0b-8c72-8ef025426252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag [~/.conda/envs/graphrag/]",
   "language": "python",
   "name": "conda_graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
