{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406e4226-2c9a-4598-8841-a13c242f9b72",
   "metadata": {},
   "source": [
    "## match graph data format to Graphrag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c1beb-da1a-4565-822f-42b9646f115c",
   "metadata": {},
   "source": [
    "## load llama expanded triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a63f912-81b6-4dbf-85af-8647e96b673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['head', 'predictions', 'text', 'tails'],\n",
      "    num_rows: 5714\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.load_from_disk(\"/projects/JHA/shared/dataset/head_predictions_filtered_tails_dataset\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8215319d-1fa0-45d4-9edb-bb71039e0c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7728\n",
      "CSV file saved at: /projects/JHA/shared/graph/pubmed/unique_triples.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = dataset.remove_columns([col for col in dataset.column_names if col not in ['head', 'tails']])\n",
    "\n",
    "# Create a set to store unique (head, tail) pairs\n",
    "unique_triples = set()\n",
    "\n",
    "for entry in dataset:\n",
    "    head = entry[\"head\"]\n",
    "    tails = [tail for tail in entry[\"tails\"] if tail.strip()]  # Remove empty strings\n",
    "    \n",
    "    for tail in tails:\n",
    "        unique_triples.add((head, tail))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(unique_triples, columns=[\"head\", \"tail\"])\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"/projects/JHA/shared/graph/pubmed/unique_triples.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(len(df))\n",
    "print(f\"CSV file saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0f9681-ce46-4d68-baba-c044447141f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Read the CSV file\n",
    "root = '/projects/JHA/shared/graph/pubmed'\n",
    "injected_path = os.path.join(root, 'injections_train.csv')\n",
    "expanded_path = os.path.join(root, 'unique_triples.csv')\n",
    "output_path = os.path.join(root, 'injected_expanded')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# read the CSV file\n",
    "df = pd.read_csv(injected_path)\n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.MultiDiGraph()  # Use DiGraph for a directed graph. Use Graph() for an undirected graph.\n",
    "\n",
    "def add_edge_if_not_exists(graph, u, v, key=None, **attr):\n",
    "    # Check if an edge exists between u and v\n",
    "    if graph.has_edge(u, v):\n",
    "        # Get all edges between u and v\n",
    "        edges_data = graph.get_edge_data(u, v)\n",
    "        # Iterate through all edges between u and v\n",
    "        for edge_key, edge_attrs in edges_data.items():\n",
    "            # Check if the attributes match\n",
    "            if edge_attrs == attr:\n",
    "                #print(f\"Edge ({u}, {v}) with attributes {attr} already exists. Skipping.\")\n",
    "                return\n",
    "    # If no matching edge is found, add the new edge\n",
    "    graph.add_edge(u, v, key=key, **attr)\n",
    "    #print(f\"Edge ({u}, {v}) with attributes {attr} added.\")\n",
    "\n",
    "# Iterate over the rows in the CSV file\n",
    "for index, row in df.iterrows():\n",
    "    head = row['root']\n",
    "    relation = row['relation']\n",
    "    tail = row['tail']\n",
    "    # Add an edge to the graph with the relation as an edge attribute\n",
    "    add_edge_if_not_exists(G, head, tail, rel=relation)\n",
    "\n",
    "df2 = pd.read_csv(expanded_path)\n",
    "# Iterate over the rows in the CSV file\n",
    "for index, row in df2.iterrows():\n",
    "    head = row['head']\n",
    "    relation = 'brings about'\n",
    "    tail = row['tail']\n",
    "    # Add an edge to the graph with the relation as an edge attribute\n",
    "    add_edge_if_not_exists(G, head, tail, rel=relation)\n",
    "\n",
    "# Prepare entities dataframe\n",
    "nodes_data = []\n",
    "for human_readable_id, node in enumerate(G.nodes()):\n",
    "    node_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each node.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"title\": node,                        # Use the node name for the title.\n",
    "        \"description\": node,                  # Use the node name for the description.\n",
    "        \"degree\": G.degree(node)              # Calculate the node's degree.\n",
    "    }\n",
    "    nodes_data.append(node_entry)\n",
    "\n",
    "entities_df = pd.DataFrame(nodes_data)\n",
    "\n",
    "# Process edges to create the relationships DataFrame.\n",
    "edges_data = []\n",
    "for human_readable_id, (source, target, data) in enumerate(G.edges(data=True)):\n",
    "    combined_degree = G.degree(source) + G.degree(target)\n",
    "    rel = data.get(\"rel\", \"\")\n",
    "    if rel == 'isa': \n",
    "        rel = 'is a'\n",
    "    edge_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each edge.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"source\": source,                     # Source node (using the node name).\n",
    "        \"target\": target,                      # Target node (using the node name).\n",
    "        \"combined_degree\": combined_degree, \n",
    "        \"description\": f\"{source} {rel} {target}\"\n",
    "        # 'rel' attribute is available in data if needed: data.get('rel')\n",
    "    }\n",
    "    edges_data.append(edge_entry)\n",
    "\n",
    "relationships_df = pd.DataFrame(edges_data)\n",
    "\n",
    "# Save DataFrames to parquet files\n",
    "entities_df.to_parquet(os.path.join(output_path, 'entities.parquet'), index=False)\n",
    "relationships_df.to_parquet(os.path.join(output_path, 'relationships.parquet'), index=False)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e462f0-f7a4-4d90-a9b8-a73ca644903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8043\n",
      "12388\n"
     ]
    }
   ],
   "source": [
    "print(len(G.nodes()))\n",
    "print(len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff734875-2b5b-485d-80a9-e787fe97aa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32316745622434c847c351f767808ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049912c049d4492cb58d969ee6c8ccf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded73cb3c55c48baa4924a2b18472d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/70.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51572ac1bff140d0a7bf0191a331a607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2099212b20c64e8d9021e35a724f2489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d964a707ed453bbec09f201ed82fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_hf_nomic_bert.py:   0%|          | 0.00/1.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- configuration_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73033c027b1b4ea091fa2f429b31473b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_hf_nomic_bert.py:   0%|          | 0.00/103k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- modeling_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d1ec08da864c97b6260cd237ab2169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/547M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jx0800/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/e5042dce39060cc34bc223455f25cf1d26db4655/modeling_hf_nomic_bert.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad78de4c25bd48cfb6f1eb4367619798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2b3c45004845b0bcfe1afcdee2e97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5844c1eeb4604f2892557466e9726629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66cce9b51714691aec7783fa3479114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caadc6217c4041cc921d6433809d3112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8043\n",
      "0\n",
      "Total embeddings generated: 8043\n",
      "\n",
      "Entities have been embedded and saved to LanceDB successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-03T16:42:11Z WARN  lance::dataset::write::insert] No existing dataset at /projects/JHA/shared/graph/pubmed/injected_expanded/lancedb/default-entity-description.lance.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "\n",
    "entities_df = pd.read_parquet(os.path.join(output_path, 'entities.parquet'))\n",
    "sentences = list(entities_df['description'])\n",
    "print(len(sentences))\n",
    "final_embeddings = []\n",
    "batch_size = 50000\n",
    "for i in range(0, len(sentences)+1, batch_size):\n",
    "    print(i)\n",
    "    if i+batch_size<=len(sentences)+1:\n",
    "        batch = sentences[i:i+batch_size]\n",
    "    else:\n",
    "        batch = sentences[i:]\n",
    "    response = model.encode(batch)\n",
    "    final_embeddings.extend(response)\n",
    "\n",
    "import json\n",
    "print(f\"Total embeddings generated: {len(final_embeddings)}\")\n",
    "\n",
    "entities_df['vector'] = final_embeddings\n",
    "\n",
    "# Create a new column \"attributes\" as a dictionary with the title\n",
    "entities_df['attributes'] = entities_df['title'].apply(lambda title: json.dumps({\"title\": title}))\n",
    "print()\n",
    "final_entities_df = entities_df[['id', 'description', 'vector', 'attributes']].rename(columns={'description': 'text'})\n",
    "\n",
    "# Connect to (or create) a LanceDB database and save the DataFrame.\n",
    "db = lancedb.connect(os.path.join(output_path, 'lancedb'))\n",
    "table = db.create_table(\"default-entity-description\", final_entities_df, mode=\"overwrite\")\n",
    "\n",
    "print(\"Entities have been embedded and saved to LanceDB successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662467d-afc5-4a34-822c-204c7ed13daf",
   "metadata": {},
   "source": [
    "## extract llama filtered triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc48f1b-2ef8-421b-8a22-db0a72c16998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Read the CSV file\n",
    "root = '/projects/JHA/shared/graph/pubmed'\n",
    "injected_path = os.path.join(root, 'injections_train.csv')\n",
    "expanded_path = os.path.join(root, 'unique_triples.csv')\n",
    "output_path = os.path.join(root, 'injected_expanded_filtered')\n",
    "filtered_path = os.path.join(root, 'new_triples_with_responses.csv')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# read the CSV file\n",
    "df = pd.read_csv(injected_path)\n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.MultiDiGraph()  # Use DiGraph for a directed graph. Use Graph() for an undirected graph.\n",
    "\n",
    "def add_edge_if_not_exists(graph, u, v, key=None, **attr):\n",
    "    # Check if an edge exists between u and v\n",
    "    if graph.has_edge(u, v):\n",
    "        # Get all edges between u and v\n",
    "        edges_data = graph.get_edge_data(u, v)\n",
    "        # Iterate through all edges between u and v\n",
    "        for edge_key, edge_attrs in edges_data.items():\n",
    "            # Check if the attributes match\n",
    "            if edge_attrs == attr:\n",
    "                #print(f\"Edge ({u}, {v}) with attributes {attr} already exists. Skipping.\")\n",
    "                return\n",
    "    # If no matching edge is found, add the new edge\n",
    "    graph.add_edge(u, v, key=key, **attr)\n",
    "    #print(f\"Edge ({u}, {v}) with attributes {attr} added.\")\n",
    "\n",
    "# Iterate over the rows in the CSV file\n",
    "for index, row in df.iterrows():\n",
    "    head = row['root']\n",
    "    relation = row['relation']\n",
    "    tail = row['tail']\n",
    "    # Add an edge to the graph with the relation as an edge attribute\n",
    "    add_edge_if_not_exists(G, head, tail, rel=relation)\n",
    "\n",
    "# read filtered triples\n",
    "df = pd.read_csv(filtered_path)\n",
    "df = df.drop(columns=['response'])\n",
    "df = df[df['valid'] == True]\n",
    "# Iterate over the rows in the CSV file\n",
    "for index, row in df.iterrows():\n",
    "    head = row['head']\n",
    "    relation = 'brings about'\n",
    "    tail = row['tail']\n",
    "    # Add an edge to the graph with the relation as an edge attribute\n",
    "    add_edge_if_not_exists(G, head, tail, rel=relation)\n",
    "\n",
    "# Prepare entities dataframe\n",
    "nodes_data = []\n",
    "for human_readable_id, node in enumerate(G.nodes()):\n",
    "    node_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each node.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"title\": node,                        # Use the node name for the title.\n",
    "        \"description\": node,                  # Use the node name for the description.\n",
    "        \"degree\": G.degree(node)              # Calculate the node's degree.\n",
    "    }\n",
    "    nodes_data.append(node_entry)\n",
    "\n",
    "entities_df = pd.DataFrame(nodes_data)\n",
    "\n",
    "# Process edges to create the relationships DataFrame.\n",
    "edges_data = []\n",
    "for human_readable_id, (source, target, data) in enumerate(G.edges(data=True)):\n",
    "    combined_degree = G.degree(source) + G.degree(target)\n",
    "    rel = data.get(\"rel\", \"\")\n",
    "    if rel == 'isa': \n",
    "        rel = 'is a'\n",
    "    edge_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each edge.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"source\": source,                     # Source node (using the node name).\n",
    "        \"target\": target,                      # Target node (using the node name).\n",
    "        \"combined_degree\": combined_degree, \n",
    "        \"description\": f\"{source} {rel} {target}\"\n",
    "        # 'rel' attribute is available in data if needed: data.get('rel')\n",
    "    }\n",
    "    edges_data.append(edge_entry)\n",
    "\n",
    "relationships_df = pd.DataFrame(edges_data)\n",
    "\n",
    "# Save DataFrames to parquet files\n",
    "entities_df.to_parquet(os.path.join(output_path, 'entities.parquet'), index=False)\n",
    "relationships_df.to_parquet(os.path.join(output_path, 'relationships.parquet'), index=False)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f557095-9201-4dea-99ff-8c6c9ba03291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jx0800/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/e5042dce39060cc34bc223455f25cf1d26db4655/modeling_hf_nomic_bert.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6664\n",
      "0\n",
      "Total embeddings generated: 6664\n",
      "\n",
      "Entities have been embedded and saved to LanceDB successfully.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "\n",
    "entities_df = pd.read_parquet(os.path.join(output_path, 'entities.parquet'))\n",
    "sentences = list(entities_df['description'])\n",
    "print(len(sentences))\n",
    "final_embeddings = []\n",
    "batch_size = 50000\n",
    "for i in range(0, len(sentences)+1, batch_size):\n",
    "    print(i)\n",
    "    if i+batch_size<=len(sentences)+1:\n",
    "        batch = sentences[i:i+batch_size]\n",
    "    else:\n",
    "        batch = sentences[i:]\n",
    "    response = model.encode(batch)\n",
    "    final_embeddings.extend(response)\n",
    "\n",
    "import json\n",
    "print(f\"Total embeddings generated: {len(final_embeddings)}\")\n",
    "\n",
    "entities_df['vector'] = final_embeddings\n",
    "\n",
    "# Create a new column \"attributes\" as a dictionary with the title\n",
    "entities_df['attributes'] = entities_df['title'].apply(lambda title: json.dumps({\"title\": title}))\n",
    "print()\n",
    "final_entities_df = entities_df[['id', 'description', 'vector', 'attributes']].rename(columns={'description': 'text'})\n",
    "\n",
    "# Connect to (or create) a LanceDB database and save the DataFrame.\n",
    "db = lancedb.connect(os.path.join(output_path, 'lancedb'))\n",
    "table = db.create_table(\"default-entity-description\", final_entities_df, mode=\"overwrite\")\n",
    "\n",
    "print(\"Entities have been embedded and saved to LanceDB successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98c671-a5ea-46d7-966d-baa2bf435731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag [~/.conda/envs/graphrag/]",
   "language": "python",
   "name": "conda_graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
