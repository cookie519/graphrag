{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4fa53e-21eb-45d5-8899-595cb9ec47ff",
   "metadata": {},
   "source": [
    "## MCQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8483e74-a593-4228-8888-c17debeab5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'opa', 'opb', 'opc', 'opd', 'cop'],\n",
      "    num_rows: 10333\n",
      "})\n",
      "{'question': 'A 54-year-old man is admitted to the hospital due to severe headaches. A CT examination reveals an internal carotid artery aneurysm inside the cavernous sinus. Which of the following nerves would be typically affected first?', 'opa': 'Abducens nerve', 'opb': 'Oculomotor nerve', 'opc': 'Ophthalmic nerve', 'opd': 'Maxillary nerve', 'cop': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.load_from_disk(\"/projects/JHA/shared/dataset/mcq_filtered_by_match\")\n",
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f149e81-b676-498e-bef6-cfc28a51d4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries have been written to /projects/JHA/shared/dataset/mcq_filtered_by_match/queries.txt\n",
      "answers have been written to /projects/JHA/shared/dataset/mcq_filtered_by_match/answers.json\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset.load_from_disk(\"/projects/JHA/shared/dataset/mcq_filtered_by_match\")\n",
    "\n",
    "# Shuffle the dataset and select 1000 samples\n",
    "sampled_dataset = dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Function to format a single instance into a query\n",
    "def format_query(instance):\n",
    "    question = instance['question'].replace(\"\\n\", \"\")\n",
    "    opa = instance['opa']\n",
    "    opb = instance['opb']\n",
    "    opc = instance['opc']\n",
    "    opd = instance['opd']\n",
    "    \n",
    "    query = f\"{question}: A. {opa}, B. {opb}, C. {opc}, D. {opd}. Please select the correct answer from A, B, C, D. Put your answer in \\\\boxed{{}}.\"\n",
    "    return query\n",
    "\n",
    "# Generate all queries\n",
    "queries = [format_query(instance) for instance in sampled_dataset]\n",
    "\n",
    "# Write all queries into a text file\n",
    "output_file_path = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/queries.txt\"\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file.write(\"\\n\".join(queries))\n",
    "\n",
    "print(f\"Queries have been written to {output_file_path}\")\n",
    "\n",
    "\n",
    "answers = [instance['cop'] for instance in sampled_dataset]\n",
    "\n",
    "json_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/answers.json\"\n",
    "with open(json_file, \"w\") as file:\n",
    "    json.dump(answers, file, indent=4)  # `indent=4` makes the JSON file human-readable\n",
    "print(f\"answers have been written to {json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fecd6e7-b01a-4a20-931a-8539b7ac00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(query_file, \"r\") as file:\n",
    "    queries = file.readlines()\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    if 'A' not in query:\n",
    "        print(f'{i}: {query}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82cc7ace-55b0-45a0-8904-0e6949be78f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 48-year-old Caucasian man, diagnosed with diabetes 5 years ago, comes to the physician because of palpitations, Physical examination shows a diffuse darkening of his skin and testicular atrophy. Laboratory studies show elevated liver function and elevated blood sugars. A liver biopsy shows significantly elevated iron levels A diagnosis of hereditary hemochromatosis is made. Which one of the following laboratory findings is seen in hereditary hemochromatosis?: A. Decreased serum ceruloplasmin, B. Decreased serum ferritin, C. Decreased serum iron, D. Decreased total iron-binding capacity. Please select the correct answer from A, B, C, D. Put your answer in \\boxed{}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(queries[174])  #174-175, 409-415 should be one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd443c-88e0-46c1-86d3-968b9b503ae0",
   "metadata": {},
   "source": [
    "## debug graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903ad951-f93c-46de-9e3b-55fdc20c65f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['reports', 'entities', 'relationships', 'claims', 'sources'])\n",
      "[{'id': '5', 'title': 'Artificial Intelligence and Convolutional Neural Networks', 'content': \"# Artificial Intelligence and Convolutional Neural Networks\\n\\nThe community revolves around Artificial Intelligence and its subsets, including Convolutional Neural Networks, which are used for various tasks such as image analysis and object detection. The community also includes related entities such as Natural Language Processing, Machine Learning, and specific neural network models like AlexNet and VGGNet.\\n\\n## Artificial Intelligence as the central entity\\n\\nArtificial Intelligence is the central entity in this community, with various subsets and related fields, including Machine Learning and Natural Language Processing. This entity is connected to other key entities, such as Convolutional Neural Networks, through relationships that highlight their significance in the community [Data: Entities (1), Relationships (0, 7); Entities (8)]. Artificial Intelligence has a broad scope, encompassing multiple approaches, including neural networks, to achieve its goals [Data: Entities (1)].\\n\\n## Convolutional Neural Networks' role in the community\\n\\nConvolutional Neural Networks are a crucial entity in this community, with various applications, including image analysis and object detection. They are connected to other key entities, such as AlexNet, VGGNet, and ResNet, through relationships that highlight their significance in the community [Data: Entities (30), Relationships (47, 48, 49); Relationships (50, 51)]. Convolutional Neural Networks can be combined with other neural networks, such as Recurrent Neural Networks and Transformers, for tasks involving sequential data [Data: Relationships (55, 56)].\\n\\n## Natural Language Processing' connection to the community\\n\\nNatural Language Processing is a related entity in this community, connected to Artificial Intelligence and Machine Learning through relationships that highlight its significance in the community [Data: Entities (8), Relationships (7)]. Natural Language Processing uses machine learning for various tasks, including speech recognition and text analysis [Data: Entities (8)].\\n\\n## Neural network models' significance in the community\\n\\nSpecific neural network models, such as AlexNet, VGGNet, and ResNet, are significant entities in this community, connected to Convolutional Neural Networks through relationships that highlight their importance [Data: Entities (47, 48, 49), Relationships (47, 48, 49)]. These models have achieved state-of-the-art performance on benchmark datasets, such as ImageNet [Data: Entities (52)].\\n\\n## Applications of Convolutional Neural Networks\\n\\nConvolutional Neural Networks have various applications, including medical imaging, genomics, and climate science, which are highlighted through relationships with entities such as Medical Imaging, Genomics, and Climate Science [Data: Relationships (60, 57, 58)]. These applications demonstrate the significant influence of Convolutional Neural Networks on various fields [Data: Relationships (60, 57, 58)].\"}, {'id': '1', 'title': 'Transformer Neural Networks and Related Entities', 'content': \"# Transformer Neural Networks and Related Entities\\n\\nThe community revolves around Transformer Neural Networks, which is a type of deep learning architecture used for natural language processing and other tasks. It has relationships with various entities such as Vaswani, NLP, BERT, GPT, T5, Vision Transformer, Reformer, Linformer, Longformer, RNN, CNN, Deep Learning, and Protein Folding, all of which are associated with the application or development of transformer neural networks. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Transformer Neural Networks as the central entity\\n\\nTransformer Neural Networks is the central entity in this community, being a type of deep learning architecture used for natural language processing and other tasks. [Data: Entities (64); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)] The significance of transformer neural networks can be seen in its relationships with various entities such as Vaswani, NLP, BERT, GPT, T5, Vision Transformer, Reformer, Linformer, Longformer, RNN, CNN, Deep Learning, and Protein Folding. [Data: Entities (65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Vaswani's role in introducing transformer neural networks\\n\\nVaswani is the author of the seminal paper 'Attention is All You Need' that introduced transformer neural networks. [Data: Entities (65); Relationships (68)] This introduction has had a significant impact on the development of natural language processing and other fields. [Data: Entities (66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Application of transformer neural networks in NLP\\n\\nTransformer neural networks have been widely applied in the field of natural language processing. [Data: Entities (66); Relationships (69, 70, 71, 72)] This application can be seen in models such as BERT, GPT, and T5, which use transformer neural networks for text classification, text generation, and text-to-text transfer tasks. [Data: Entities (67, 68, 69); Relationships (70, 71, 72)]\\n\\n## Relationship between transformer neural networks and deep learning\\n\\nTransformer neural networks are a type of deep learning architecture. [Data: Entities (64, 76); Relationships (81)] This relationship highlights the significance of transformer neural networks in the development of deep learning techniques. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Application of transformer neural networks in protein folding\\n\\nTransformer neural networks have been applied in the field of protein folding. [Data: Entities (77); Relationships (84)] This application demonstrates the potential of transformer neural networks in solving complex problems in biology. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\"}]\n"
     ]
    }
   ],
   "source": [
    "a = {'reports': [{'id': '5', 'title': 'Artificial Intelligence and Convolutional Neural Networks', 'content': \"# Artificial Intelligence and Convolutional Neural Networks\\n\\nThe community revolves around Artificial Intelligence and its subsets, including Convolutional Neural Networks, which are used for various tasks such as image analysis and object detection. The community also includes related entities such as Natural Language Processing, Machine Learning, and specific neural network models like AlexNet and VGGNet.\\n\\n## Artificial Intelligence as the central entity\\n\\nArtificial Intelligence is the central entity in this community, with various subsets and related fields, including Machine Learning and Natural Language Processing. This entity is connected to other key entities, such as Convolutional Neural Networks, through relationships that highlight their significance in the community [Data: Entities (1), Relationships (0, 7); Entities (8)]. Artificial Intelligence has a broad scope, encompassing multiple approaches, including neural networks, to achieve its goals [Data: Entities (1)].\\n\\n## Convolutional Neural Networks' role in the community\\n\\nConvolutional Neural Networks are a crucial entity in this community, with various applications, including image analysis and object detection. They are connected to other key entities, such as AlexNet, VGGNet, and ResNet, through relationships that highlight their significance in the community [Data: Entities (30), Relationships (47, 48, 49); Relationships (50, 51)]. Convolutional Neural Networks can be combined with other neural networks, such as Recurrent Neural Networks and Transformers, for tasks involving sequential data [Data: Relationships (55, 56)].\\n\\n## Natural Language Processing' connection to the community\\n\\nNatural Language Processing is a related entity in this community, connected to Artificial Intelligence and Machine Learning through relationships that highlight its significance in the community [Data: Entities (8), Relationships (7)]. Natural Language Processing uses machine learning for various tasks, including speech recognition and text analysis [Data: Entities (8)].\\n\\n## Neural network models' significance in the community\\n\\nSpecific neural network models, such as AlexNet, VGGNet, and ResNet, are significant entities in this community, connected to Convolutional Neural Networks through relationships that highlight their importance [Data: Entities (47, 48, 49), Relationships (47, 48, 49)]. These models have achieved state-of-the-art performance on benchmark datasets, such as ImageNet [Data: Entities (52)].\\n\\n## Applications of Convolutional Neural Networks\\n\\nConvolutional Neural Networks have various applications, including medical imaging, genomics, and climate science, which are highlighted through relationships with entities such as Medical Imaging, Genomics, and Climate Science [Data: Relationships (60, 57, 58)]. These applications demonstrate the significant influence of Convolutional Neural Networks on various fields [Data: Relationships (60, 57, 58)].\"}, {'id': '1', 'title': 'Transformer Neural Networks and Related Entities', 'content': \"# Transformer Neural Networks and Related Entities\\n\\nThe community revolves around Transformer Neural Networks, which is a type of deep learning architecture used for natural language processing and other tasks. It has relationships with various entities such as Vaswani, NLP, BERT, GPT, T5, Vision Transformer, Reformer, Linformer, Longformer, RNN, CNN, Deep Learning, and Protein Folding, all of which are associated with the application or development of transformer neural networks. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Transformer Neural Networks as the central entity\\n\\nTransformer Neural Networks is the central entity in this community, being a type of deep learning architecture used for natural language processing and other tasks. [Data: Entities (64); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)] The significance of transformer neural networks can be seen in its relationships with various entities such as Vaswani, NLP, BERT, GPT, T5, Vision Transformer, Reformer, Linformer, Longformer, RNN, CNN, Deep Learning, and Protein Folding. [Data: Entities (65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Vaswani's role in introducing transformer neural networks\\n\\nVaswani is the author of the seminal paper 'Attention is All You Need' that introduced transformer neural networks. [Data: Entities (65); Relationships (68)] This introduction has had a significant impact on the development of natural language processing and other fields. [Data: Entities (66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Application of transformer neural networks in NLP\\n\\nTransformer neural networks have been widely applied in the field of natural language processing. [Data: Entities (66); Relationships (69, 70, 71, 72)] This application can be seen in models such as BERT, GPT, and T5, which use transformer neural networks for text classification, text generation, and text-to-text transfer tasks. [Data: Entities (67, 68, 69); Relationships (70, 71, 72)]\\n\\n## Relationship between transformer neural networks and deep learning\\n\\nTransformer neural networks are a type of deep learning architecture. [Data: Entities (64, 76); Relationships (81)] This relationship highlights the significance of transformer neural networks in the development of deep learning techniques. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Application of transformer neural networks in protein folding\\n\\nTransformer neural networks have been applied in the field of protein folding. [Data: Entities (77); Relationships (84)] This application demonstrates the potential of transformer neural networks in solving complex problems in biology. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\"}], 'entities': [{'id': '75', 'entity': 'CNN', 'description': 'Convolutional Neural Networks are a type of deep learning architecture', 'number of relationships': '1', 'in_context': True}, {'id': '74', 'entity': 'RNN', 'description': 'Recurrent Neural Networks are a type of deep learning architecture', 'number of relationships': '1', 'in_context': True}, {'id': '30', 'entity': 'CONVOLUTIONAL NEURAL NETWORKS', 'description': 'Convolutional Neural Networks are a type of neural network designed for processing grid data, such as images. Additionally, they can be extended by Graph Convolutional Networks to also process graph data, thereby broadening their applicability beyond traditional image processing to more complex data structures. This versatility makes Convolutional Neural Networks a fundamental component in various deep learning applications, allowing them to handle a wide range of data types, from images to graphs, through their extensions and related network architectures.', 'number of relationships': '22', 'in_context': True}, {'id': '21', 'entity': 'DEEP NEURAL NETWORKS', 'description': 'Deep neural networks are a type of machine learning model', 'number of relationships': '1', 'in_context': True}, {'id': '62', 'entity': 'GPU', 'description': 'GPU is a type of hardware used for training Convolutional Neural Networks', 'number of relationships': '1', 'in_context': True}, {'id': '76', 'entity': 'DEEP LEARNING', 'description': 'Deep Learning is a field where transformer neural networks have been widely applied', 'number of relationships': '1', 'in_context': True}, {'id': '46', 'entity': 'LECUN', 'description': 'LeCun is a researcher who popularized Convolutional Neural Networks through the development of LeNet for digit recognition', 'number of relationships': '2', 'in_context': True}, {'id': '59', 'entity': 'LENET', 'description': 'LeNet is a convolutional neural network model developed by LeCun for digit recognition', 'number of relationships': '1', 'in_context': True}, {'id': '55', 'entity': 'RECURRENT NEURAL NETWORKS', 'description': 'Recurrent Neural Networks are a type of neural network designed for processing sequential data', 'number of relationships': '1', 'in_context': True}, {'id': '50', 'entity': 'YOLO', 'description': 'YOLO is an object detection framework that leverages Convolutional Neural Networks to identify and localize objects within images', 'number of relationships': '1', 'in_context': True}, {'id': '27', 'entity': 'GRAPH CONVOLUTIONAL NETWORKS', 'description': 'Graph Convolutional Networks are a type of Graph Neural Network that extend the concept of convolutional neural networks to graph data', 'number of relationships': '2', 'in_context': True}, {'id': '51', 'entity': 'FASTER R-CNN', 'description': 'Faster R-CNN is an object detection framework that leverages Convolutional Neural Networks to identify and localize objects within images', 'number of relationships': '1', 'in_context': True}, {'id': '56', 'entity': 'TRANSFORMERS', 'description': 'Transformers are a type of neural network designed for processing sequential data', 'number of relationships': '1', 'in_context': True}, {'id': '52', 'entity': 'IMAGENET', 'description': 'ImageNet is a benchmark dataset for image classification tasks', 'number of relationships': '1', 'in_context': True}, {'id': '63', 'entity': 'TPU', 'description': 'TPU is a type of hardware used for training Convolutional Neural Networks', 'number of relationships': '1', 'in_context': True}, {'id': '26', 'entity': 'GRAPH NEURAL NETWORKS', 'description': 'Graph Neural Networks are a class of machine learning algorithms designed to perform inference on data represented as graphs', 'number of relationships': '14', 'in_context': True}, {'id': '66', 'entity': 'NLP', 'description': 'Natural Language Processing is a field where transformer neural networks have been widely applied', 'number of relationships': '1', 'in_context': True}, {'id': '48', 'entity': 'VGGNET', 'description': 'VGGNet is a convolutional neural network model that achieved state-of-the-art performance on benchmark datasets such as ImageNet', 'number of relationships': '1', 'in_context': True}, {'id': '20', 'entity': 'SENTIMENT ANALYSIS', 'description': 'Sentiment analysis is a task that uses machine learning to analyze sentiment in text', 'number of relationships': '1', 'in_context': True}, {'id': '64', 'entity': 'TRANSFORMER NEURAL NETWORKS', 'description': 'Transformer neural networks are a type of deep learning architecture used for natural language processing and other tasks', 'number of relationships': '18', 'in_context': True}], 'relationships': [{'id': '39', 'source': 'GRAPH CONVOLUTIONAL NETWORKS', 'target': 'CONVOLUTIONAL NEURAL NETWORKS', 'description': 'Graph Convolutional Networks extend Convolutional Neural Networks to graph data', 'weight': '18.0', 'links': '1', 'in_context': True}, {'id': '46', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'LECUN', 'description': 'LeCun popularized Convolutional Neural Networks through the development of LeNet for digit recognition', 'weight': '16.0', 'links': '1', 'in_context': True}, {'id': '48', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'VGGNET', 'description': 'VGGNet is a type of Convolutional Neural Network model', 'weight': '12.0', 'links': '1', 'in_context': True}, {'id': '50', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'YOLO', 'description': 'YOLO is an object detection framework that leverages Convolutional Neural Networks', 'weight': '14.0', 'links': '1', 'in_context': True}, {'id': '51', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'FASTER R-CNN', 'description': 'Faster R-CNN is an object detection framework that leverages Convolutional Neural Networks', 'weight': '14.0', 'links': '1', 'in_context': True}, {'id': '52', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'IMAGENET', 'description': 'Convolutional Neural Networks are often trained on ImageNet', 'weight': '10.0', 'links': '1', 'in_context': True}, {'id': '55', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'RECURRENT NEURAL NETWORKS', 'description': 'Convolutional Neural Networks can be combined with Recurrent Neural Networks for tasks involving sequential data', 'weight': '6.0', 'links': '1', 'in_context': True}, {'id': '56', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'TRANSFORMERS', 'description': 'Convolutional Neural Networks can be combined with Transformers for tasks involving sequential data', 'weight': '6.0', 'links': '1', 'in_context': True}, {'id': '64', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'GPU', 'description': 'Convolutional Neural Networks are often trained on GPU hardware', 'weight': '5.0', 'links': '1', 'in_context': True}, {'id': '65', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'TPU', 'description': 'Convolutional Neural Networks are often trained on TPU hardware', 'weight': '5.0', 'links': '1', 'in_context': True}, {'id': '69', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'NLP', 'description': 'Transformer neural networks have been widely applied in the field of natural language processing', 'weight': '16.0', 'links': '4', 'in_context': True}, {'id': '77', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'RNN', 'description': 'RNN is a type of deep learning architecture that is different from transformer neural networks', 'weight': '6.0', 'links': '4', 'in_context': True}, {'id': '78', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'CNN', 'description': 'CNN is a type of deep learning architecture that is different from transformer neural networks', 'weight': '6.0', 'links': '4', 'in_context': True}, {'id': '81', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'DEEP LEARNING', 'description': 'Transformer neural networks are a type of deep learning architecture', 'weight': '9.0', 'links': '4', 'in_context': True}, {'id': '25', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'GRAPH CONVOLUTIONAL NETWORKS', 'description': 'Graph Neural Networks include Graph Convolutional Networks as a type', 'weight': '16.0', 'links': '1', 'in_context': True}, {'id': '67', 'source': 'LECUN', 'target': 'LENET', 'description': 'LeCun developed LeNet for digit recognition', 'weight': '9.0', 'links': '1', 'in_context': True}, {'id': '63', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'MACHINE LEARNING', 'description': 'Convolutional Neural Networks are a key component of machine learning', 'weight': '9.0', 'links': '5', 'in_context': True}, {'id': '80', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'MACHINE LEARNING', 'description': 'Transformer neural networks have been widely applied in the field of machine learning', 'weight': '8.0', 'links': '5', 'in_context': True}, {'id': '35', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'MACHINE LEARNING', 'description': 'Graph Neural Networks are a class of machine learning algorithms', 'weight': '9.0', 'links': '5', 'in_context': True}, {'id': '20', 'source': 'MACHINE LEARNING', 'target': 'SENTIMENT ANALYSIS', 'description': 'Sentiment analysis uses machine learning to analyze sentiment in text', 'weight': '8.0', 'links': '5', 'in_context': True}, {'id': '21', 'source': 'MACHINE LEARNING', 'target': 'DEEP NEURAL NETWORKS', 'description': 'Deep neural networks are a type of machine learning model', 'weight': '9.0', 'links': '5', 'in_context': True}, {'id': '59', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'COMPUTER VISION', 'description': 'Convolutional Neural Networks are a key component of computer vision', 'weight': '9.0', 'links': '2', 'in_context': True}, {'id': '82', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'COMPUTER VISION', 'description': 'Transformer neural networks have been applied in the field of computer vision', 'weight': '7.0', 'links': '2', 'in_context': True}, {'id': '85', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'REINFORCEMENT LEARNING', 'description': 'Transformer neural networks have been applied in the field of reinforcement learning', 'weight': '1.0', 'links': '2', 'in_context': True}, {'id': '79', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'AI', 'description': 'Transformer neural networks have been widely applied in the field of artificial intelligence', 'weight': '8.0', 'links': '2', 'in_context': True}, {'id': '36', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'REINFORCEMENT LEARNING', 'description': 'Graph Neural Networks can be integrated with reinforcement learning to create hybrid models', 'weight': '5.0', 'links': '2', 'in_context': True}, {'id': '38', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'AI', 'description': 'Graph Neural Networks are a part of the AI landscape', 'weight': '8.0', 'links': '2', 'in_context': True}, {'id': '61', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'NATURAL LANGUAGE PROCESSING', 'description': 'Convolutional Neural Networks can be used in natural language processing for text analysis', 'weight': '6.0', 'links': '1', 'in_context': True}, {'id': '62', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'ARTIFICIAL INTELLIGENCE', 'description': 'Convolutional Neural Networks are a key component of artificial intelligence', 'weight': '9.0', 'links': '1', 'in_context': True}, {'id': '47', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'ALEXNET', 'description': 'AlexNet is a type of Convolutional Neural Network model', 'weight': '12.0', 'links': '1', 'in_context': True}, {'id': '49', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'RESNET', 'description': 'ResNet is a type of Convolutional Neural Network model', 'weight': '12.0', 'links': '1', 'in_context': True}, {'id': '53', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'MOBILENETS', 'description': 'MobileNets are a type of efficient Convolutional Neural Network architecture', 'weight': '8.0', 'links': '1', 'in_context': True}, {'id': '54', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'EFFICIENTNETS', 'description': 'EfficientNets are a type of efficient Convolutional Neural Network architecture', 'weight': '8.0', 'links': '1', 'in_context': True}, {'id': '57', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'GENOMICS', 'description': 'Convolutional Neural Networks can be applied to genomics for tasks such as image analysis', 'weight': '4.0', 'links': '1', 'in_context': True}, {'id': '58', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'CLIMATE SCIENCE', 'description': 'Convolutional Neural Networks can be applied to climate science for tasks such as image analysis', 'weight': '3.0', 'links': '1', 'in_context': True}, {'id': '60', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'MEDICAL IMAGING', 'description': 'Convolutional Neural Networks are used in medical imaging for analyzing medical images', 'weight': '8.0', 'links': '1', 'in_context': True}, {'id': '66', 'source': 'CONVOLUTIONAL NEURAL NETWORKS', 'target': 'NEURAL ARCHITECTURE SEARCH', 'description': 'Neural Architecture Search is used to automate the design of Convolutional Neural Networks', 'weight': '1.0', 'links': '1', 'in_context': True}, {'id': '83', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'SPEECH RECOGNITION', 'description': 'Transformer neural networks have been applied in the field of speech recognition', 'weight': '7.0', 'links': '1', 'in_context': True}, {'id': '68', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'VASWANI', 'description': 'Vaswani introduced transformer neural networks in the paper \"Attention is All You Need\"', 'weight': '18.0', 'links': '1', 'in_context': True}, {'id': '70', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'BERT', 'description': 'BERT uses transformer neural networks for text classification and other NLP tasks', 'weight': '18.0', 'links': '1', 'in_context': True}, {'id': '71', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'GPT', 'description': 'GPT uses transformer neural networks for text generation and other NLP tasks', 'weight': '18.0', 'links': '1', 'in_context': True}, {'id': '72', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'T5', 'description': 'T5 uses transformer neural networks for text-to-text transfer and other NLP tasks', 'weight': '18.0', 'links': '1', 'in_context': True}, {'id': '73', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'VISION TRANSFORMER', 'description': 'Vision Transformer applies transformer neural networks to computer vision tasks', 'weight': '16.0', 'links': '1', 'in_context': True}, {'id': '74', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'REFORMER', 'description': 'Reformer aims to make transformer neural networks more efficient', 'weight': '14.0', 'links': '1', 'in_context': True}, {'id': '75', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'LINFORMER', 'description': 'Linformer aims to make transformer neural networks more efficient', 'weight': '14.0', 'links': '1', 'in_context': True}, {'id': '76', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'LONGFORMER', 'description': 'Longformer aims to make transformer neural networks more efficient', 'weight': '8.0', 'links': '1', 'in_context': True}, {'id': '84', 'source': 'TRANSFORMER NEURAL NETWORKS', 'target': 'PROTEIN FOLDING', 'description': 'Transformer neural networks have been applied in the field of protein folding', 'weight': '7.0', 'links': '1', 'in_context': True}, {'id': '30', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'KNOWLEDGE GRAPHS', 'description': 'Graph Neural Networks improve entity recognition and relationship extraction in knowledge graphs', 'weight': '10.0', 'links': '1', 'in_context': True}, {'id': '34', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'QUANTUM COMPUTING', 'description': 'Quantum computing holds potential for further enhancing the capabilities of Graph Neural Networks', 'weight': '3.0', 'links': '1', 'in_context': True}, {'id': '37', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'UNSUPERVISED LEARNING', 'description': 'Graph Neural Networks can be integrated with unsupervised learning to create hybrid models', 'weight': '5.0', 'links': '1', 'in_context': True}, {'id': '26', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'GRAPH ATTENTION NETWORKS', 'description': 'Graph Neural Networks include Graph Attention Networks as a type', 'weight': '16.0', 'links': '1', 'in_context': True}, {'id': '27', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'GRAPH RECURRENT NEURAL NETWORKS', 'description': 'Graph Neural Networks include Graph Recurrent Neural Networks as a type', 'weight': '16.0', 'links': '1', 'in_context': True}, {'id': '28', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'SOCIAL NETWORKS', 'description': 'Graph Neural Networks can be applied to social networks', 'weight': '10.0', 'links': '1', 'in_context': True}, {'id': '29', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'CHEMISTRY', 'description': 'Graph Neural Networks are used in chemistry to predict molecular properties', 'weight': '10.0', 'links': '1', 'in_context': True}, {'id': '31', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'TRANSPORTATION', 'description': 'Graph Neural Networks are employed in transportation for traffic prediction and route optimization', 'weight': '10.0', 'links': '1', 'in_context': True}, {'id': '32', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'FINANCE', 'description': 'Graph Neural Networks are employed in finance for fraud detection and risk assessment', 'weight': '10.0', 'links': '1', 'in_context': True}, {'id': '33', 'source': 'GRAPH NEURAL NETWORKS', 'target': 'BIOLOGY', 'description': 'Graph Neural Networks are employed in biology for protein structure prediction and interaction modeling', 'weight': '10.0', 'links': '1', 'in_context': True}], 'claims': [], 'sources': [{'id': '3', 'text': 'Introduction to Transformer Neural Networks\\nTransformer neural networks represent a revolutionary architecture in the field of deep learning, particularly for natural language processing (NLP) tasks. Introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al. in 2017, transformers have since become the backbone of numerous state-of-the-art models due to their ability to handle long-range dependencies and parallelize training processes. Unlike traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), transformers rely entirely on a mechanism called self-attention to process input data. This mechanism allows transformers to weigh the importance of different words in a sentence or elements in a sequence simultaneously, thus capturing context more effectively and efficiently.\\n\\nArchitecture of Transformers\\nThe core component of the transformer architecture is the self-attention mechanism, which enables the model to focus on different parts of the input sequence when producing an output. The transformer consists of an encoder and a decoder, each made up of a stack of identical layers. The encoder processes the input sequence and generates a set of attention-weighted vectors, while the decoder uses these vectors, along with the previously generated outputs, to produce the final sequence. Each layer in the encoder and decoder contains sub-layers, including multi-head self-attention mechanisms and position-wise fully connected feed-forward networks, followed by layer normalization and residual connections. This design allows the transformer to process entire sequences at once rather than step-by-step, making it highly parallelizable and efficient for training on large datasets.\\n\\nApplications of Transformer Neural Networks\\nTransformers have revolutionized various applications across different domains. In NLP, they power models like BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and T5 (Text-to-Text Transfer Transformer), which excel in tasks such as text classification, machine translation, question answering, and text generation. Beyond NLP, transformers have also shown remarkable performance in computer vision with models like Vision Transformer (ViT), which treats images as sequences of patches, similar to words in a sentence. Additionally, transformers are being explored in areas such as speech recognition, protein folding, and reinforcement learning, demonstrating their versatility and robustness in handling diverse types of data. The ability to process long-range dependencies and capture intricate patterns has made transformers indispensable in advancing the state of the art in many machine learning tasks.\\n\\nChallenges and Limitations\\nDespite their success, transformer neural networks come with several challenges and limitations. One of the primary concerns is their computational and memory requirements, which are significantly higher compared to traditional models. The quadratic complexity of the self-attention mechanism with respect to the input sequence length can lead to inefficiencies, especially when dealing with very long sequences. To mitigate this, various approaches like sparse attention and efficient transformers have been proposed. Another challenge is the interpretability of transformers, as the attention mechanisms, though providing some insights, do not fully explain the model\\'s decisions. Furthermore, transformers require large amounts of data and computational resources for training, which can be a barrier for smaller organizations or those with limited resources. Addressing these challenges is crucial for making transformers more accessible and scalable for a broader range of applications.\\n\\nFuture Directions\\nThe future of transformer neural networks is bright, with ongoing research focused on enhancing their efficiency, scalability, and applicability. One promising direction is the development of more efficient transformer architectures that reduce computational complexity and memory usage, such as the Reformer, Linformer, and Longformer. These models aim to make transformers feasible for longer sequences and real-time applications. Another important area is improving the interpretability of transformers, with efforts to develop methods that provide clearer explanations of their decision-making processes. Additionally, integrating transformers with other neural network architectures, such as combining them with convolutional networks for multimodal tasks, holds significant potential. The application of transformers beyond traditional domains, like in time-series forecasting, healthcare, and finance, is also expected to grow. As advancements continue, transformers are set to remain at the forefront of AI and machine learning, driving innovation and breakthroughs across various fields.'}, {'id': '2', 'text': 'Introduction to Convolutional Neural Networks\\nConvolutional Neural Networks (CNNs) are a specialized type of neural network designed primarily for processing structured grid data, such as images. Inspired by the visual cortex of animals, CNNs have become the cornerstone of computer vision applications due to their ability to automatically and adaptively learn spatial hierarchies of features. Introduced in the 1980s and popularized by LeCun et al. through the development of LeNet for digit recognition, CNNs have since evolved and expanded into various fields, achieving remarkable success in image classification, object detection, and segmentation tasks. The architecture of CNNs leverages convolutional layers to efficiently capture local patterns and features in data, making them highly effective for tasks involving high-dimensional inputs like images.\\n\\nArchitecture of Convolutional Neural Networks\\nThe architecture of a typical Convolutional Neural Network consists of several key components: convolutional layers, pooling layers, and fully connected layers. The convolutional layer is the core building block, where filters (or kernels) slide over the input data to produce feature maps. These filters are trained to recognize various patterns such as edges, textures, and shapes. Following convolutional layers, pooling layers (such as max pooling) are used to reduce the spatial dimensions of the feature maps, thereby decreasing the computational load and controlling overfitting. The fully connected layers, typically at the end of the network, take the high-level filtered and pooled features and perform the final classification or regression task. CNNs also often incorporate activation functions like ReLU (Rectified Linear Unit) and regularization techniques such as dropout to enhance performance and prevent overfitting.\\n\\nApplications of Convolutional Neural Networks\\nConvolutional Neural Networks have revolutionized various applications across multiple domains. In computer vision, CNNs are the backbone of image classification models like AlexNet, VGGNet, and ResNet, which have achieved state-of-the-art performance on benchmark datasets such as ImageNet. Object detection frameworks like YOLO (You Only Look Once) and Faster R-CNN leverage CNNs to identify and localize objects within images. In medical imaging, CNNs assist in diagnosing diseases by analyzing X-rays, MRIs, and CT scans. Beyond vision tasks, CNNs are employed in natural language processing for tasks like sentence classification and text generation when combined with other architectures. Additionally, CNNs are used in video analysis, facial recognition systems, and even in autonomous vehicles for tasks like lane detection and obstacle recognition. Their ability to automatically learn and extract relevant features from raw data has made CNNs indispensable in advancing artificial intelligence technologies.\\n\\nChallenges and Limitations\\nDespite their impressive capabilities, Convolutional Neural Networks face several challenges and limitations. One major issue is their computational intensity, which requires significant processing power and memory, especially for deeper and more complex networks. Training large CNNs often necessitates specialized hardware such as GPUs or TPUs. Another challenge is the need for large labeled datasets to train effectively, which can be a limiting factor in domains where data is scarce or expensive to annotate. Additionally, CNNs can struggle with understanding global context due to their inherent focus on local features, making them less effective for tasks requiring long-range dependencies. Transfer learning and data augmentation techniques are commonly used to mitigate some of these issues, but they do not fully address the fundamental challenges. Interpretability is also a concern, as CNNs are often viewed as black-box models, making it difficult to understand the rationale behind their predictions. Enhancing the transparency and efficiency of CNNs remains an active area of research.\\n\\nFuture Directions\\nThe future of Convolutional Neural Networks holds exciting possibilities as researchers continue to innovate and address existing limitations. One promising direction is the development of more efficient architectures, such as MobileNets and EfficientNets, which aim to reduce computational complexity while maintaining high performance. Advances in neural architecture search (NAS) allow for automated design of optimized network structures tailored to specific tasks and hardware constraints. Integrating CNNs with other neural network types, such as recurrent neural networks (RNNs) or transformers, can lead to more powerful hybrid models capable of handling diverse data types and tasks. Additionally, the application of CNNs is expanding into new fields such as genomics, climate science, and even art generation, demonstrating their versatility. Efforts to improve the interpretability of CNNs, such as developing techniques for visualizing and understanding the learned filters and feature maps, are also crucial for building trust and transparency. As these advancements continue, CNNs are poised to remain a central tool in the ongoing evolution of artificial intelligence and machine learning.'}, {'id': '1', 'text': 'Introduction to Graph Neural Networks\\nGraph Neural Networks (GNNs) are a class of machine learning algorithms designed to perform inference on data represented as graphs. Unlike traditional neural networks that operate on grid-like data structures such as images or sequences, GNNs are specifically tailored to handle graph-structured data, where the relationships (edges) between entities (nodes) are paramount. This ability to incorporate both node features and graph topology into learning processes makes GNNs incredibly powerful for a variety of applications. From social networks and molecular structures to knowledge graphs and recommendation systems, GNNs leverage the inherent structure of graphs to capture complex patterns and dependencies.\\n\\nTypes of Graph Neural Networks\\nThere are several types of Graph Neural Networks, each designed to address specific aspects of graph data. The most common types include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Graph Recurrent Neural Networks (GRNNs). GCNs extend the concept of convolutional neural networks (CNNs) to graph data by performing convolutions on nodes, aggregating features from neighboring nodes to learn node embeddings. GATs enhance this process by incorporating attention mechanisms, allowing the model to weigh the importance of different neighbors differently during the aggregation process. GRNNs, on the other hand, utilize recurrent neural network architectures to capture temporal or sequential dependencies in dynamic graphs. Each of these models has unique strengths, enabling them to be applied effectively to various graph-related tasks.\\n\\nApplications of Graph Neural Networks\\nGraph Neural Networks have found applications in numerous fields, revolutionizing how we process and interpret graph-structured data. In social network analysis, GNNs can predict user behavior, detect communities, and recommend friends or content. In the field of chemistry, GNNs are used to predict molecular properties, aiding in drug discovery and material science. Knowledge graphs, which underpin many search engines and recommendation systems, benefit from GNNs through improved entity recognition and relationship extraction. Additionally, GNNs have been employed in transportation for traffic prediction and route optimization, in finance for fraud detection and risk assessment, and in biology for protein structure prediction and interaction modeling. The versatility of GNNs in handling diverse and complex data structures makes them invaluable across various domains.\\n\\nChallenges and Limitations\\nDespite their powerful capabilities, Graph Neural Networks face several challenges and limitations. One major challenge is scalability, as the computational complexity of GNNs can grow rapidly with the size of the graph, making it difficult to apply them to large-scale graphs. Efficient sampling and approximation methods are required to address this issue. Another challenge is the over-smoothing problem, where repeated aggregations can cause node representations to become indistinguishable, especially in deep GNNs. Addressing this requires careful design of the network architecture and training strategies. Additionally, graph data can be highly heterogeneous and dynamic, posing challenges in creating models that can adapt to varying graph structures and temporal changes. Lastly, GNNs, like other AI models, face issues related to interpretability and explainability, making it difficult to understand how predictions are made, which is crucial for trust and deployment in critical applications.\\n\\nFuture Directions\\nThe future of Graph Neural Networks is promising, with ongoing research focused on overcoming current limitations and expanding their applicability. One exciting direction is the development of more scalable GNN architectures, capable of handling massive graphs with billions of nodes and edges. Techniques such as graph sampling, mini-batching, and distributed computing are being explored to achieve this. Another important area is improving the robustness and generalization of GNNs to various types of graphs and tasks, including those involving dynamic and heterogeneous data. Advances in explainability and interpretability are also crucial, with efforts to develop methods that provide insights into the decision-making process of GNNs. Integration of GNNs with other AI and machine learning paradigms, such as reinforcement learning and unsupervised learning, is expected to create more powerful hybrid models. Moreover, the advent of quantum computing holds potential for further enhancing the capabilities of GNNs, enabling them to solve even more complex problems efficiently. As these advancements continue, GNNs are poised to become an even more integral part of the AI and machine learning landscape.'}, {'id': '0', 'text': 'Introduction to Machine Learning\\nMachine learning (ML) is a subset of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead, these systems learn from data patterns and make decisions based on their insights. This paradigm shift has revolutionized various industries by allowing for automated decision-making, predictive analytics, and advanced data processing capabilities. The core idea behind machine learning is to construct algorithms that can receive input data and use statistical analysis to predict an output value within an acceptable range. This iterative learning process improves the accuracy and efficiency of the algorithms, making them highly valuable in complex problem-solving scenarios.\\n\\nTypes of Machine Learning\\nMachine learning can be broadly categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves training a model on a labeled dataset, meaning that each training example is paired with an output label. This type of learning is typically used for classification and regression tasks. In contrast, unsupervised learning deals with unlabeled data, and the system tries to learn the underlying structure from the input data. Techniques like clustering and dimensionality reduction are common in this category. Reinforcement learning, on the other hand, focuses on training models to make a sequence of decisions by rewarding or penalizing them based on the actions taken. This approach is particularly useful in environments where the decision-making process is complex, such as robotics and game playing.\\n\\nApplications of Machine Learning\\nMachine learning has a wide array of applications across different domains. In healthcare, it is used for predictive diagnostics, personalized treatment plans, and drug discovery. Financial services leverage machine learning for fraud detection, credit scoring, and algorithmic trading. In the realm of e-commerce, recommendation systems powered by ML algorithms enhance user experience by suggesting relevant products. Additionally, machine learning plays a critical role in natural language processing (NLP) tasks such as speech recognition, language translation, and sentiment analysis. Autonomous vehicles, powered by sophisticated ML models, are becoming increasingly prevalent, showcasing the transformative potential of machine learning in transportation.\\n\\nChallenges and Limitations\\nDespite its numerous advantages, machine learning faces several challenges and limitations. One significant issue is the quality and quantity of data available for training. High-quality, large datasets are crucial for developing accurate and reliable models, but such data can be difficult to obtain. Another challenge is the interpretability of models, especially those that are highly complex like deep neural networks. These models often act as \"black boxes,\" making it difficult to understand how they arrive at specific decisions. Additionally, ethical considerations such as data privacy, security, and bias in AI systems are critical concerns that need to be addressed. Ensuring that ML systems are transparent, fair, and secure is essential for their widespread adoption and trustworthiness.\\n\\nFuture Directions\\nThe future of machine learning holds immense promise as research and development continue to advance. One exciting direction is the integration of machine learning with other AI disciplines, such as computer vision and NLP, to create more comprehensive and intelligent systems. Moreover, advancements in quantum computing are expected to significantly enhance the processing capabilities of ML algorithms, enabling them to tackle even more complex problems. The development of explainable AI (XAI) aims to make machine learning models more transparent and understandable, which is crucial for gaining user trust and meeting regulatory requirements. Furthermore, ongoing efforts to improve data efficiency, model robustness, and ethical standards will shape the future landscape of machine learning, making it an indispensable tool in various fields.'}]}\n",
    "print(a.keys())\n",
    "print(a['reports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "901c9307-af38-4093-bfb7-40395578adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "b= {'role': 'system', 'content': '\\n---Role---\\n\\nYou are a helpful assistant responding to questions about data in the tables provided.\\n\\n\\n---Goal---\\n\\nGenerate a response of the target length and format that responds to the user\\'s question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\\n\\nIf you don\\'t know the answer, just say so. Do not make anything up.\\n\\nPoints supported by data should list their data references as follows:\\n\\n\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\\n\\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\\n\\nFor example:\\n\\n\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\"\\n\\nwhere 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\\n\\nDo not include information where the supporting evidence for it is not provided.\\n\\n\\n---Target response length and format---\\n\\nMultiple Paragraphs\\n\\n\\n---Data tables---\\n\\n-----Reports-----\\nid|title|content\\n5|Artificial Intelligence and Convolutional Neural Networks|\"# Artificial Intelligence and Convolutional Neural Networks\\n\\nThe community revolves around Artificial Intelligence and its subsets, including Convolutional Neural Networks, which are used for various tasks such as image analysis and object detection. The community also includes related entities such as Natural Language Processing, Machine Learning, and specific neural network models like AlexNet and VGGNet.\\n\\n## Artificial Intelligence as the central entity\\n\\nArtificial Intelligence is the central entity in this community, with various subsets and related fields, including Machine Learning and Natural Language Processing. This entity is connected to other key entities, such as Convolutional Neural Networks, through relationships that highlight their significance in the community [Data: Entities (1), Relationships (0, 7); Entities (8)]. Artificial Intelligence has a broad scope, encompassing multiple approaches, including neural networks, to achieve its goals [Data: Entities (1)].\\n\\n## Convolutional Neural Networks\\' role in the community\\n\\nConvolutional Neural Networks are a crucial entity in this community, with various applications, including image analysis and object detection. They are connected to other key entities, such as AlexNet, VGGNet, and ResNet, through relationships that highlight their significance in the community [Data: Entities (30), Relationships (47, 48, 49); Relationships (50, 51)]. Convolutional Neural Networks can be combined with other neural networks, such as Recurrent Neural Networks and Transformers, for tasks involving sequential data [Data: Relationships (55, 56)].\\n\\n## Natural Language Processing\\' connection to the community\\n\\nNatural Language Processing is a related entity in this community, connected to Artificial Intelligence and Machine Learning through relationships that highlight its significance in the community [Data: Entities (8), Relationships (7)]. Natural Language Processing uses machine learning for various tasks, including speech recognition and text analysis [Data: Entities (8)].\\n\\n## Neural network models\\' significance in the community\\n\\nSpecific neural network models, such as AlexNet, VGGNet, and ResNet, are significant entities in this community, connected to Convolutional Neural Networks through relationships that highlight their importance [Data: Entities (47, 48, 49), Relationships (47, 48, 49)]. These models have achieved state-of-the-art performance on benchmark datasets, such as ImageNet [Data: Entities (52)].\\n\\n## Applications of Convolutional Neural Networks\\n\\nConvolutional Neural Networks have various applications, including medical imaging, genomics, and climate science, which are highlighted through relationships with entities such as Medical Imaging, Genomics, and Climate Science [Data: Relationships (60, 57, 58)]. These applications demonstrate the significant influence of Convolutional Neural Networks on various fields [Data: Relationships (60, 57, 58)].\"\\n1|Transformer Neural Networks and Related Entities|\"# Transformer Neural Networks and Related Entities\\n\\nThe community revolves around Transformer Neural Networks, which is a type of deep learning architecture used for natural language processing and other tasks. It has relationships with various entities such as Vaswani, NLP, BERT, GPT, T5, Vision Transformer, Reformer, Linformer, Longformer, RNN, CNN, Deep Learning, and Protein Folding, all of which are associated with the application or development of transformer neural networks. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Transformer Neural Networks as the central entity\\n\\nTransformer Neural Networks is the central entity in this community, being a type of deep learning architecture used for natural language processing and other tasks. [Data: Entities (64); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)] The significance of transformer neural networks can be seen in its relationships with various entities such as Vaswani, NLP, BERT, GPT, T5, Vision Transformer, Reformer, Linformer, Longformer, RNN, CNN, Deep Learning, and Protein Folding. [Data: Entities (65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Vaswani\\'s role in introducing transformer neural networks\\n\\nVaswani is the author of the seminal paper \\'Attention is All You Need\\' that introduced transformer neural networks. [Data: Entities (65); Relationships (68)] This introduction has had a significant impact on the development of natural language processing and other fields. [Data: Entities (66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Application of transformer neural networks in NLP\\n\\nTransformer neural networks have been widely applied in the field of natural language processing. [Data: Entities (66); Relationships (69, 70, 71, 72)] This application can be seen in models such as BERT, GPT, and T5, which use transformer neural networks for text classification, text generation, and text-to-text transfer tasks. [Data: Entities (67, 68, 69); Relationships (70, 71, 72)]\\n\\n## Relationship between transformer neural networks and deep learning\\n\\nTransformer neural networks are a type of deep learning architecture. [Data: Entities (64, 76); Relationships (81)] This relationship highlights the significance of transformer neural networks in the development of deep learning techniques. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\\n\\n## Application of transformer neural networks in protein folding\\n\\nTransformer neural networks have been applied in the field of protein folding. [Data: Entities (77); Relationships (84)] This application demonstrates the potential of transformer neural networks in solving complex problems in biology. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\"\\n\\n\\n-----Entities-----\\nid|entity|description|number of relationships\\n75|CNN|Convolutional Neural Networks are a type of deep learning architecture|1\\n74|RNN|Recurrent Neural Networks are a type of deep learning architecture|1\\n30|CONVOLUTIONAL NEURAL NETWORKS|Convolutional Neural Networks are a type of neural network designed for processing grid data, such as images. Additionally, they can be extended by Graph Convolutional Networks to also process graph data, thereby broadening their applicability beyond traditional image processing to more complex data structures. This versatility makes Convolutional Neural Networks a fundamental component in various deep learning applications, allowing them to handle a wide range of data types, from images to graphs, through their extensions and related network architectures.|22\\n21|DEEP NEURAL NETWORKS|Deep neural networks are a type of machine learning model|1\\n62|GPU|GPU is a type of hardware used for training Convolutional Neural Networks|1\\n76|DEEP LEARNING|Deep Learning is a field where transformer neural networks have been widely applied|1\\n46|LECUN|LeCun is a researcher who popularized Convolutional Neural Networks through the development of LeNet for digit recognition|2\\n59|LENET|LeNet is a convolutional neural network model developed by LeCun for digit recognition|1\\n55|RECURRENT NEURAL NETWORKS|Recurrent Neural Networks are a type of neural network designed for processing sequential data|1\\n50|YOLO|YOLO is an object detection framework that leverages Convolutional Neural Networks to identify and localize objects within images|1\\n27|GRAPH CONVOLUTIONAL NETWORKS|Graph Convolutional Networks are a type of Graph Neural Network that extend the concept of convolutional neural networks to graph data|2\\n51|FASTER R-CNN|Faster R-CNN is an object detection framework that leverages Convolutional Neural Networks to identify and localize objects within images|1\\n56|TRANSFORMERS|Transformers are a type of neural network designed for processing sequential data|1\\n52|IMAGENET|ImageNet is a benchmark dataset for image classification tasks|1\\n63|TPU|TPU is a type of hardware used for training Convolutional Neural Networks|1\\n26|GRAPH NEURAL NETWORKS|Graph Neural Networks are a class of machine learning algorithms designed to perform inference on data represented as graphs|14\\n66|NLP|Natural Language Processing is a field where transformer neural networks have been widely applied|1\\n48|VGGNET|VGGNet is a convolutional neural network model that achieved state-of-the-art performance on benchmark datasets such as ImageNet|1\\n20|SENTIMENT ANALYSIS|Sentiment analysis is a task that uses machine learning to analyze sentiment in text|1\\n64|TRANSFORMER NEURAL NETWORKS|Transformer neural networks are a type of deep learning architecture used for natural language processing and other tasks|18\\n\\n\\n-----Relationships-----\\nid|source|target|description|weight|links\\n39|GRAPH CONVOLUTIONAL NETWORKS|CONVOLUTIONAL NEURAL NETWORKS|Graph Convolutional Networks extend Convolutional Neural Networks to graph data|18.0|1\\n46|CONVOLUTIONAL NEURAL NETWORKS|LECUN|LeCun popularized Convolutional Neural Networks through the development of LeNet for digit recognition|16.0|1\\n48|CONVOLUTIONAL NEURAL NETWORKS|VGGNET|VGGNet is a type of Convolutional Neural Network model|12.0|1\\n50|CONVOLUTIONAL NEURAL NETWORKS|YOLO|YOLO is an object detection framework that leverages Convolutional Neural Networks|14.0|1\\n51|CONVOLUTIONAL NEURAL NETWORKS|FASTER R-CNN|Faster R-CNN is an object detection framework that leverages Convolutional Neural Networks|14.0|1\\n52|CONVOLUTIONAL NEURAL NETWORKS|IMAGENET|Convolutional Neural Networks are often trained on ImageNet|10.0|1\\n55|CONVOLUTIONAL NEURAL NETWORKS|RECURRENT NEURAL NETWORKS|Convolutional Neural Networks can be combined with Recurrent Neural Networks for tasks involving sequential data|6.0|1\\n56|CONVOLUTIONAL NEURAL NETWORKS|TRANSFORMERS|Convolutional Neural Networks can be combined with Transformers for tasks involving sequential data|6.0|1\\n64|CONVOLUTIONAL NEURAL NETWORKS|GPU|Convolutional Neural Networks are often trained on GPU hardware|5.0|1\\n65|CONVOLUTIONAL NEURAL NETWORKS|TPU|Convolutional Neural Networks are often trained on TPU hardware|5.0|1\\n69|TRANSFORMER NEURAL NETWORKS|NLP|Transformer neural networks have been widely applied in the field of natural language processing|16.0|4\\n77|TRANSFORMER NEURAL NETWORKS|RNN|RNN is a type of deep learning architecture that is different from transformer neural networks|6.0|4\\n78|TRANSFORMER NEURAL NETWORKS|CNN|CNN is a type of deep learning architecture that is different from transformer neural networks|6.0|4\\n81|TRANSFORMER NEURAL NETWORKS|DEEP LEARNING|Transformer neural networks are a type of deep learning architecture|9.0|4\\n25|GRAPH NEURAL NETWORKS|GRAPH CONVOLUTIONAL NETWORKS|Graph Neural Networks include Graph Convolutional Networks as a type|16.0|1\\n67|LECUN|LENET|LeCun developed LeNet for digit recognition|9.0|1\\n63|CONVOLUTIONAL NEURAL NETWORKS|MACHINE LEARNING|Convolutional Neural Networks are a key component of machine learning|9.0|5\\n80|TRANSFORMER NEURAL NETWORKS|MACHINE LEARNING|Transformer neural networks have been widely applied in the field of machine learning|8.0|5\\n35|GRAPH NEURAL NETWORKS|MACHINE LEARNING|Graph Neural Networks are a class of machine learning algorithms|9.0|5\\n20|MACHINE LEARNING|SENTIMENT ANALYSIS|Sentiment analysis uses machine learning to analyze sentiment in text|8.0|5\\n21|MACHINE LEARNING|DEEP NEURAL NETWORKS|Deep neural networks are a type of machine learning model|9.0|5\\n59|CONVOLUTIONAL NEURAL NETWORKS|COMPUTER VISION|Convolutional Neural Networks are a key component of computer vision|9.0|2\\n82|TRANSFORMER NEURAL NETWORKS|COMPUTER VISION|Transformer neural networks have been applied in the field of computer vision|7.0|2\\n85|TRANSFORMER NEURAL NETWORKS|REINFORCEMENT LEARNING|Transformer neural networks have been applied in the field of reinforcement learning|1.0|2\\n79|TRANSFORMER NEURAL NETWORKS|AI|Transformer neural networks have been widely applied in the field of artificial intelligence|8.0|2\\n36|GRAPH NEURAL NETWORKS|REINFORCEMENT LEARNING|Graph Neural Networks can be integrated with reinforcement learning to create hybrid models|5.0|2\\n38|GRAPH NEURAL NETWORKS|AI|Graph Neural Networks are a part of the AI landscape|8.0|2\\n61|CONVOLUTIONAL NEURAL NETWORKS|NATURAL LANGUAGE PROCESSING|Convolutional Neural Networks can be used in natural language processing for text analysis|6.0|1\\n62|CONVOLUTIONAL NEURAL NETWORKS|ARTIFICIAL INTELLIGENCE|Convolutional Neural Networks are a key component of artificial intelligence|9.0|1\\n47|CONVOLUTIONAL NEURAL NETWORKS|ALEXNET|AlexNet is a type of Convolutional Neural Network model|12.0|1\\n49|CONVOLUTIONAL NEURAL NETWORKS|RESNET|ResNet is a type of Convolutional Neural Network model|12.0|1\\n53|CONVOLUTIONAL NEURAL NETWORKS|MOBILENETS|MobileNets are a type of efficient Convolutional Neural Network architecture|8.0|1\\n54|CONVOLUTIONAL NEURAL NETWORKS|EFFICIENTNETS|EfficientNets are a type of efficient Convolutional Neural Network architecture|8.0|1\\n57|CONVOLUTIONAL NEURAL NETWORKS|GENOMICS|Convolutional Neural Networks can be applied to genomics for tasks such as image analysis|4.0|1\\n58|CONVOLUTIONAL NEURAL NETWORKS|CLIMATE SCIENCE|Convolutional Neural Networks can be applied to climate science for tasks such as image analysis|3.0|1\\n60|CONVOLUTIONAL NEURAL NETWORKS|MEDICAL IMAGING|Convolutional Neural Networks are used in medical imaging for analyzing medical images|8.0|1\\n66|CONVOLUTIONAL NEURAL NETWORKS|NEURAL ARCHITECTURE SEARCH|Neural Architecture Search is used to automate the design of Convolutional Neural Networks|1.0|1\\n83|TRANSFORMER NEURAL NETWORKS|SPEECH RECOGNITION|Transformer neural networks have been applied in the field of speech recognition|7.0|1\\n68|TRANSFORMER NEURAL NETWORKS|VASWANI|Vaswani introduced transformer neural networks in the paper \"Attention is All You Need\"|18.0|1\\n70|TRANSFORMER NEURAL NETWORKS|BERT|BERT uses transformer neural networks for text classification and other NLP tasks|18.0|1\\n71|TRANSFORMER NEURAL NETWORKS|GPT|GPT uses transformer neural networks for text generation and other NLP tasks|18.0|1\\n72|TRANSFORMER NEURAL NETWORKS|T5|T5 uses transformer neural networks for text-to-text transfer and other NLP tasks|18.0|1\\n73|TRANSFORMER NEURAL NETWORKS|VISION TRANSFORMER|Vision Transformer applies transformer neural networks to computer vision tasks|16.0|1\\n74|TRANSFORMER NEURAL NETWORKS|REFORMER|Reformer aims to make transformer neural networks more efficient|14.0|1\\n75|TRANSFORMER NEURAL NETWORKS|LINFORMER|Linformer aims to make transformer neural networks more efficient|14.0|1\\n76|TRANSFORMER NEURAL NETWORKS|LONGFORMER|Longformer aims to make transformer neural networks more efficient|8.0|1\\n84|TRANSFORMER NEURAL NETWORKS|PROTEIN FOLDING|Transformer neural networks have been applied in the field of protein folding|7.0|1\\n30|GRAPH NEURAL NETWORKS|KNOWLEDGE GRAPHS|Graph Neural Networks improve entity recognition and relationship extraction in knowledge graphs|10.0|1\\n34|GRAPH NEURAL NETWORKS|QUANTUM COMPUTING|Quantum computing holds potential for further enhancing the capabilities of Graph Neural Networks|3.0|1\\n37|GRAPH NEURAL NETWORKS|UNSUPERVISED LEARNING|Graph Neural Networks can be integrated with unsupervised learning to create hybrid models|5.0|1\\n26|GRAPH NEURAL NETWORKS|GRAPH ATTENTION NETWORKS|Graph Neural Networks include Graph Attention Networks as a type|16.0|1\\n27|GRAPH NEURAL NETWORKS|GRAPH RECURRENT NEURAL NETWORKS|Graph Neural Networks include Graph Recurrent Neural Networks as a type|16.0|1\\n28|GRAPH NEURAL NETWORKS|SOCIAL NETWORKS|Graph Neural Networks can be applied to social networks|10.0|1\\n29|GRAPH NEURAL NETWORKS|CHEMISTRY|Graph Neural Networks are used in chemistry to predict molecular properties|10.0|1\\n31|GRAPH NEURAL NETWORKS|TRANSPORTATION|Graph Neural Networks are employed in transportation for traffic prediction and route optimization|10.0|1\\n32|GRAPH NEURAL NETWORKS|FINANCE|Graph Neural Networks are employed in finance for fraud detection and risk assessment|10.0|1\\n33|GRAPH NEURAL NETWORKS|BIOLOGY|Graph Neural Networks are employed in biology for protein structure prediction and interaction modeling|10.0|1\\n\\n\\n\\n\\n-----Sources-----\\nid|text\\n3|Introduction to Transformer Neural Networks\\nTransformer neural networks represent a revolutionary architecture in the field of deep learning, particularly for natural language processing (NLP) tasks. Introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al. in 2017, transformers have since become the backbone of numerous state-of-the-art models due to their ability to handle long-range dependencies and parallelize training processes. Unlike traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), transformers rely entirely on a mechanism called self-attention to process input data. This mechanism allows transformers to weigh the importance of different words in a sentence or elements in a sequence simultaneously, thus capturing context more effectively and efficiently.\\n\\nArchitecture of Transformers\\nThe core component of the transformer architecture is the self-attention mechanism, which enables the model to focus on different parts of the input sequence when producing an output. The transformer consists of an encoder and a decoder, each made up of a stack of identical layers. The encoder processes the input sequence and generates a set of attention-weighted vectors, while the decoder uses these vectors, along with the previously generated outputs, to produce the final sequence. Each layer in the encoder and decoder contains sub-layers, including multi-head self-attention mechanisms and position-wise fully connected feed-forward networks, followed by layer normalization and residual connections. This design allows the transformer to process entire sequences at once rather than step-by-step, making it highly parallelizable and efficient for training on large datasets.\\n\\nApplications of Transformer Neural Networks\\nTransformers have revolutionized various applications across different domains. In NLP, they power models like BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and T5 (Text-to-Text Transfer Transformer), which excel in tasks such as text classification, machine translation, question answering, and text generation. Beyond NLP, transformers have also shown remarkable performance in computer vision with models like Vision Transformer (ViT), which treats images as sequences of patches, similar to words in a sentence. Additionally, transformers are being explored in areas such as speech recognition, protein folding, and reinforcement learning, demonstrating their versatility and robustness in handling diverse types of data. The ability to process long-range dependencies and capture intricate patterns has made transformers indispensable in advancing the state of the art in many machine learning tasks.\\n\\nChallenges and Limitations\\nDespite their success, transformer neural networks come with several challenges and limitations. One of the primary concerns is their computational and memory requirements, which are significantly higher compared to traditional models. The quadratic complexity of the self-attention mechanism with respect to the input sequence length can lead to inefficiencies, especially when dealing with very long sequences. To mitigate this, various approaches like sparse attention and efficient transformers have been proposed. Another challenge is the interpretability of transformers, as the attention mechanisms, though providing some insights, do not fully explain the model\\'s decisions. Furthermore, transformers require large amounts of data and computational resources for training, which can be a barrier for smaller organizations or those with limited resources. Addressing these challenges is crucial for making transformers more accessible and scalable for a broader range of applications.\\n\\nFuture Directions\\nThe future of transformer neural networks is bright, with ongoing research focused on enhancing their efficiency, scalability, and applicability. One promising direction is the development of more efficient transformer architectures that reduce computational complexity and memory usage, such as the Reformer, Linformer, and Longformer. These models aim to make transformers feasible for longer sequences and real-time applications. Another important area is improving the interpretability of transformers, with efforts to develop methods that provide clearer explanations of their decision-making processes. Additionally, integrating transformers with other neural network architectures, such as combining them with convolutional networks for multimodal tasks, holds significant potential. The application of transformers beyond traditional domains, like in time-series forecasting, healthcare, and finance, is also expected to grow. As advancements continue, transformers are set to remain at the forefront of AI and machine learning, driving innovation and breakthroughs across various fields.\\n2|Introduction to Convolutional Neural Networks\\nConvolutional Neural Networks (CNNs) are a specialized type of neural network designed primarily for processing structured grid data, such as images. Inspired by the visual cortex of animals, CNNs have become the cornerstone of computer vision applications due to their ability to automatically and adaptively learn spatial hierarchies of features. Introduced in the 1980s and popularized by LeCun et al. through the development of LeNet for digit recognition, CNNs have since evolved and expanded into various fields, achieving remarkable success in image classification, object detection, and segmentation tasks. The architecture of CNNs leverages convolutional layers to efficiently capture local patterns and features in data, making them highly effective for tasks involving high-dimensional inputs like images.\\n\\nArchitecture of Convolutional Neural Networks\\nThe architecture of a typical Convolutional Neural Network consists of several key components: convolutional layers, pooling layers, and fully connected layers. The convolutional layer is the core building block, where filters (or kernels) slide over the input data to produce feature maps. These filters are trained to recognize various patterns such as edges, textures, and shapes. Following convolutional layers, pooling layers (such as max pooling) are used to reduce the spatial dimensions of the feature maps, thereby decreasing the computational load and controlling overfitting. The fully connected layers, typically at the end of the network, take the high-level filtered and pooled features and perform the final classification or regression task. CNNs also often incorporate activation functions like ReLU (Rectified Linear Unit) and regularization techniques such as dropout to enhance performance and prevent overfitting.\\n\\nApplications of Convolutional Neural Networks\\nConvolutional Neural Networks have revolutionized various applications across multiple domains. In computer vision, CNNs are the backbone of image classification models like AlexNet, VGGNet, and ResNet, which have achieved state-of-the-art performance on benchmark datasets such as ImageNet. Object detection frameworks like YOLO (You Only Look Once) and Faster R-CNN leverage CNNs to identify and localize objects within images. In medical imaging, CNNs assist in diagnosing diseases by analyzing X-rays, MRIs, and CT scans. Beyond vision tasks, CNNs are employed in natural language processing for tasks like sentence classification and text generation when combined with other architectures. Additionally, CNNs are used in video analysis, facial recognition systems, and even in autonomous vehicles for tasks like lane detection and obstacle recognition. Their ability to automatically learn and extract relevant features from raw data has made CNNs indispensable in advancing artificial intelligence technologies.\\n\\nChallenges and Limitations\\nDespite their impressive capabilities, Convolutional Neural Networks face several challenges and limitations. One major issue is their computational intensity, which requires significant processing power and memory, especially for deeper and more complex networks. Training large CNNs often necessitates specialized hardware such as GPUs or TPUs. Another challenge is the need for large labeled datasets to train effectively, which can be a limiting factor in domains where data is scarce or expensive to annotate. Additionally, CNNs can struggle with understanding global context due to their inherent focus on local features, making them less effective for tasks requiring long-range dependencies. Transfer learning and data augmentation techniques are commonly used to mitigate some of these issues, but they do not fully address the fundamental challenges. Interpretability is also a concern, as CNNs are often viewed as black-box models, making it difficult to understand the rationale behind their predictions. Enhancing the transparency and efficiency of CNNs remains an active area of research.\\n\\nFuture Directions\\nThe future of Convolutional Neural Networks holds exciting possibilities as researchers continue to innovate and address existing limitations. One promising direction is the development of more efficient architectures, such as MobileNets and EfficientNets, which aim to reduce computational complexity while maintaining high performance. Advances in neural architecture search (NAS) allow for automated design of optimized network structures tailored to specific tasks and hardware constraints. Integrating CNNs with other neural network types, such as recurrent neural networks (RNNs) or transformers, can lead to more powerful hybrid models capable of handling diverse data types and tasks. Additionally, the application of CNNs is expanding into new fields such as genomics, climate science, and even art generation, demonstrating their versatility. Efforts to improve the interpretability of CNNs, such as developing techniques for visualizing and understanding the learned filters and feature maps, are also crucial for building trust and transparency. As these advancements continue, CNNs are poised to remain a central tool in the ongoing evolution of artificial intelligence and machine learning.\\n1|Introduction to Graph Neural Networks\\nGraph Neural Networks (GNNs) are a class of machine learning algorithms designed to perform inference on data represented as graphs. Unlike traditional neural networks that operate on grid-like data structures such as images or sequences, GNNs are specifically tailored to handle graph-structured data, where the relationships (edges) between entities (nodes) are paramount. This ability to incorporate both node features and graph topology into learning processes makes GNNs incredibly powerful for a variety of applications. From social networks and molecular structures to knowledge graphs and recommendation systems, GNNs leverage the inherent structure of graphs to capture complex patterns and dependencies.\\n\\nTypes of Graph Neural Networks\\nThere are several types of Graph Neural Networks, each designed to address specific aspects of graph data. The most common types include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Graph Recurrent Neural Networks (GRNNs). GCNs extend the concept of convolutional neural networks (CNNs) to graph data by performing convolutions on nodes, aggregating features from neighboring nodes to learn node embeddings. GATs enhance this process by incorporating attention mechanisms, allowing the model to weigh the importance of different neighbors differently during the aggregation process. GRNNs, on the other hand, utilize recurrent neural network architectures to capture temporal or sequential dependencies in dynamic graphs. Each of these models has unique strengths, enabling them to be applied effectively to various graph-related tasks.\\n\\nApplications of Graph Neural Networks\\nGraph Neural Networks have found applications in numerous fields, revolutionizing how we process and interpret graph-structured data. In social network analysis, GNNs can predict user behavior, detect communities, and recommend friends or content. In the field of chemistry, GNNs are used to predict molecular properties, aiding in drug discovery and material science. Knowledge graphs, which underpin many search engines and recommendation systems, benefit from GNNs through improved entity recognition and relationship extraction. Additionally, GNNs have been employed in transportation for traffic prediction and route optimization, in finance for fraud detection and risk assessment, and in biology for protein structure prediction and interaction modeling. The versatility of GNNs in handling diverse and complex data structures makes them invaluable across various domains.\\n\\nChallenges and Limitations\\nDespite their powerful capabilities, Graph Neural Networks face several challenges and limitations. One major challenge is scalability, as the computational complexity of GNNs can grow rapidly with the size of the graph, making it difficult to apply them to large-scale graphs. Efficient sampling and approximation methods are required to address this issue. Another challenge is the over-smoothing problem, where repeated aggregations can cause node representations to become indistinguishable, especially in deep GNNs. Addressing this requires careful design of the network architecture and training strategies. Additionally, graph data can be highly heterogeneous and dynamic, posing challenges in creating models that can adapt to varying graph structures and temporal changes. Lastly, GNNs, like other AI models, face issues related to interpretability and explainability, making it difficult to understand how predictions are made, which is crucial for trust and deployment in critical applications.\\n\\nFuture Directions\\nThe future of Graph Neural Networks is promising, with ongoing research focused on overcoming current limitations and expanding their applicability. One exciting direction is the development of more scalable GNN architectures, capable of handling massive graphs with billions of nodes and edges. Techniques such as graph sampling, mini-batching, and distributed computing are being explored to achieve this. Another important area is improving the robustness and generalization of GNNs to various types of graphs and tasks, including those involving dynamic and heterogeneous data. Advances in explainability and interpretability are also crucial, with efforts to develop methods that provide insights into the decision-making process of GNNs. Integration of GNNs with other AI and machine learning paradigms, such as reinforcement learning and unsupervised learning, is expected to create more powerful hybrid models. Moreover, the advent of quantum computing holds potential for further enhancing the capabilities of GNNs, enabling them to solve even more complex problems efficiently. As these advancements continue, GNNs are poised to become an even more integral part of the AI and machine learning landscape.\\n0|Introduction to Machine Learning\\nMachine learning (ML) is a subset of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead, these systems learn from data patterns and make decisions based on their insights. This paradigm shift has revolutionized various industries by allowing for automated decision-making, predictive analytics, and advanced data processing capabilities. The core idea behind machine learning is to construct algorithms that can receive input data and use statistical analysis to predict an output value within an acceptable range. This iterative learning process improves the accuracy and efficiency of the algorithms, making them highly valuable in complex problem-solving scenarios.\\n\\nTypes of Machine Learning\\nMachine learning can be broadly categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves training a model on a labeled dataset, meaning that each training example is paired with an output label. This type of learning is typically used for classification and regression tasks. In contrast, unsupervised learning deals with unlabeled data, and the system tries to learn the underlying structure from the input data. Techniques like clustering and dimensionality reduction are common in this category. Reinforcement learning, on the other hand, focuses on training models to make a sequence of decisions by rewarding or penalizing them based on the actions taken. This approach is particularly useful in environments where the decision-making process is complex, such as robotics and game playing.\\n\\nApplications of Machine Learning\\nMachine learning has a wide array of applications across different domains. In healthcare, it is used for predictive diagnostics, personalized treatment plans, and drug discovery. Financial services leverage machine learning for fraud detection, credit scoring, and algorithmic trading. In the realm of e-commerce, recommendation systems powered by ML algorithms enhance user experience by suggesting relevant products. Additionally, machine learning plays a critical role in natural language processing (NLP) tasks such as speech recognition, language translation, and sentiment analysis. Autonomous vehicles, powered by sophisticated ML models, are becoming increasingly prevalent, showcasing the transformative potential of machine learning in transportation.\\n\\nChallenges and Limitations\\nDespite its numerous advantages, machine learning faces several challenges and limitations. One significant issue is the quality and quantity of data available for training. High-quality, large datasets are crucial for developing accurate and reliable models, but such data can be difficult to obtain. Another challenge is the interpretability of models, especially those that are highly complex like deep neural networks. These models often act as \"black boxes,\" making it difficult to understand how they arrive at specific decisions. Additionally, ethical considerations such as data privacy, security, and bias in AI systems are critical concerns that need to be addressed. Ensuring that ML systems are transparent, fair, and secure is essential for their widespread adoption and trustworthiness.\\n\\nFuture Directions\\nThe future of machine learning holds immense promise as research and development continue to advance. One exciting direction is the integration of machine learning with other AI disciplines, such as computer vision and NLP, to create more comprehensive and intelligent systems. Moreover, advancements in quantum computing are expected to significantly enhance the processing capabilities of ML algorithms, enabling them to tackle even more complex problems. The development of explainable AI (XAI) aims to make machine learning models more transparent and understandable, which is crucial for gaining user trust and meeting regulatory requirements. Furthermore, ongoing efforts to improve data efficiency, model robustness, and ethical standards will shape the future landscape of machine learning, making it an indispensable tool in various fields.\\n\\n\\n\\n---Goal---\\n\\nGenerate a response of the target length and format that responds to the user\\'s question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\\n\\nIf you don\\'t know the answer, just say so. Do not make anything up.\\n\\nPoints supported by data should list their data references as follows:\\n\\n\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\\n\\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\\n\\nFor example:\\n\\n\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\"\\n\\nwhere 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\\n\\nDo not include information where the supporting evidence for it is not provided.\\n\\n\\n---Target response length and format---\\n\\nMultiple Paragraphs\\n\\nAdd sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\\n'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af994a2b-bed7-4c8c-a3b8-26cdb01a6aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Role---\n",
      "\n",
      "You are a helpful assistant responding to questions about data in the tables provided.\n",
      "\n",
      "\n",
      "---Goal---\n",
      "\n",
      "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n",
      "\n",
      "If you don't know the answer, just say so. Do not make anything up.\n",
      "\n",
      "Points supported by data should list their data references as follows:\n",
      "\n",
      "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
      "\n",
      "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
      "\n",
      "For example:\n",
      "\n",
      "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\"\n",
      "\n",
      "where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
      "\n",
      "Do not include information where the supporting evidence for it is not provided.\n",
      "\n",
      "\n",
      "---Target response length and format---\n",
      "\n",
      "Multiple Paragraphs\n",
      "\n",
      "\n",
      "---Data tables---\n",
      "\n",
      "-----Reports-----\n",
      "id|title|content\n",
      "5|Artificial Intelligence and Convolutional Neural Networks|\"# Artificial Intelligence and Convolutional Neural Networks\n",
      "\n",
      "The community revolves around Artificial Intelligence and its subsets, including Convolutional Neural Networks, which are used for various tasks such as image analysis and object detection. The community also includes related entities such as Natural Language Processing, Machine Learning, and specific neural network models like AlexNet and VGGNet.\n",
      "\n",
      "## Artificial Intelligence as the central entity\n",
      "\n",
      "Artificial Intelligence is the central entity in this community, with various subsets and related fields, including Machine Learning and Natural Language Processing. This entity is connected to other key entities, such as Convolutional Neural Networks, through relationships that highlight their significance in the community [Data: Entities (1), Relationships (0, 7); Entities (8)]. Artificial Intelligence has a broad scope, encompassing multiple approaches, including neural networks, to achieve its goals [Data: Entities (1)].\n",
      "\n",
      "## Convolutional Neural Networks' role in the community\n",
      "\n",
      "Convolutional Neural Networks are a crucial entity in this community, with various applications, including image analysis and object detection. They are connected to other key entities, such as AlexNet, VGGNet, and ResNet, through relationships that highlight their significance in the community [Data: Entities (30), Relationships (47, 48, 49); Relationships (50, 51)]. Convolutional Neural Networks can be combined with other neural networks, such as Recurrent Neural Networks and Transformers, for tasks involving sequential data [Data: Relationships (55, 56)].\n",
      "\n",
      "## Natural Language Processing' connection to the community\n",
      "\n",
      "Natural Language Processing is a related entity in this community, connected to Artificial Intelligence and Machine Learning through relationships that highlight its significance in the community [Data: Entities (8), Relationships (7)]. Natural Language Processing uses machine learning for various tasks, including speech recognition and text analysis [Data: Entities (8)].\n",
      "\n",
      "## Neural network models' significance in the community\n",
      "\n",
      "Specific neural network models, such as AlexNet, VGGNet, and ResNet, are significant entities in this community, connected to Convolutional Neural Networks through relationships that highlight their importance [Data: Entities (47, 48, 49), Relationships (47, 48, 49)]. These models have achieved state-of-the-art performance on benchmark datasets, such as ImageNet [Data: Entities (52)].\n",
      "\n",
      "## Applications of Convolutional Neural Networks\n",
      "\n",
      "Convolutional Neural Networks have various applications, including medical imaging, genomics, and climate science, which are highlighted through relationships with entities such as Medical Imaging, Genomics, and Climate Science [Data: Relationships (60, 57, 58)]. These applications demonstrate the significant influence of Convolutional Neural Networks on various fields [Data: Relationships (60, 57, 58)].\"\n",
      "1|Transformer Neural Networks and Related Entities|\"# Transformer Neural Networks and Related Entities\n",
      "\n",
      "The community revolves around Transformer Neural Networks, which is a type of deep learning architecture used for natural language processing and other tasks. It has relationships with various entities such as Vaswani, NLP, BERT, GPT, T5, Vision Transformer, Reformer, Linformer, Longformer, RNN, CNN, Deep Learning, and Protein Folding, all of which are associated with the application or development of transformer neural networks. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\n",
      "\n",
      "## Transformer Neural Networks as the central entity\n",
      "\n",
      "Transformer Neural Networks is the central entity in this community, being a type of deep learning architecture used for natural language processing and other tasks. [Data: Entities (64); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)] The significance of transformer neural networks can be seen in its relationships with various entities such as Vaswani, NLP, BERT, GPT, T5, Vision Transformer, Reformer, Linformer, Longformer, RNN, CNN, Deep Learning, and Protein Folding. [Data: Entities (65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\n",
      "\n",
      "## Vaswani's role in introducing transformer neural networks\n",
      "\n",
      "Vaswani is the author of the seminal paper 'Attention is All You Need' that introduced transformer neural networks. [Data: Entities (65); Relationships (68)] This introduction has had a significant impact on the development of natural language processing and other fields. [Data: Entities (66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\n",
      "\n",
      "## Application of transformer neural networks in NLP\n",
      "\n",
      "Transformer neural networks have been widely applied in the field of natural language processing. [Data: Entities (66); Relationships (69, 70, 71, 72)] This application can be seen in models such as BERT, GPT, and T5, which use transformer neural networks for text classification, text generation, and text-to-text transfer tasks. [Data: Entities (67, 68, 69); Relationships (70, 71, 72)]\n",
      "\n",
      "## Relationship between transformer neural networks and deep learning\n",
      "\n",
      "Transformer neural networks are a type of deep learning architecture. [Data: Entities (64, 76); Relationships (81)] This relationship highlights the significance of transformer neural networks in the development of deep learning techniques. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\n",
      "\n",
      "## Application of transformer neural networks in protein folding\n",
      "\n",
      "Transformer neural networks have been applied in the field of protein folding. [Data: Entities (77); Relationships (84)] This application demonstrates the potential of transformer neural networks in solving complex problems in biology. [Data: Entities (64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77); Relationships (68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 84, +more)]\"\n",
      "\n",
      "\n",
      "-----Entities-----\n",
      "id|entity|description|number of relationships\n",
      "75|CNN|Convolutional Neural Networks are a type of deep learning architecture|1\n",
      "74|RNN|Recurrent Neural Networks are a type of deep learning architecture|1\n",
      "30|CONVOLUTIONAL NEURAL NETWORKS|Convolutional Neural Networks are a type of neural network designed for processing grid data, such as images. Additionally, they can be extended by Graph Convolutional Networks to also process graph data, thereby broadening their applicability beyond traditional image processing to more complex data structures. This versatility makes Convolutional Neural Networks a fundamental component in various deep learning applications, allowing them to handle a wide range of data types, from images to graphs, through their extensions and related network architectures.|22\n",
      "21|DEEP NEURAL NETWORKS|Deep neural networks are a type of machine learning model|1\n",
      "62|GPU|GPU is a type of hardware used for training Convolutional Neural Networks|1\n",
      "76|DEEP LEARNING|Deep Learning is a field where transformer neural networks have been widely applied|1\n",
      "46|LECUN|LeCun is a researcher who popularized Convolutional Neural Networks through the development of LeNet for digit recognition|2\n",
      "59|LENET|LeNet is a convolutional neural network model developed by LeCun for digit recognition|1\n",
      "55|RECURRENT NEURAL NETWORKS|Recurrent Neural Networks are a type of neural network designed for processing sequential data|1\n",
      "50|YOLO|YOLO is an object detection framework that leverages Convolutional Neural Networks to identify and localize objects within images|1\n",
      "27|GRAPH CONVOLUTIONAL NETWORKS|Graph Convolutional Networks are a type of Graph Neural Network that extend the concept of convolutional neural networks to graph data|2\n",
      "51|FASTER R-CNN|Faster R-CNN is an object detection framework that leverages Convolutional Neural Networks to identify and localize objects within images|1\n",
      "56|TRANSFORMERS|Transformers are a type of neural network designed for processing sequential data|1\n",
      "52|IMAGENET|ImageNet is a benchmark dataset for image classification tasks|1\n",
      "63|TPU|TPU is a type of hardware used for training Convolutional Neural Networks|1\n",
      "26|GRAPH NEURAL NETWORKS|Graph Neural Networks are a class of machine learning algorithms designed to perform inference on data represented as graphs|14\n",
      "66|NLP|Natural Language Processing is a field where transformer neural networks have been widely applied|1\n",
      "48|VGGNET|VGGNet is a convolutional neural network model that achieved state-of-the-art performance on benchmark datasets such as ImageNet|1\n",
      "20|SENTIMENT ANALYSIS|Sentiment analysis is a task that uses machine learning to analyze sentiment in text|1\n",
      "64|TRANSFORMER NEURAL NETWORKS|Transformer neural networks are a type of deep learning architecture used for natural language processing and other tasks|18\n",
      "\n",
      "\n",
      "-----Relationships-----\n",
      "id|source|target|description|weight|links\n",
      "39|GRAPH CONVOLUTIONAL NETWORKS|CONVOLUTIONAL NEURAL NETWORKS|Graph Convolutional Networks extend Convolutional Neural Networks to graph data|18.0|1\n",
      "46|CONVOLUTIONAL NEURAL NETWORKS|LECUN|LeCun popularized Convolutional Neural Networks through the development of LeNet for digit recognition|16.0|1\n",
      "48|CONVOLUTIONAL NEURAL NETWORKS|VGGNET|VGGNet is a type of Convolutional Neural Network model|12.0|1\n",
      "50|CONVOLUTIONAL NEURAL NETWORKS|YOLO|YOLO is an object detection framework that leverages Convolutional Neural Networks|14.0|1\n",
      "51|CONVOLUTIONAL NEURAL NETWORKS|FASTER R-CNN|Faster R-CNN is an object detection framework that leverages Convolutional Neural Networks|14.0|1\n",
      "52|CONVOLUTIONAL NEURAL NETWORKS|IMAGENET|Convolutional Neural Networks are often trained on ImageNet|10.0|1\n",
      "55|CONVOLUTIONAL NEURAL NETWORKS|RECURRENT NEURAL NETWORKS|Convolutional Neural Networks can be combined with Recurrent Neural Networks for tasks involving sequential data|6.0|1\n",
      "56|CONVOLUTIONAL NEURAL NETWORKS|TRANSFORMERS|Convolutional Neural Networks can be combined with Transformers for tasks involving sequential data|6.0|1\n",
      "64|CONVOLUTIONAL NEURAL NETWORKS|GPU|Convolutional Neural Networks are often trained on GPU hardware|5.0|1\n",
      "65|CONVOLUTIONAL NEURAL NETWORKS|TPU|Convolutional Neural Networks are often trained on TPU hardware|5.0|1\n",
      "69|TRANSFORMER NEURAL NETWORKS|NLP|Transformer neural networks have been widely applied in the field of natural language processing|16.0|4\n",
      "77|TRANSFORMER NEURAL NETWORKS|RNN|RNN is a type of deep learning architecture that is different from transformer neural networks|6.0|4\n",
      "78|TRANSFORMER NEURAL NETWORKS|CNN|CNN is a type of deep learning architecture that is different from transformer neural networks|6.0|4\n",
      "81|TRANSFORMER NEURAL NETWORKS|DEEP LEARNING|Transformer neural networks are a type of deep learning architecture|9.0|4\n",
      "25|GRAPH NEURAL NETWORKS|GRAPH CONVOLUTIONAL NETWORKS|Graph Neural Networks include Graph Convolutional Networks as a type|16.0|1\n",
      "67|LECUN|LENET|LeCun developed LeNet for digit recognition|9.0|1\n",
      "63|CONVOLUTIONAL NEURAL NETWORKS|MACHINE LEARNING|Convolutional Neural Networks are a key component of machine learning|9.0|5\n",
      "80|TRANSFORMER NEURAL NETWORKS|MACHINE LEARNING|Transformer neural networks have been widely applied in the field of machine learning|8.0|5\n",
      "35|GRAPH NEURAL NETWORKS|MACHINE LEARNING|Graph Neural Networks are a class of machine learning algorithms|9.0|5\n",
      "20|MACHINE LEARNING|SENTIMENT ANALYSIS|Sentiment analysis uses machine learning to analyze sentiment in text|8.0|5\n",
      "21|MACHINE LEARNING|DEEP NEURAL NETWORKS|Deep neural networks are a type of machine learning model|9.0|5\n",
      "59|CONVOLUTIONAL NEURAL NETWORKS|COMPUTER VISION|Convolutional Neural Networks are a key component of computer vision|9.0|2\n",
      "82|TRANSFORMER NEURAL NETWORKS|COMPUTER VISION|Transformer neural networks have been applied in the field of computer vision|7.0|2\n",
      "85|TRANSFORMER NEURAL NETWORKS|REINFORCEMENT LEARNING|Transformer neural networks have been applied in the field of reinforcement learning|1.0|2\n",
      "79|TRANSFORMER NEURAL NETWORKS|AI|Transformer neural networks have been widely applied in the field of artificial intelligence|8.0|2\n",
      "36|GRAPH NEURAL NETWORKS|REINFORCEMENT LEARNING|Graph Neural Networks can be integrated with reinforcement learning to create hybrid models|5.0|2\n",
      "38|GRAPH NEURAL NETWORKS|AI|Graph Neural Networks are a part of the AI landscape|8.0|2\n",
      "61|CONVOLUTIONAL NEURAL NETWORKS|NATURAL LANGUAGE PROCESSING|Convolutional Neural Networks can be used in natural language processing for text analysis|6.0|1\n",
      "62|CONVOLUTIONAL NEURAL NETWORKS|ARTIFICIAL INTELLIGENCE|Convolutional Neural Networks are a key component of artificial intelligence|9.0|1\n",
      "47|CONVOLUTIONAL NEURAL NETWORKS|ALEXNET|AlexNet is a type of Convolutional Neural Network model|12.0|1\n",
      "49|CONVOLUTIONAL NEURAL NETWORKS|RESNET|ResNet is a type of Convolutional Neural Network model|12.0|1\n",
      "53|CONVOLUTIONAL NEURAL NETWORKS|MOBILENETS|MobileNets are a type of efficient Convolutional Neural Network architecture|8.0|1\n",
      "54|CONVOLUTIONAL NEURAL NETWORKS|EFFICIENTNETS|EfficientNets are a type of efficient Convolutional Neural Network architecture|8.0|1\n",
      "57|CONVOLUTIONAL NEURAL NETWORKS|GENOMICS|Convolutional Neural Networks can be applied to genomics for tasks such as image analysis|4.0|1\n",
      "58|CONVOLUTIONAL NEURAL NETWORKS|CLIMATE SCIENCE|Convolutional Neural Networks can be applied to climate science for tasks such as image analysis|3.0|1\n",
      "60|CONVOLUTIONAL NEURAL NETWORKS|MEDICAL IMAGING|Convolutional Neural Networks are used in medical imaging for analyzing medical images|8.0|1\n",
      "66|CONVOLUTIONAL NEURAL NETWORKS|NEURAL ARCHITECTURE SEARCH|Neural Architecture Search is used to automate the design of Convolutional Neural Networks|1.0|1\n",
      "83|TRANSFORMER NEURAL NETWORKS|SPEECH RECOGNITION|Transformer neural networks have been applied in the field of speech recognition|7.0|1\n",
      "68|TRANSFORMER NEURAL NETWORKS|VASWANI|Vaswani introduced transformer neural networks in the paper \"Attention is All You Need\"|18.0|1\n",
      "70|TRANSFORMER NEURAL NETWORKS|BERT|BERT uses transformer neural networks for text classification and other NLP tasks|18.0|1\n",
      "71|TRANSFORMER NEURAL NETWORKS|GPT|GPT uses transformer neural networks for text generation and other NLP tasks|18.0|1\n",
      "72|TRANSFORMER NEURAL NETWORKS|T5|T5 uses transformer neural networks for text-to-text transfer and other NLP tasks|18.0|1\n",
      "73|TRANSFORMER NEURAL NETWORKS|VISION TRANSFORMER|Vision Transformer applies transformer neural networks to computer vision tasks|16.0|1\n",
      "74|TRANSFORMER NEURAL NETWORKS|REFORMER|Reformer aims to make transformer neural networks more efficient|14.0|1\n",
      "75|TRANSFORMER NEURAL NETWORKS|LINFORMER|Linformer aims to make transformer neural networks more efficient|14.0|1\n",
      "76|TRANSFORMER NEURAL NETWORKS|LONGFORMER|Longformer aims to make transformer neural networks more efficient|8.0|1\n",
      "84|TRANSFORMER NEURAL NETWORKS|PROTEIN FOLDING|Transformer neural networks have been applied in the field of protein folding|7.0|1\n",
      "30|GRAPH NEURAL NETWORKS|KNOWLEDGE GRAPHS|Graph Neural Networks improve entity recognition and relationship extraction in knowledge graphs|10.0|1\n",
      "34|GRAPH NEURAL NETWORKS|QUANTUM COMPUTING|Quantum computing holds potential for further enhancing the capabilities of Graph Neural Networks|3.0|1\n",
      "37|GRAPH NEURAL NETWORKS|UNSUPERVISED LEARNING|Graph Neural Networks can be integrated with unsupervised learning to create hybrid models|5.0|1\n",
      "26|GRAPH NEURAL NETWORKS|GRAPH ATTENTION NETWORKS|Graph Neural Networks include Graph Attention Networks as a type|16.0|1\n",
      "27|GRAPH NEURAL NETWORKS|GRAPH RECURRENT NEURAL NETWORKS|Graph Neural Networks include Graph Recurrent Neural Networks as a type|16.0|1\n",
      "28|GRAPH NEURAL NETWORKS|SOCIAL NETWORKS|Graph Neural Networks can be applied to social networks|10.0|1\n",
      "29|GRAPH NEURAL NETWORKS|CHEMISTRY|Graph Neural Networks are used in chemistry to predict molecular properties|10.0|1\n",
      "31|GRAPH NEURAL NETWORKS|TRANSPORTATION|Graph Neural Networks are employed in transportation for traffic prediction and route optimization|10.0|1\n",
      "32|GRAPH NEURAL NETWORKS|FINANCE|Graph Neural Networks are employed in finance for fraud detection and risk assessment|10.0|1\n",
      "33|GRAPH NEURAL NETWORKS|BIOLOGY|Graph Neural Networks are employed in biology for protein structure prediction and interaction modeling|10.0|1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----Sources-----\n",
      "id|text\n",
      "3|Introduction to Transformer Neural Networks\n",
      "Transformer neural networks represent a revolutionary architecture in the field of deep learning, particularly for natural language processing (NLP) tasks. Introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al. in 2017, transformers have since become the backbone of numerous state-of-the-art models due to their ability to handle long-range dependencies and parallelize training processes. Unlike traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), transformers rely entirely on a mechanism called self-attention to process input data. This mechanism allows transformers to weigh the importance of different words in a sentence or elements in a sequence simultaneously, thus capturing context more effectively and efficiently.\n",
      "\n",
      "Architecture of Transformers\n",
      "The core component of the transformer architecture is the self-attention mechanism, which enables the model to focus on different parts of the input sequence when producing an output. The transformer consists of an encoder and a decoder, each made up of a stack of identical layers. The encoder processes the input sequence and generates a set of attention-weighted vectors, while the decoder uses these vectors, along with the previously generated outputs, to produce the final sequence. Each layer in the encoder and decoder contains sub-layers, including multi-head self-attention mechanisms and position-wise fully connected feed-forward networks, followed by layer normalization and residual connections. This design allows the transformer to process entire sequences at once rather than step-by-step, making it highly parallelizable and efficient for training on large datasets.\n",
      "\n",
      "Applications of Transformer Neural Networks\n",
      "Transformers have revolutionized various applications across different domains. In NLP, they power models like BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and T5 (Text-to-Text Transfer Transformer), which excel in tasks such as text classification, machine translation, question answering, and text generation. Beyond NLP, transformers have also shown remarkable performance in computer vision with models like Vision Transformer (ViT), which treats images as sequences of patches, similar to words in a sentence. Additionally, transformers are being explored in areas such as speech recognition, protein folding, and reinforcement learning, demonstrating their versatility and robustness in handling diverse types of data. The ability to process long-range dependencies and capture intricate patterns has made transformers indispensable in advancing the state of the art in many machine learning tasks.\n",
      "\n",
      "Challenges and Limitations\n",
      "Despite their success, transformer neural networks come with several challenges and limitations. One of the primary concerns is their computational and memory requirements, which are significantly higher compared to traditional models. The quadratic complexity of the self-attention mechanism with respect to the input sequence length can lead to inefficiencies, especially when dealing with very long sequences. To mitigate this, various approaches like sparse attention and efficient transformers have been proposed. Another challenge is the interpretability of transformers, as the attention mechanisms, though providing some insights, do not fully explain the model's decisions. Furthermore, transformers require large amounts of data and computational resources for training, which can be a barrier for smaller organizations or those with limited resources. Addressing these challenges is crucial for making transformers more accessible and scalable for a broader range of applications.\n",
      "\n",
      "Future Directions\n",
      "The future of transformer neural networks is bright, with ongoing research focused on enhancing their efficiency, scalability, and applicability. One promising direction is the development of more efficient transformer architectures that reduce computational complexity and memory usage, such as the Reformer, Linformer, and Longformer. These models aim to make transformers feasible for longer sequences and real-time applications. Another important area is improving the interpretability of transformers, with efforts to develop methods that provide clearer explanations of their decision-making processes. Additionally, integrating transformers with other neural network architectures, such as combining them with convolutional networks for multimodal tasks, holds significant potential. The application of transformers beyond traditional domains, like in time-series forecasting, healthcare, and finance, is also expected to grow. As advancements continue, transformers are set to remain at the forefront of AI and machine learning, driving innovation and breakthroughs across various fields.\n",
      "2|Introduction to Convolutional Neural Networks\n",
      "Convolutional Neural Networks (CNNs) are a specialized type of neural network designed primarily for processing structured grid data, such as images. Inspired by the visual cortex of animals, CNNs have become the cornerstone of computer vision applications due to their ability to automatically and adaptively learn spatial hierarchies of features. Introduced in the 1980s and popularized by LeCun et al. through the development of LeNet for digit recognition, CNNs have since evolved and expanded into various fields, achieving remarkable success in image classification, object detection, and segmentation tasks. The architecture of CNNs leverages convolutional layers to efficiently capture local patterns and features in data, making them highly effective for tasks involving high-dimensional inputs like images.\n",
      "\n",
      "Architecture of Convolutional Neural Networks\n",
      "The architecture of a typical Convolutional Neural Network consists of several key components: convolutional layers, pooling layers, and fully connected layers. The convolutional layer is the core building block, where filters (or kernels) slide over the input data to produce feature maps. These filters are trained to recognize various patterns such as edges, textures, and shapes. Following convolutional layers, pooling layers (such as max pooling) are used to reduce the spatial dimensions of the feature maps, thereby decreasing the computational load and controlling overfitting. The fully connected layers, typically at the end of the network, take the high-level filtered and pooled features and perform the final classification or regression task. CNNs also often incorporate activation functions like ReLU (Rectified Linear Unit) and regularization techniques such as dropout to enhance performance and prevent overfitting.\n",
      "\n",
      "Applications of Convolutional Neural Networks\n",
      "Convolutional Neural Networks have revolutionized various applications across multiple domains. In computer vision, CNNs are the backbone of image classification models like AlexNet, VGGNet, and ResNet, which have achieved state-of-the-art performance on benchmark datasets such as ImageNet. Object detection frameworks like YOLO (You Only Look Once) and Faster R-CNN leverage CNNs to identify and localize objects within images. In medical imaging, CNNs assist in diagnosing diseases by analyzing X-rays, MRIs, and CT scans. Beyond vision tasks, CNNs are employed in natural language processing for tasks like sentence classification and text generation when combined with other architectures. Additionally, CNNs are used in video analysis, facial recognition systems, and even in autonomous vehicles for tasks like lane detection and obstacle recognition. Their ability to automatically learn and extract relevant features from raw data has made CNNs indispensable in advancing artificial intelligence technologies.\n",
      "\n",
      "Challenges and Limitations\n",
      "Despite their impressive capabilities, Convolutional Neural Networks face several challenges and limitations. One major issue is their computational intensity, which requires significant processing power and memory, especially for deeper and more complex networks. Training large CNNs often necessitates specialized hardware such as GPUs or TPUs. Another challenge is the need for large labeled datasets to train effectively, which can be a limiting factor in domains where data is scarce or expensive to annotate. Additionally, CNNs can struggle with understanding global context due to their inherent focus on local features, making them less effective for tasks requiring long-range dependencies. Transfer learning and data augmentation techniques are commonly used to mitigate some of these issues, but they do not fully address the fundamental challenges. Interpretability is also a concern, as CNNs are often viewed as black-box models, making it difficult to understand the rationale behind their predictions. Enhancing the transparency and efficiency of CNNs remains an active area of research.\n",
      "\n",
      "Future Directions\n",
      "The future of Convolutional Neural Networks holds exciting possibilities as researchers continue to innovate and address existing limitations. One promising direction is the development of more efficient architectures, such as MobileNets and EfficientNets, which aim to reduce computational complexity while maintaining high performance. Advances in neural architecture search (NAS) allow for automated design of optimized network structures tailored to specific tasks and hardware constraints. Integrating CNNs with other neural network types, such as recurrent neural networks (RNNs) or transformers, can lead to more powerful hybrid models capable of handling diverse data types and tasks. Additionally, the application of CNNs is expanding into new fields such as genomics, climate science, and even art generation, demonstrating their versatility. Efforts to improve the interpretability of CNNs, such as developing techniques for visualizing and understanding the learned filters and feature maps, are also crucial for building trust and transparency. As these advancements continue, CNNs are poised to remain a central tool in the ongoing evolution of artificial intelligence and machine learning.\n",
      "1|Introduction to Graph Neural Networks\n",
      "Graph Neural Networks (GNNs) are a class of machine learning algorithms designed to perform inference on data represented as graphs. Unlike traditional neural networks that operate on grid-like data structures such as images or sequences, GNNs are specifically tailored to handle graph-structured data, where the relationships (edges) between entities (nodes) are paramount. This ability to incorporate both node features and graph topology into learning processes makes GNNs incredibly powerful for a variety of applications. From social networks and molecular structures to knowledge graphs and recommendation systems, GNNs leverage the inherent structure of graphs to capture complex patterns and dependencies.\n",
      "\n",
      "Types of Graph Neural Networks\n",
      "There are several types of Graph Neural Networks, each designed to address specific aspects of graph data. The most common types include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Graph Recurrent Neural Networks (GRNNs). GCNs extend the concept of convolutional neural networks (CNNs) to graph data by performing convolutions on nodes, aggregating features from neighboring nodes to learn node embeddings. GATs enhance this process by incorporating attention mechanisms, allowing the model to weigh the importance of different neighbors differently during the aggregation process. GRNNs, on the other hand, utilize recurrent neural network architectures to capture temporal or sequential dependencies in dynamic graphs. Each of these models has unique strengths, enabling them to be applied effectively to various graph-related tasks.\n",
      "\n",
      "Applications of Graph Neural Networks\n",
      "Graph Neural Networks have found applications in numerous fields, revolutionizing how we process and interpret graph-structured data. In social network analysis, GNNs can predict user behavior, detect communities, and recommend friends or content. In the field of chemistry, GNNs are used to predict molecular properties, aiding in drug discovery and material science. Knowledge graphs, which underpin many search engines and recommendation systems, benefit from GNNs through improved entity recognition and relationship extraction. Additionally, GNNs have been employed in transportation for traffic prediction and route optimization, in finance for fraud detection and risk assessment, and in biology for protein structure prediction and interaction modeling. The versatility of GNNs in handling diverse and complex data structures makes them invaluable across various domains.\n",
      "\n",
      "Challenges and Limitations\n",
      "Despite their powerful capabilities, Graph Neural Networks face several challenges and limitations. One major challenge is scalability, as the computational complexity of GNNs can grow rapidly with the size of the graph, making it difficult to apply them to large-scale graphs. Efficient sampling and approximation methods are required to address this issue. Another challenge is the over-smoothing problem, where repeated aggregations can cause node representations to become indistinguishable, especially in deep GNNs. Addressing this requires careful design of the network architecture and training strategies. Additionally, graph data can be highly heterogeneous and dynamic, posing challenges in creating models that can adapt to varying graph structures and temporal changes. Lastly, GNNs, like other AI models, face issues related to interpretability and explainability, making it difficult to understand how predictions are made, which is crucial for trust and deployment in critical applications.\n",
      "\n",
      "Future Directions\n",
      "The future of Graph Neural Networks is promising, with ongoing research focused on overcoming current limitations and expanding their applicability. One exciting direction is the development of more scalable GNN architectures, capable of handling massive graphs with billions of nodes and edges. Techniques such as graph sampling, mini-batching, and distributed computing are being explored to achieve this. Another important area is improving the robustness and generalization of GNNs to various types of graphs and tasks, including those involving dynamic and heterogeneous data. Advances in explainability and interpretability are also crucial, with efforts to develop methods that provide insights into the decision-making process of GNNs. Integration of GNNs with other AI and machine learning paradigms, such as reinforcement learning and unsupervised learning, is expected to create more powerful hybrid models. Moreover, the advent of quantum computing holds potential for further enhancing the capabilities of GNNs, enabling them to solve even more complex problems efficiently. As these advancements continue, GNNs are poised to become an even more integral part of the AI and machine learning landscape.\n",
      "0|Introduction to Machine Learning\n",
      "Machine learning (ML) is a subset of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead, these systems learn from data patterns and make decisions based on their insights. This paradigm shift has revolutionized various industries by allowing for automated decision-making, predictive analytics, and advanced data processing capabilities. The core idea behind machine learning is to construct algorithms that can receive input data and use statistical analysis to predict an output value within an acceptable range. This iterative learning process improves the accuracy and efficiency of the algorithms, making them highly valuable in complex problem-solving scenarios.\n",
      "\n",
      "Types of Machine Learning\n",
      "Machine learning can be broadly categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves training a model on a labeled dataset, meaning that each training example is paired with an output label. This type of learning is typically used for classification and regression tasks. In contrast, unsupervised learning deals with unlabeled data, and the system tries to learn the underlying structure from the input data. Techniques like clustering and dimensionality reduction are common in this category. Reinforcement learning, on the other hand, focuses on training models to make a sequence of decisions by rewarding or penalizing them based on the actions taken. This approach is particularly useful in environments where the decision-making process is complex, such as robotics and game playing.\n",
      "\n",
      "Applications of Machine Learning\n",
      "Machine learning has a wide array of applications across different domains. In healthcare, it is used for predictive diagnostics, personalized treatment plans, and drug discovery. Financial services leverage machine learning for fraud detection, credit scoring, and algorithmic trading. In the realm of e-commerce, recommendation systems powered by ML algorithms enhance user experience by suggesting relevant products. Additionally, machine learning plays a critical role in natural language processing (NLP) tasks such as speech recognition, language translation, and sentiment analysis. Autonomous vehicles, powered by sophisticated ML models, are becoming increasingly prevalent, showcasing the transformative potential of machine learning in transportation.\n",
      "\n",
      "Challenges and Limitations\n",
      "Despite its numerous advantages, machine learning faces several challenges and limitations. One significant issue is the quality and quantity of data available for training. High-quality, large datasets are crucial for developing accurate and reliable models, but such data can be difficult to obtain. Another challenge is the interpretability of models, especially those that are highly complex like deep neural networks. These models often act as \"black boxes,\" making it difficult to understand how they arrive at specific decisions. Additionally, ethical considerations such as data privacy, security, and bias in AI systems are critical concerns that need to be addressed. Ensuring that ML systems are transparent, fair, and secure is essential for their widespread adoption and trustworthiness.\n",
      "\n",
      "Future Directions\n",
      "The future of machine learning holds immense promise as research and development continue to advance. One exciting direction is the integration of machine learning with other AI disciplines, such as computer vision and NLP, to create more comprehensive and intelligent systems. Moreover, advancements in quantum computing are expected to significantly enhance the processing capabilities of ML algorithms, enabling them to tackle even more complex problems. The development of explainable AI (XAI) aims to make machine learning models more transparent and understandable, which is crucial for gaining user trust and meeting regulatory requirements. Furthermore, ongoing efforts to improve data efficiency, model robustness, and ethical standards will shape the future landscape of machine learning, making it an indispensable tool in various fields.\n",
      "\n",
      "\n",
      "\n",
      "---Goal---\n",
      "\n",
      "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n",
      "\n",
      "If you don't know the answer, just say so. Do not make anything up.\n",
      "\n",
      "Points supported by data should list their data references as follows:\n",
      "\n",
      "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
      "\n",
      "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
      "\n",
      "For example:\n",
      "\n",
      "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\"\n",
      "\n",
      "where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
      "\n",
      "Do not include information where the supporting evidence for it is not provided.\n",
      "\n",
      "\n",
      "---Target response length and format---\n",
      "\n",
      "Multiple Paragraphs\n",
      "\n",
      "Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(b['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7354907e-4f4d-4090-a9ae-f4803ef5a555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_path: /scratch/gpfs/jx0800/data/graphrag/settings.yaml\n"
     ]
    }
   ],
   "source": [
    "from graphrag.config.load_config import load_config\n",
    "from pathlib import Path\n",
    "root = Path(\"/scratch/gpfs/jx0800/data/graphrag\")\n",
    "config = load_config(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3aeede8-4ae0-4ea0-9fd2-8e848c03d0bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Role---\n",
      "\n",
      "You are a helpful assistant responding to questions about data in the tables provided.\n",
      "\n",
      "\n",
      "---Data tables---\n",
      "\n",
      "{context_data}\n",
      "\n",
      "\n",
      "---Goal---\n",
      "\n",
      "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n",
      "\n",
      "If you don't know the answer, just say so. Do not make anything up.\n",
      "\n",
      "Points supported by data should list their data references as follows:\n",
      "\n",
      "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
      "\n",
      "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
      "\n",
      "For example:\n",
      "\n",
      "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\"\n",
      "\n",
      "where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
      "\n",
      "Do not include information where the supporting evidence for it is not provided.\n",
      "\n",
      "\n",
      "---Target response format---\n",
      "\n",
      "Response type: {response_type}\n",
      "\n",
      "Answer in the following format:\n",
      "\n",
      "**Summary of the Input Data Table**:  \n",
      "- Briefly summarize the information in the input data table that is relevant to the question.  \n",
      "- Explicitly reference data points that support your summary (e.g., [Data: Relationships (2, 3, 4); Entities (35, 36, 37, 39, 55, +more)]).  \n",
      "\n",
      "**Analysis of the question with references**: \n",
      "- Understand the question and find out what is needed to answer it\n",
      "- Analyze and answer the question using the input data table and the summary, reference data points in the given format (e.g., [Data: Entities (7, 8); Relationships (5)]).\n",
      "\n",
      "**Answer**:\n",
      "- Provide a concise answer using `\\boxed{{}}`, select only the correct letter (e.g., \\boxed{{C}}) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from graphrag.utils.api import load_search_prompt\n",
    "prompt = load_search_prompt(config.root_dir, config.local_search.prompt)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b73021d9-6a08-4573-96e4-0d46c8e5f52a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Role---\n",
      "\n",
      "You are a helpful assistant responding to questions about data in the tables provided.\n",
      "\n",
      "\n",
      "---Data tables---\n",
      "\n",
      "a\n",
      "\n",
      "\n",
      "---Goal---\n",
      "\n",
      "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n",
      "\n",
      "If you don't know the answer, just say so. Do not make anything up.\n",
      "\n",
      "Points supported by data should list their data references as follows:\n",
      "\n",
      "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
      "\n",
      "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
      "\n",
      "For example:\n",
      "\n",
      "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\"\n",
      "\n",
      "where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
      "\n",
      "Do not include information where the supporting evidence for it is not provided.\n",
      "\n",
      "\n",
      "---Target response format---\n",
      "\n",
      "Response type: b\n",
      "\n",
      "Answer in the following format:\n",
      "\n",
      "**Summary of the Input Data Table**:  \n",
      "- Briefly summarize the information in the input data table that is relevant to the question.  \n",
      "- Explicitly reference data points that support your summary (e.g., [Data: Relationships (2, 3, 4); Entities (35, 36, 37, 39, 55, +more)]).  \n",
      "\n",
      "**Analysis of the question with references**: \n",
      "- Understand the question and find out what is needed to answer it\n",
      "- Analyze and answer the question using the input data table and the summary, reference data points in the given format (e.g., [Data: Entities (7, 8); Relationships (5)]).\n",
      "\n",
      "**Answer**:\n",
      "- Provide a concise answer using `\\boxed{}`, select only the correct letter (e.g., \\boxed{C}) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(context_data='a',\n",
    "                    response_type='b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90cd6a6-f05e-450b-86a1-ae3da3148d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default_vector_store': {'type': 'lancedb', 'db_uri': '/scratch/gpfs/jx0800/data/graphrag/output/lancedb', 'url': None, 'api_key': None, 'audience': None, 'container_name': 'default', 'database_name': None, 'overwrite': True}}\n"
     ]
    }
   ],
   "source": [
    "vector_store_args = {}\n",
    "for index, store in config.vector_store.items():\n",
    "    vector_store_args[index] = store.model_dump()\n",
    "\n",
    "print(vector_store_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe83b6f-cc94-4abc-955e-5b603beab418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default_vector_store': {'type': 'lancedb', 'db_uri': '/scratch/gpfs/jx0800/data/graphrag/output/lancedb', 'url': None, 'api_key': None, 'audience': None, 'container_name': 'default', 'database_name': None, 'overwrite': True}}\n"
     ]
    }
   ],
   "source": [
    "vector_store_args = {}\n",
    "for index, store in config.vector_store.items():\n",
    "    vector_store_args[index] = store.model_dump()\n",
    "\n",
    "print(vector_store_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae2f79c-81ef-4afc-a47e-042f5a121fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  human_readable_id  \\\n",
      "0  4be5cc2c-a691-46ce-8ac5-c4b2308645d4                  0   \n",
      "1  41b58320-f499-4cb3-989f-4834ed0f8f9f                  1   \n",
      "2  8f86cdd7-ee5a-43ba-8992-283bc3e6ac2e                  2   \n",
      "3  85f92753-dbd8-4645-bc0c-bee1c8346b8a                  3   \n",
      "4  d8ece7e8-9ae0-420b-a24d-9d5dba8412fa                  4   \n",
      "\n",
      "                     title          type  \\\n",
      "0         MACHINE LEARNING  ORGANIZATION   \n",
      "1  ARTIFICIAL INTELLIGENCE  ORGANIZATION   \n",
      "2      SUPERVISED LEARNING         EVENT   \n",
      "3    UNSUPERVISED LEARNING         EVENT   \n",
      "4   REINFORCEMENT LEARNING         EVENT   \n",
      "\n",
      "                                         description  \\\n",
      "0  MACHINE LEARNING is a subset of artificial int...   \n",
      "1  Artificial Intelligence is a field of study th...   \n",
      "2  Supervised learning involves training a model ...   \n",
      "3  Unsupervised learning deals with unlabeled dat...   \n",
      "4  Reinforcement Learning is a field of study tha...   \n",
      "\n",
      "                                       text_unit_ids  degree  x  y  \n",
      "0  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...      20  0  0  \n",
      "1  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...       2  0  0  \n",
      "2  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...       1  0  0  \n",
      "3  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...       2  0  0  \n",
      "4  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...       4  0  0  \n",
      "id                                4be5cc2c-a691-46ce-8ac5-c4b2308645d4\n",
      "human_readable_id                                                    0\n",
      "title                                                 MACHINE LEARNING\n",
      "type                                                      ORGANIZATION\n",
      "description          MACHINE LEARNING is a subset of artificial int...\n",
      "text_unit_ids        [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...\n",
      "degree                                                              20\n",
      "x                                                                    0\n",
      "y                                                                    0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"/scratch/gpfs/jx0800/data/graphrag/output/entities.parquet\"\n",
    "\n",
    "# Read the Parquet file into a Pandas DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "print(df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8df4ca71-2528-4565-829d-fac7a06c4460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  human_readable_id            source  \\\n",
      "0  9abeb5f6-f8c1-4306-b2a9-24353e3c1128                  0  MACHINE LEARNING   \n",
      "1  e9474810-a4f7-4dfd-9852-ba710937f7a0                  1  MACHINE LEARNING   \n",
      "2  d31bdddd-a232-4003-a746-f844b93e0e98                  2  MACHINE LEARNING   \n",
      "3  72105a69-45cb-4b24-aa70-f8c0e2bc4367                  3  MACHINE LEARNING   \n",
      "4  a7fc2cd4-71b8-46bf-a21b-91ce526d9a93                  4  MACHINE LEARNING   \n",
      "\n",
      "                    target                                        description  \\\n",
      "0  ARTIFICIAL INTELLIGENCE  Machine learning is a subset of artificial int...   \n",
      "1      SUPERVISED LEARNING  Supervised learning is a type of machine learning   \n",
      "2    UNSUPERVISED LEARNING  Unsupervised learning is a type of machine lea...   \n",
      "3   REINFORCEMENT LEARNING  Reinforcement learning is a type of machine le...   \n",
      "4               HEALTHCARE  Healthcare uses machine learning for predictiv...   \n",
      "\n",
      "   weight  combined_degree                                      text_unit_ids  \n",
      "0    18.0               22  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...  \n",
      "1    16.0               21  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...  \n",
      "2    16.0               22  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...  \n",
      "3    16.0               24  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...  \n",
      "4    12.0               22  [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...  \n",
      "id                                a99a72b7-e7cd-49a0-b3e1-9a4e8cf08978\n",
      "human_readable_id                                                    5\n",
      "source                                                MACHINE LEARNING\n",
      "target                                              FINANCIAL SERVICES\n",
      "description          Financial services use machine learning for fr...\n",
      "weight                                                            12.0\n",
      "combined_degree                                                     21\n",
      "text_unit_ids        [e72fce0bdeb62991ec38c3b6cf3aa675f8eabcce43a30...\n",
      "Name: 5, dtype: object\n",
      "(81, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"/scratch/gpfs/jx0800/data/graphrag/output/relationships.parquet\"\n",
    "\n",
    "# Read the Parquet file into a Pandas DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "print(df.iloc[5])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80c043c3-0dff-4606-b91c-234d590ac595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from graphrag.query.input.loaders.utils import (\n",
    "    to_optional_dict,\n",
    "    to_optional_float,\n",
    "    to_optional_int,\n",
    "    to_optional_list,\n",
    "    to_optional_str,\n",
    "    to_str,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"/scratch/gpfs/jx0800/data/graphrag/output/entities.parquet\"\n",
    "\n",
    "# Read the Parquet file into a Pandas DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    a =to_optional_float(row, None)\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "907df9e4-7052-4d76-821e-2b10ceb9710b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132962 entries, 0 to 132961\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   id          132962 non-null  object\n",
      " 1   text        132962 non-null  object\n",
      " 2   vector      132962 non-null  object\n",
      " 3   attributes  132962 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 4.1+ MB\n",
      "None\n",
      "                                     id                               text  \\\n",
      "0  0e24ff61-4185-4892-97bb-1d1e0f597f51           intrauterine transfusion   \n",
      "1  91ac2a6d-4283-4fa2-95b6-e249582e7654                     blood material   \n",
      "2  c9cb6190-2ba9-40d0-a2b1-b55fced780c1              percutaneous approach   \n",
      "3  a9a5b55b-1cda-4147-a3d2-7b1eb54cf35f                    cryoprecipitate   \n",
      "4  47124696-7799-4653-ad7e-eaaecdf7b3e4  intrauterine exchange transfusion   \n",
      "\n",
      "                                              vector  \\\n",
      "0  [-0.0043643704, 0.04901778, 0.0012996075, -0.0...   \n",
      "1  [0.018517913, 0.001478622, -0.029236525, -0.04...   \n",
      "2  [0.049003832, -0.021690117, -0.014992222, -0.0...   \n",
      "3  [0.05303567, -0.00590552, 0.00805665, -0.04629...   \n",
      "4  [-0.010339196, 0.057096526, 0.003487452, -0.05...   \n",
      "\n",
      "                                       attributes  \n",
      "0           {'title': 'intrauterine transfusion'}  \n",
      "1                     {'title': 'blood material'}  \n",
      "2              {'title': 'percutaneous approach'}  \n",
      "3                    {'title': 'cryoprecipitate'}  \n",
      "4  {'title': 'intrauterine exchange transfusion'}  \n",
      "\n",
      "intrauterine transfusion\n",
      "\n",
      "{'title': 'intrauterine transfusion'}\n"
     ]
    }
   ],
   "source": [
    "import lance\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your LanceDB directory\n",
    "lance_path = \"/scratch/gpfs/jx0800/data/graphrag/output/lancedb/default-entity-description.lance\"\n",
    "\n",
    "# Open the Lance dataset\n",
    "dataset = lance.dataset(lance_path)\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "df = dataset.to_table().to_pandas()\n",
    "\n",
    "# Display DataFrame info\n",
    "print(df.info())\n",
    "\n",
    "# Show first few rows\n",
    "print(df.head())\n",
    "print()\n",
    "print(df.loc[0]['text'])\n",
    "print()\n",
    "print(df.loc[0]['attributes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1e1264-c47b-4177-96b0-e7e08fc05afe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the output in the required format:\n",
      "\n",
      "(\"entity_types\": [\"organization\", \"person\", \"geo\", \"event\"])\n",
      " \n",
      "(\"entities\": [\n",
      "    (\"Graph Neural Networks\", \"organization\"),\n",
      "    (\"GNNs\", \"organization\"),\n",
      "    (\"machine learning\", \"field\"),\n",
      "    (\"algorithms\", \"concept\"),\n",
      "    (\"social networks\", \"application\"),\n",
      "    (\"molecular structures\", \"application\"),\n",
      "    (\"knowledge graphs\", \"application\"),\n",
      "    (\"recommendation systems\", \"application\"),\n",
      "    (\"Graph Convolutional Networks\", \"organization\"),\n",
      "    (\"GCNs\", \"organization\"),\n",
      "    (\"Graph Attention Networks\", \"organization\"),\n",
      "    (\"GATs\", \"organization\"),\n",
      "    (\"Graph Recurrent Neural Networks\", \"organization\"),\n",
      "    (\"GRNNs\", \"organization\")\n",
      "])\n",
      "\n",
      "(\"relationships\": [\n",
      "    (\"Graph Neural Networks\", \"utilize\", \"machine learning\"),\n",
      "    (\"GNNs\", \"apply to\", \"social networks\"),\n",
      "    (\"GNNs\", \"apply to\", \"molecular structures\"),\n",
      "    (\"GNNs\", \"apply to\", \"knowledge graphs\"),\n",
      "    (\"GNNs\", \"apply to\", \"recommendation systems\"),\n",
      "    (\"Graph Convolutional Networks\", \"extend\", \"convolutional neural networks\"),\n",
      "    (\"Graph Attention Networks\", \"enhance\", \"Graph Convolutional Networks\"),\n",
      "    (\"Graph Recurrent Neural Networks\", \"capture\", \"temporal dependencies\")\n",
      "])\n",
      "\n",
      "(\"events\": [\n",
      "    (\"development of Graph Neural Networks\", \"event\"),\n",
      "    (\"application of GNNs in various fields\", \"event\"),\n",
      "    (\"research on overcoming limitations of GNNs\", \"event\"),\n",
      "    (\"integration of GNNs with other AI paradigms\", \"event\")\n",
      "])\n",
      "\n",
      "Note: The output is based on the provided text and might not be exhaustive. It includes entities, relationships, and events extracted from the text related to Graph Neural Networks. \n",
      "\n",
      "Also, note that some entities can belong to more than one category (e.g., \"Graph Convolutional Networks\" can be both an organization and a concept), but for simplicity, I've assigned each entity to one category. Similarly, relationships can be complex and nuanced, but I've tried to capture the main connections between entities. Events are also identified based on the text, focusing on key developments and research directions in the field of Graph Neural Networks.(\"entities\": [\n",
      "    (\"University of California\", \"organization\"),\n",
      "    (\"Stanford University\", \"organization\"),\n",
      "    (\"MIT\", \"organization\"),\n",
      "    (\"Google\", \"organization\"),\n",
      "    (\"Facebook\", \"organization\"),\n",
      "    (\"Microsoft\", \"organization\"),\n",
      "    (\"Amazon\", \"organization\"),\n",
      "    (\"IBM\", \"organization\"),\n",
      "    (\"Yann LeCun\", \"person\"),\n",
      "    (\"Geoffrey Hinton\", \"person\"),\n",
      "    (\"Fei-Fei Li\", \"person\"),\n",
      "    (\"Demis Hassabis\", \"person\"),\n",
      "    (\"Andrew Ng\", \"person\"),\n",
      "    (\"China\", \"geo\"),\n",
      "    (\"United States\", \"geo\"),\n",
      "    (\"Europe\", \"geo\"),\n",
      "    (\"NeurIPS\", \"event\"),\n",
      "    (\"ICLR\", \"event\"),\n",
      "    (\"ICML\", \"event\"),\n",
      "    (\"AAAI\", \"event\")\n",
      "])\n",
      "\n",
      "(\"relationships\": [\n",
      "    (\"Graph Neural Networks\", \"developed by\", \"Yann LeCun\"),\n",
      "    (\"Graph Convolutional Networks\", \"proposed by\", \"Thomas Kipf\"),\n",
      "    (\"Graph Attention Networks\", \"introduced by\", \"Petar Velikovi\"),\n",
      "    (\"Graph Recurrent Neural Networks\", \"developed by\", \"Sepp Hochreiter\"),\n",
      "    (\"University of California\", \"affiliated with\", \"Yann LeCun\"),\n",
      "    (\"Stanford University\", \"affiliated with\", \"Fei-Fei Li\"),\n",
      "    (\"MIT\", \"affiliated with\", \"Demis Hassabis\"),\n",
      "    (\"Google\", \"founded by\", \"Larry Page\"),\n",
      "    (\"Facebook\", \"founded by\", \"Mark Zuckerberg\"),\n",
      "    (\"Microsoft\", \"founded by\", \"Bill Gates\"),\n",
      "    (\"Amazon\", \"founded by\", \"Jeff Bezos\"),\n",
      "    (\"IBM\", \"founded by\", \"Charles Ranlett Flint\"),\n",
      "    (\"NeurIPS\", \"organized by\", \"University of California\"),\n",
      "    (\"ICLR\", \"organized by\", \"MIT\"),\n",
      "    (\"ICML\", \"organized by\", \"Stanford University\"),\n",
      "    (\"AAAI\", \"organized by\", \"Association for the Advancement of Artificial Intelligence\")\n",
      "])\n",
      "\n",
      "(\"entities\": [\n",
      "    (\"Association for the Advancement of Artificial Intelligence\", \"organization\"),\n",
      "    (\"International Joint Conference on Artificial Intelligence\", \"event\"),\n",
      "    (\"Conference on Computer Vision and Pattern Recognition\", \"event\"),\n",
      "    (\"IEEE Transactions on Neural Networks and Learning Systems\", \"event\"),\n",
      "    (\"Journal of Machine Learning Research\", \"event\")\n",
      "])\n",
      "\n",
      "(\"relationships\": [\n",
      "    (\"Graph Neural Networks\", \"published in\", \"NeurIPS\"),\n",
      "    (\"Graph Convolutional Networks\", \"published in\", \"ICLR\"),\n",
      "    (\"Graph Attention Networks\", \"published in\", \"ICML\"),\n",
      "    (\"Graph Recurrent Neural Networks\", \"published in\", \"AAAI\"),\n",
      "    (\"Yann LeCun\", \"awarded by\", \"Association for the Advancement of Artificial Intelligence\"),\n",
      "    (\"Geoffrey Hinton\", \"awarded by\", \"International Joint Conference on Artificial Intelligence\"),\n",
      "    (\"Fei-Fei Li\", \"awarded by\", \"Conference on Computer Vision and Pattern Recognition\")\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "all_records={0: 'Here is the output in the required format:\\n\\n(\"entity_types\": [\"organization\", \"person\", \"geo\", \"event\"])\\n \\n(\"entities\": [\\n    (\"Graph Neural Networks\", \"organization\"),\\n    (\"GNNs\", \"organization\"),\\n    (\"machine learning\", \"field\"),\\n    (\"algorithms\", \"concept\"),\\n    (\"social networks\", \"application\"),\\n    (\"molecular structures\", \"application\"),\\n    (\"knowledge graphs\", \"application\"),\\n    (\"recommendation systems\", \"application\"),\\n    (\"Graph Convolutional Networks\", \"organization\"),\\n    (\"GCNs\", \"organization\"),\\n    (\"Graph Attention Networks\", \"organization\"),\\n    (\"GATs\", \"organization\"),\\n    (\"Graph Recurrent Neural Networks\", \"organization\"),\\n    (\"GRNNs\", \"organization\")\\n])\\n\\n(\"relationships\": [\\n    (\"Graph Neural Networks\", \"utilize\", \"machine learning\"),\\n    (\"GNNs\", \"apply to\", \"social networks\"),\\n    (\"GNNs\", \"apply to\", \"molecular structures\"),\\n    (\"GNNs\", \"apply to\", \"knowledge graphs\"),\\n    (\"GNNs\", \"apply to\", \"recommendation systems\"),\\n    (\"Graph Convolutional Networks\", \"extend\", \"convolutional neural networks\"),\\n    (\"Graph Attention Networks\", \"enhance\", \"Graph Convolutional Networks\"),\\n    (\"Graph Recurrent Neural Networks\", \"capture\", \"temporal dependencies\")\\n])\\n\\n(\"events\": [\\n    (\"development of Graph Neural Networks\", \"event\"),\\n    (\"application of GNNs in various fields\", \"event\"),\\n    (\"research on overcoming limitations of GNNs\", \"event\"),\\n    (\"integration of GNNs with other AI paradigms\", \"event\")\\n])\\n\\nNote: The output is based on the provided text and might not be exhaustive. It includes entities, relationships, and events extracted from the text related to Graph Neural Networks. \\n\\nAlso, note that some entities can belong to more than one category (e.g., \"Graph Convolutional Networks\" can be both an organization and a concept), but for simplicity, I\\'ve assigned each entity to one category. Similarly, relationships can be complex and nuanced, but I\\'ve tried to capture the main connections between entities. Events are also identified based on the text, focusing on key developments and research directions in the field of Graph Neural Networks.(\"entities\": [\\n    (\"University of California\", \"organization\"),\\n    (\"Stanford University\", \"organization\"),\\n    (\"MIT\", \"organization\"),\\n    (\"Google\", \"organization\"),\\n    (\"Facebook\", \"organization\"),\\n    (\"Microsoft\", \"organization\"),\\n    (\"Amazon\", \"organization\"),\\n    (\"IBM\", \"organization\"),\\n    (\"Yann LeCun\", \"person\"),\\n    (\"Geoffrey Hinton\", \"person\"),\\n    (\"Fei-Fei Li\", \"person\"),\\n    (\"Demis Hassabis\", \"person\"),\\n    (\"Andrew Ng\", \"person\"),\\n    (\"China\", \"geo\"),\\n    (\"United States\", \"geo\"),\\n    (\"Europe\", \"geo\"),\\n    (\"NeurIPS\", \"event\"),\\n    (\"ICLR\", \"event\"),\\n    (\"ICML\", \"event\"),\\n    (\"AAAI\", \"event\")\\n])\\n\\n(\"relationships\": [\\n    (\"Graph Neural Networks\", \"developed by\", \"Yann LeCun\"),\\n    (\"Graph Convolutional Networks\", \"proposed by\", \"Thomas Kipf\"),\\n    (\"Graph Attention Networks\", \"introduced by\", \"Petar Velikovi\"),\\n    (\"Graph Recurrent Neural Networks\", \"developed by\", \"Sepp Hochreiter\"),\\n    (\"University of California\", \"affiliated with\", \"Yann LeCun\"),\\n    (\"Stanford University\", \"affiliated with\", \"Fei-Fei Li\"),\\n    (\"MIT\", \"affiliated with\", \"Demis Hassabis\"),\\n    (\"Google\", \"founded by\", \"Larry Page\"),\\n    (\"Facebook\", \"founded by\", \"Mark Zuckerberg\"),\\n    (\"Microsoft\", \"founded by\", \"Bill Gates\"),\\n    (\"Amazon\", \"founded by\", \"Jeff Bezos\"),\\n    (\"IBM\", \"founded by\", \"Charles Ranlett Flint\"),\\n    (\"NeurIPS\", \"organized by\", \"University of California\"),\\n    (\"ICLR\", \"organized by\", \"MIT\"),\\n    (\"ICML\", \"organized by\", \"Stanford University\"),\\n    (\"AAAI\", \"organized by\", \"Association for the Advancement of Artificial Intelligence\")\\n])\\n\\n(\"entities\": [\\n    (\"Association for the Advancement of Artificial Intelligence\", \"organization\"),\\n    (\"International Joint Conference on Artificial Intelligence\", \"event\"),\\n    (\"Conference on Computer Vision and Pattern Recognition\", \"event\"),\\n    (\"IEEE Transactions on Neural Networks and Learning Systems\", \"event\"),\\n    (\"Journal of Machine Learning Research\", \"event\")\\n])\\n\\n(\"relationships\": [\\n    (\"Graph Neural Networks\", \"published in\", \"NeurIPS\"),\\n    (\"Graph Convolutional Networks\", \"published in\", \"ICLR\"),\\n    (\"Graph Attention Networks\", \"published in\", \"ICML\"),\\n    (\"Graph Recurrent Neural Networks\", \"published in\", \"AAAI\"),\\n    (\"Yann LeCun\", \"awarded by\", \"Association for the Advancement of Artificial Intelligence\"),\\n    (\"Geoffrey Hinton\", \"awarded by\", \"International Joint Conference on Artificial Intelligence\"),\\n    (\"Fei-Fei Li\", \"awarded by\", \"Conference on Computer Vision and Pattern Recognition\")\\n])'}\n",
    "print(all_records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e5ba9-0b53-4198-b420-7d1bd9fdfd37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = 'Here is the output:\\n\\n(\"entity_types\": [\"organization\", \"person\", \"geo\", \"event\"])\\n(\"entities\": [\\n    (\"Vaswani\", \"person\"),\\n    (\"Transformer Neural Networks\", \"organization\"),\\n    (\"NLP\", \"organization\"),\\n    (\"RNNs\", \"organization\"),\\n    (\"CNNs\", \"organization\"),\\n    (\"BERT\", \"organization\"),\\n    (\"GPT\", \"organization\"),\\n    (\"T5\", \"organization\"),\\n    (\"Vision Transformer\", \"organization\"),\\n    (\"ViT\", \"organization\"),\\n    (\"Reformer\", \"organization\"),\\n    (\"Linformer\", \"organization\"),\\n    (\"Longformer\", \"organization\")\\n])\\n(\"relations\": [\\n    (\"Vaswani\", \"introduced\", \"Transformer Neural Networks\"),\\n    (\"Transformer Neural Networks\", \"used in\", \"NLP\"),\\n    (\"RNNs\", \"compared to\", \"Transformer Neural Networks\"),\\n    (\"CNNs\", \"compared to\", \"Transformer Neural Networks\"),\\n    (\"BERT\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"GPT\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"T5\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"Vision Transformer\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"ViT\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"Reformer\", \"improves\", \"Transformer Neural Networks\"),\\n    (\"Linformer\", \"improves\", \"Transformer Neural Networks\"),\\n    (\"Longformer\", \"improves\", \"Transformer Neural Networks\")\\n])\\n(\"events\": [\\n    (\"Introduction of Transformer Neural Networks\", \"event\"),\\n    (\"Development of BERT\", \"event\"),\\n    (\"Development of GPT\", \"event\"),\\n    (\"Development of T5\", \"event\"),\\n    (\"Application of Transformers in Computer Vision\", \"event\"),\\n    (\"Research on Efficient Transformers\", \"event\")\\n])\\n(\"geo\": []) \\n\\nNote: The output is in a JSON-like format, with entities, relations, and events extracted from the text. The \"geo\" field is empty since there are no geographic locations mentioned in the text.Here is the updated output with additional entities and relationships:\\n\\n\\n(\"entity_types\": [\"organization\", \"person\", \"geo\", \"event\"])\\n(\"entities\": [\\n    (\"Vaswani\", \"person\"),\\n    (\"Transformer Neural Networks\", \"organization\"),\\n    (\"NLP\", \"organization\"),\\n    (\"RNNs\", \"organization\"),\\n    (\"CNNs\", \"organization\"),\\n    (\"BERT\", \"organization\"),\\n    (\"GPT\", \"organization\"),\\n    (\"T5\", \"organization\"),\\n    (\"Vision Transformer\", \"organization\"),\\n    (\"ViT\", \"organization\"),\\n    (\"Reformer\", \"organization\"),\\n    (\"Linformer\", \"organization\"),\\n    (\"Longformer\", \"organization\"),\\n    (\"Ashish Vaswani\", \"person\"),\\n    (\"Noam Shazeer\", \"person\"),\\n    (\"Niki Parmar\", \"person\"),\\n    (\"Jakob Uszkoreit\", \"person\"),\\n    (\"Llion Jones\", \"person\"),\\n    (\"Aidan N. Gomez\", \"person\"),\\n    (\"Lukasz Kaiser\", \"person\"),\\n    (\"Illia Polosukhin\", \"person\"),\\n    (\"Google\", \"organization\"),\\n    (\"Stanford University\", \"organization\"),\\n    (\"University of California, Berkeley\", \"organization\"),\\n    (\"MIT\", \"organization\"),\\n    (\"Harvard University\", \"organization\")\\n])\\n(\"relations\": [\\n    (\"Vaswani\", \"introduced\", \"Transformer Neural Networks\"),\\n    (\"Transformer Neural Networks\", \"used in\", \"NLP\"),\\n    (\"RNNs\", \"compared to\", \"Transformer Neural Networks\"),\\n    (\"CNNs\", \"compared to\", \"Transformer Neural Networks\"),\\n    (\"BERT\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"GPT\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"T5\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"Vision Transformer\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"ViT\", \"uses\", \"Transformer Neural Networks\"),\\n    (\"Reformer\", \"improves\", \"Transformer Neural Networks\"),\\n    (\"Linformer\", \"improves\", \"Transformer Neural Networks\"),\\n    (\"Longformer\", \"improves\", \"Transformer Neural Networks\"),\\n    (\"Ashish Vaswani\", \"co-authored with\", \"Noam Shazeer\"),\\n    (\"Niki Parmar\", \"collaborated with\", \"Jakob Uszkoreit\"),\\n    (\"Llion Jones\", \"worked at\", \"Google\"),\\n    (\"Aidan N. Gomez\", \"studied at\", \"Stanford University\"),\\n    (\"Lukasz Kaiser\", \"researched at\", \"University of California, Berkeley\"),\\n    (\"Illia Polosukhin\", \"published paper with\", \"Ashish Vaswani\")\\n])\\n(\"events\": [\\n    (\"Introduction of Transformer Neural Networks\", \"event\"),\\n    (\"Development of BERT\", \"event\"),\\n    (\"Development of GPT\", \"event\"),\\n    (\"Development of T5\", \"event\"),\\n    (\"Application of Transformers in Computer Vision\", \"event\"),\\n    (\"Research on Efficient Transformers\", \"event\"),\\n    (\"Publication of Transformer Paper\", \"event\"),\\n    (\"Transformer Conference\", \"event\")\\n])\\n(\"geo\": [\\n    (\"California\", \"geo\"),\\n    (\"Massachusetts\", \"geo\")\\n])'\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e4226-2c9a-4598-8841-a13c242f9b72",
   "metadata": {},
   "source": [
    "## match graph data format to Graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cb84f3d-fccc-4a5c-a0a7-b6895cd8b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import csv\n",
    "\n",
    "# Read the CSV file\n",
    "root = '/projects/JHA/shared/graph/pubmed/'\n",
    "graph_path = root + 'graph_data_umls_cleaned.csv'\n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.MultiDiGraph()  # Use DiGraph for a directed graph. Use Graph() for an undirected graph.\n",
    "\n",
    "with open(graph_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        head = line[0]\n",
    "        # Iterate over the subsequent columns in pairs (relation, tail)\n",
    "        for i in range(1, len(line), 2):\n",
    "            if pd.isna(line[i]) or pd.isna(line[i+1]):\n",
    "                break\n",
    "            relation = line[i]\n",
    "            tail = line[i+1]\n",
    "            # Add an edge to the graph with the relation as an edge attribute\n",
    "            G.add_edge(head, tail, rel=relation)\n",
    "\n",
    "# Prepare entities dataframe\n",
    "nodes_data = []\n",
    "for human_readable_id, node in enumerate(G.nodes()):\n",
    "    node_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each node.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"title\": node,                        # Use the node name for the title.\n",
    "        \"description\": node,                  # Use the node name for the description.\n",
    "        \"degree\": G.degree(node)              # Calculate the node's degree.\n",
    "    }\n",
    "    nodes_data.append(node_entry)\n",
    "\n",
    "entities_df = pd.DataFrame(nodes_data)\n",
    "\n",
    "# Process edges to create the relationships DataFrame.\n",
    "edges_data = []\n",
    "for human_readable_id, (source, target, data) in enumerate(G.edges(data=True)):\n",
    "    combined_degree = G.degree(source) + G.degree(target)\n",
    "    rel = data.get(\"rel\", \"\")\n",
    "    if rel == 'isa': \n",
    "        rel = 'is a'\n",
    "    edge_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each edge.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"source\": source,                     # Source node (using the node name).\n",
    "        \"target\": target,                      # Target node (using the node name).\n",
    "        \"combined_degree\": combined_degree, \n",
    "        \"description\": f\"{source} {rel} {target}\"\n",
    "        # 'rel' attribute is available in data if needed: data.get('rel')\n",
    "    }\n",
    "    edges_data.append(edge_entry)\n",
    "\n",
    "relationships_df = pd.DataFrame(edges_data)\n",
    "\n",
    "# Save DataFrames to parquet files\n",
    "entities_df.to_parquet(root+'entities.parquet', index=False)\n",
    "relationships_df.to_parquet(root+'relationships.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727caf80-b710-4d9b-8a36-e51ef24b9833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2f7ff881-494f-4914-8d0c-e8947e41f9eb</td>\n",
       "      <td>0</td>\n",
       "      <td>intrauterine transfusion</td>\n",
       "      <td>intrauterine transfusion</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39decfe6-7969-4c6d-82cd-a17afb65656b</td>\n",
       "      <td>1</td>\n",
       "      <td>blood material</td>\n",
       "      <td>blood material</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aafa8da8-b331-4087-bc2b-76bd081cc5b9</td>\n",
       "      <td>2</td>\n",
       "      <td>percutaneous approach</td>\n",
       "      <td>percutaneous approach</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9aa2fd39-f446-414c-a993-653031f5806c</td>\n",
       "      <td>3</td>\n",
       "      <td>cryoprecipitate</td>\n",
       "      <td>cryoprecipitate</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9856fb9d-07b5-4b3e-9705-4ca08ba06634</td>\n",
       "      <td>4</td>\n",
       "      <td>intrauterine exchange transfusion</td>\n",
       "      <td>intrauterine exchange transfusion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  \\\n",
       "0  2f7ff881-494f-4914-8d0c-e8947e41f9eb                  0   \n",
       "1  39decfe6-7969-4c6d-82cd-a17afb65656b                  1   \n",
       "2  aafa8da8-b331-4087-bc2b-76bd081cc5b9                  2   \n",
       "3  9aa2fd39-f446-414c-a993-653031f5806c                  3   \n",
       "4  9856fb9d-07b5-4b3e-9705-4ca08ba06634                  4   \n",
       "\n",
       "                               title                        description  \\\n",
       "0           intrauterine transfusion           intrauterine transfusion   \n",
       "1                     blood material                     blood material   \n",
       "2              percutaneous approach              percutaneous approach   \n",
       "3                    cryoprecipitate                    cryoprecipitate   \n",
       "4  intrauterine exchange transfusion  intrauterine exchange transfusion   \n",
       "\n",
       "   degree  \n",
       "0       4  \n",
       "1      14  \n",
       "2       7  \n",
       "3       3  \n",
       "4       2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe11559-b0a9-46af-9b6c-9b3dca444d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>combined_degree</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6538986b-1b6c-4f71-b3bd-4ff67e7f10cd</td>\n",
       "      <td>0</td>\n",
       "      <td>intrauterine transfusion</td>\n",
       "      <td>blood material</td>\n",
       "      <td>18</td>\n",
       "      <td>intrauterine transfusion uses blood material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ae1cdf9f-4496-4288-9622-f0df7df9856e</td>\n",
       "      <td>1</td>\n",
       "      <td>intrauterine transfusion</td>\n",
       "      <td>percutaneous approach</td>\n",
       "      <td>11</td>\n",
       "      <td>intrauterine transfusion conceptual part of pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>266bbfd2-1089-4f30-ba50-60a3c6e22891</td>\n",
       "      <td>2</td>\n",
       "      <td>blood material</td>\n",
       "      <td>cryoprecipitate</td>\n",
       "      <td>17</td>\n",
       "      <td>blood material is a cryoprecipitate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358ea0d4-3afe-4fc8-b48e-97b666c358fb</td>\n",
       "      <td>3</td>\n",
       "      <td>intrauterine exchange transfusion</td>\n",
       "      <td>blood material</td>\n",
       "      <td>16</td>\n",
       "      <td>intrauterine exchange transfusion uses blood m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9955897a-03c5-414d-a224-7aeda7def387</td>\n",
       "      <td>4</td>\n",
       "      <td>intrauterine exchange transfusion</td>\n",
       "      <td>percutaneous approach</td>\n",
       "      <td>9</td>\n",
       "      <td>intrauterine exchange transfusion conceptual p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  \\\n",
       "0  6538986b-1b6c-4f71-b3bd-4ff67e7f10cd                  0   \n",
       "1  ae1cdf9f-4496-4288-9622-f0df7df9856e                  1   \n",
       "2  266bbfd2-1089-4f30-ba50-60a3c6e22891                  2   \n",
       "3  358ea0d4-3afe-4fc8-b48e-97b666c358fb                  3   \n",
       "4  9955897a-03c5-414d-a224-7aeda7def387                  4   \n",
       "\n",
       "                              source                 target  combined_degree  \\\n",
       "0           intrauterine transfusion         blood material               18   \n",
       "1           intrauterine transfusion  percutaneous approach               11   \n",
       "2                     blood material        cryoprecipitate               17   \n",
       "3  intrauterine exchange transfusion         blood material               16   \n",
       "4  intrauterine exchange transfusion  percutaneous approach                9   \n",
       "\n",
       "                                         description  \n",
       "0       intrauterine transfusion uses blood material  \n",
       "1  intrauterine transfusion conceptual part of pe...  \n",
       "2                blood material is a cryoprecipitate  \n",
       "3  intrauterine exchange transfusion uses blood m...  \n",
       "4  intrauterine exchange transfusion conceptual p...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships_df.head()\n",
    "#print(relationships_df.loc[0]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b9db01a-88ba-4bfb-ad5a-23be47a5bf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132962\n"
     ]
    }
   ],
   "source": [
    "print(len(entities_df['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47e3daf4-b432-4e5a-817e-61344aa4ed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132962\n",
      "0\n",
      "50000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "\n",
    "entities_df = pd.read_parquet(root+'entities.parquet')\n",
    "sentences = list(entities_df['description'])\n",
    "print(len(sentences))\n",
    "final_embeddings = []\n",
    "batch_size = 50000\n",
    "for i in range(0, len(sentences)+1, batch_size):\n",
    "    print(i)\n",
    "    if i+batch_size<=len(sentences)+1:\n",
    "        batch = sentences[i:i+batch_size]\n",
    "    else:\n",
    "        batch = sentences[i:]\n",
    "    response = model.encode(batch)\n",
    "    final_embeddings.extend(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d106319-1ec1-4bb9-869e-3f48beaf6b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings generated: 132962\n",
      "\n",
      "Entities have been embedded and saved to LanceDB successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(f\"Total embeddings generated: {len(final_embeddings)}\")\n",
    "\n",
    "entities_df['vector'] = final_embeddings\n",
    "\n",
    "# Create a new column \"attributes\" as a dictionary with the title\n",
    "entities_df['attributes'] = entities_df['title'].apply(lambda title: json.dumps({\"title\": title}))\n",
    "print()\n",
    "final_entities_df = entities_df[['id', 'description', 'vector', 'attributes']].rename(columns={'description': 'text'})\n",
    "\n",
    "# Connect to (or create) a LanceDB database and save the DataFrame.\n",
    "db = lancedb.connect(root+\"lancedb_entities\")\n",
    "table = db.create_table(\"entities\", final_entities_df, mode=\"overwrite\")\n",
    "\n",
    "print(\"Entities have been embedded and saved to LanceDB successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a14cedd4-2297-4820-a5a3-79199800a65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0e24ff61-4185-4892-97bb-1d1e0f597f51</td>\n",
       "      <td>intrauterine transfusion</td>\n",
       "      <td>[-0.0043643704, 0.04901778, 0.0012996075, -0.0...</td>\n",
       "      <td>{\"title\": \"intrauterine transfusion\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91ac2a6d-4283-4fa2-95b6-e249582e7654</td>\n",
       "      <td>blood material</td>\n",
       "      <td>[0.018517913, 0.001478622, -0.029236525, -0.04...</td>\n",
       "      <td>{\"title\": \"blood material\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9cb6190-2ba9-40d0-a2b1-b55fced780c1</td>\n",
       "      <td>percutaneous approach</td>\n",
       "      <td>[0.049003832, -0.021690117, -0.014992222, -0.0...</td>\n",
       "      <td>{\"title\": \"percutaneous approach\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a9a5b55b-1cda-4147-a3d2-7b1eb54cf35f</td>\n",
       "      <td>cryoprecipitate</td>\n",
       "      <td>[0.05303567, -0.00590552, 0.00805665, -0.04629...</td>\n",
       "      <td>{\"title\": \"cryoprecipitate\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47124696-7799-4653-ad7e-eaaecdf7b3e4</td>\n",
       "      <td>intrauterine exchange transfusion</td>\n",
       "      <td>[-0.010339196, 0.057096526, 0.003487452, -0.05...</td>\n",
       "      <td>{\"title\": \"intrauterine exchange transfusion\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                               text  \\\n",
       "0  0e24ff61-4185-4892-97bb-1d1e0f597f51           intrauterine transfusion   \n",
       "1  91ac2a6d-4283-4fa2-95b6-e249582e7654                     blood material   \n",
       "2  c9cb6190-2ba9-40d0-a2b1-b55fced780c1              percutaneous approach   \n",
       "3  a9a5b55b-1cda-4147-a3d2-7b1eb54cf35f                    cryoprecipitate   \n",
       "4  47124696-7799-4653-ad7e-eaaecdf7b3e4  intrauterine exchange transfusion   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [-0.0043643704, 0.04901778, 0.0012996075, -0.0...   \n",
       "1  [0.018517913, 0.001478622, -0.029236525, -0.04...   \n",
       "2  [0.049003832, -0.021690117, -0.014992222, -0.0...   \n",
       "3  [0.05303567, -0.00590552, 0.00805665, -0.04629...   \n",
       "4  [-0.010339196, 0.057096526, 0.003487452, -0.05...   \n",
       "\n",
       "                                       attributes  \n",
       "0           {\"title\": \"intrauterine transfusion\"}  \n",
       "1                     {\"title\": \"blood material\"}  \n",
       "2              {\"title\": \"percutaneous approach\"}  \n",
       "3                    {\"title\": \"cryoprecipitate\"}  \n",
       "4  {\"title\": \"intrauterine exchange transfusion\"}  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_entities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e7372eb-f307-4c69-9555-b0fd0f373b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0e24ff61-4185-4892-97bb-1d1e0f597f51</td>\n",
       "      <td>0</td>\n",
       "      <td>intrauterine transfusion</td>\n",
       "      <td>intrauterine transfusion</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91ac2a6d-4283-4fa2-95b6-e249582e7654</td>\n",
       "      <td>1</td>\n",
       "      <td>blood material</td>\n",
       "      <td>blood material</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9cb6190-2ba9-40d0-a2b1-b55fced780c1</td>\n",
       "      <td>2</td>\n",
       "      <td>percutaneous approach</td>\n",
       "      <td>percutaneous approach</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a9a5b55b-1cda-4147-a3d2-7b1eb54cf35f</td>\n",
       "      <td>3</td>\n",
       "      <td>cryoprecipitate</td>\n",
       "      <td>cryoprecipitate</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47124696-7799-4653-ad7e-eaaecdf7b3e4</td>\n",
       "      <td>4</td>\n",
       "      <td>intrauterine exchange transfusion</td>\n",
       "      <td>intrauterine exchange transfusion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  \\\n",
       "0  0e24ff61-4185-4892-97bb-1d1e0f597f51                  0   \n",
       "1  91ac2a6d-4283-4fa2-95b6-e249582e7654                  1   \n",
       "2  c9cb6190-2ba9-40d0-a2b1-b55fced780c1                  2   \n",
       "3  a9a5b55b-1cda-4147-a3d2-7b1eb54cf35f                  3   \n",
       "4  47124696-7799-4653-ad7e-eaaecdf7b3e4                  4   \n",
       "\n",
       "                               title                        description  \\\n",
       "0           intrauterine transfusion           intrauterine transfusion   \n",
       "1                     blood material                     blood material   \n",
       "2              percutaneous approach              percutaneous approach   \n",
       "3                    cryoprecipitate                    cryoprecipitate   \n",
       "4  intrauterine exchange transfusion  intrauterine exchange transfusion   \n",
       "\n",
       "   degree  \n",
       "0       4  \n",
       "1      14  \n",
       "2       7  \n",
       "3       3  \n",
       "4       2  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_df = pd.read_parquet('entities.parquet')\n",
    "entities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "689f9b03-f009-4c7e-b9ad-f139e980122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The answer is: \\boxed{<answer: A B C or D>}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = \"\"\"\n",
    "The answer is: \\\\boxed{<answer: A B C or D>}.\n",
    "\"\"\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c9d85-ff60-4389-82b0-b8e93f219d04",
   "metadata": {},
   "source": [
    "## match graph data format to Graphrag, expanded graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be0f9681-ce46-4d68-baba-c044447141f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import csv\n",
    "\n",
    "# Read the CSV file\n",
    "root = '/projects/JHA/shared/graph/pubmed/'\n",
    "graph_path = root + 'graph_data_umls_cleaned_expanded.csv'\n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.MultiDiGraph()  # Use DiGraph for a directed graph. Use Graph() for an undirected graph.\n",
    "\n",
    "with open(graph_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        head = line[0]\n",
    "        # Iterate over the subsequent columns in pairs (relation, tail)\n",
    "        for i in range(1, len(line), 2):\n",
    "            if pd.isna(line[i]) or pd.isna(line[i+1]):\n",
    "                break\n",
    "            relation = line[i]\n",
    "            tail = line[i+1]\n",
    "            # Add an edge to the graph with the relation as an edge attribute\n",
    "            G.add_edge(head, tail, rel=relation)\n",
    "\n",
    "# Prepare entities dataframe\n",
    "nodes_data = []\n",
    "for human_readable_id, node in enumerate(G.nodes()):\n",
    "    node_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each node.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"title\": node,                        # Use the node name for the title.\n",
    "        \"description\": node,                  # Use the node name for the description.\n",
    "        \"degree\": G.degree(node)              # Calculate the node's degree.\n",
    "    }\n",
    "    nodes_data.append(node_entry)\n",
    "\n",
    "entities_df = pd.DataFrame(nodes_data)\n",
    "\n",
    "# Process edges to create the relationships DataFrame.\n",
    "edges_data = []\n",
    "for human_readable_id, (source, target, data) in enumerate(G.edges(data=True)):\n",
    "    combined_degree = G.degree(source) + G.degree(target)\n",
    "    rel = data.get(\"rel\", \"\")\n",
    "    if rel == 'isa': \n",
    "        rel = 'is a'\n",
    "    edge_entry = {\n",
    "        \"id\": str(uuid.uuid4()),              # Generate a unique UUID for each edge.\n",
    "        \"human_readable_id\": human_readable_id, # A sequential human readable id.\n",
    "        \"source\": source,                     # Source node (using the node name).\n",
    "        \"target\": target,                      # Target node (using the node name).\n",
    "        \"combined_degree\": combined_degree, \n",
    "        \"description\": f\"{source} {rel} {target}\"\n",
    "        # 'rel' attribute is available in data if needed: data.get('rel')\n",
    "    }\n",
    "    edges_data.append(edge_entry)\n",
    "\n",
    "relationships_df = pd.DataFrame(edges_data)\n",
    "\n",
    "# Save DataFrames to parquet files\n",
    "entities_df.to_parquet(root + 'entities_expanded.parquet', index=False)\n",
    "relationships_df.to_parquet(root + 'relationships_expanded.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d916d386-0ec4-4c5b-82ab-ba2f7503966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 136210 entries, 0 to 136209\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   id                 136210 non-null  object\n",
      " 1   human_readable_id  136210 non-null  int64 \n",
      " 2   title              136210 non-null  object\n",
      " 3   description        136210 non-null  object\n",
      " 4   degree             136210 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "entities_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "824b1bf6-167a-4611-bac9-e713b8a488d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- configuration_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa82aeace224684b493aaf700da43b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_hf_nomic_bert.py:   0%|          | 0.00/103k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- modeling_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "/home/jx0800/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/e5042dce39060cc34bc223455f25cf1d26db4655/modeling_hf_nomic_bert.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136210\n",
      "0\n",
      "50000\n",
      "100000\n",
      "Total embeddings generated: 136210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-24T18:24:22Z WARN  lance::dataset::write::insert] No existing dataset at /projects/JHA/shared/graph/pubmed/lancedb_entities_expanded/entities.lance, it will be created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities have been embedded and saved to LanceDB successfully.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "\n",
    "entities_df = pd.read_parquet(root + 'entities_expanded.parquet')\n",
    "sentences = list(entities_df['description'])\n",
    "print(len(sentences))\n",
    "final_embeddings = []\n",
    "batch_size = 50000\n",
    "for i in range(0, len(sentences)+1, batch_size):\n",
    "    print(i)\n",
    "    if i+batch_size<=len(sentences)+1:\n",
    "        batch = sentences[i:i+batch_size]\n",
    "    else:\n",
    "        batch = sentences[i:]\n",
    "    response = model.encode(batch)\n",
    "    final_embeddings.extend(response)\n",
    "\n",
    "import json\n",
    "print(f\"Total embeddings generated: {len(final_embeddings)}\")\n",
    "\n",
    "entities_df['vector'] = final_embeddings\n",
    "\n",
    "# Create a new column \"attributes\" as a dictionary with the title\n",
    "entities_df['attributes'] = entities_df['title'].apply(lambda title: json.dumps({\"title\": title}))\n",
    "print()\n",
    "final_entities_df = entities_df[['id', 'description', 'vector', 'attributes']].rename(columns={'description': 'text'})\n",
    "\n",
    "# Connect to (or create) a LanceDB database and save the DataFrame.\n",
    "db = lancedb.connect(root+\"lancedb_entities_expanded\")\n",
    "table = db.create_table(\"entities\", final_entities_df, mode=\"overwrite\")\n",
    "\n",
    "print(\"Entities have been embedded and saved to LanceDB successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371dced-5441-42f1-9f5c-adff9eab7cf6",
   "metadata": {},
   "source": [
    "## run baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ea8ff23-4a8e-4e5b-a398-157262d6cb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Role---\n",
      "\n",
      "You are a helpful assistant responding to questions about data in the tables provided.\n",
      "\n",
      "---Target response format---\n",
      "\n",
      "Answer in the following format:\n",
      "**Analysis of the question**: \n",
      "- Understand the question and find out what is needed to answer it\n",
      "- Analyze and answer the question\n",
      "\n",
      "**Answer**:\n",
      "- Provide a concise answer using `\\boxed{}`, select only the correct letter (e.g., \\boxed{C}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = r\"\"\"---Role---\n",
    "\n",
    "You are a helpful assistant responding to questions about data in the tables provided.\n",
    "\n",
    "---Target response format---\n",
    "\n",
    "Answer in the following format:\n",
    "**Analysis of the question**: \n",
    "- Understand the question and find out what is needed to answer it\n",
    "- Analyze and answer the question\n",
    "\n",
    "**Answer**:\n",
    "- Provide a concise answer using `\\boxed{}`, select only the correct letter (e.g., \\boxed{C}) \n",
    "\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1e207a8-5315-40c2-bb5e-d3c4d3e55965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 0:\n",
      "Processing query 1:\n",
      "Processing query 2:\n",
      "Processing query 3:\n",
      "Processing query 4:\n",
      "Processing query 5:\n",
      "Processing query 6:\n",
      "Processing query 7:\n",
      "Processing query 8:\n",
      "Processing query 9:\n",
      "Processing query 10:\n",
      "Processing query 11:\n",
      "Processing query 12:\n",
      "Processing query 13:\n",
      "Processing query 14:\n",
      "Processing query 15:\n",
      "Processing query 16:\n",
      "Processing query 17:\n",
      "Processing query 18:\n",
      "Processing query 19:\n",
      "Processing query 20:\n",
      "Processing query 21:\n",
      "Processing query 22:\n",
      "Processing query 23:\n",
      "Processing query 24:\n",
      "Processing query 25:\n",
      "Processing query 26:\n",
      "Processing query 27:\n",
      "Processing query 28:\n",
      "Processing query 29:\n",
      "Processing query 30:\n",
      "Processing query 31:\n",
      "Processing query 32:\n",
      "Processing query 33:\n",
      "Processing query 34:\n",
      "Processing query 35:\n",
      "Processing query 36:\n",
      "Processing query 37:\n",
      "Processing query 38:\n",
      "Processing query 39:\n",
      "Processing query 40:\n",
      "Processing query 41:\n",
      "Processing query 42:\n",
      "Processing query 43:\n",
      "Processing query 44:\n",
      "Processing query 45:\n",
      "Processing query 46:\n",
      "Processing query 47:\n",
      "Processing query 48:\n",
      "Processing query 49:\n",
      "Processing query 50:\n",
      "Processing query 51:\n",
      "Processing query 52:\n",
      "Processing query 53:\n",
      "Processing query 54:\n",
      "Processing query 55:\n",
      "Processing query 56:\n",
      "Processing query 57:\n",
      "Processing query 58:\n",
      "Processing query 59:\n",
      "Processing query 60:\n",
      "Processing query 61:\n",
      "Processing query 62:\n",
      "Processing query 63:\n",
      "Processing query 64:\n",
      "Processing query 65:\n",
      "Processing query 66:\n",
      "Processing query 67:\n",
      "Processing query 68:\n",
      "Processing query 69:\n",
      "Processing query 70:\n",
      "Processing query 71:\n",
      "Processing query 72:\n",
      "Processing query 73:\n",
      "Processing query 74:\n",
      "Processing query 75:\n",
      "Processing query 76:\n",
      "Processing query 77:\n",
      "Processing query 78:\n",
      "Processing query 79:\n",
      "Processing query 80:\n",
      "Processing query 81:\n",
      "Processing query 82:\n",
      "Processing query 83:\n",
      "Processing query 84:\n",
      "Processing query 85:\n",
      "Processing query 86:\n",
      "Processing query 87:\n",
      "Processing query 88:\n",
      "Processing query 89:\n",
      "Processing query 90:\n",
      "Processing query 91:\n",
      "Processing query 92:\n",
      "Processing query 93:\n",
      "Processing query 94:\n",
      "Processing query 95:\n",
      "Processing query 96:\n",
      "Processing query 97:\n",
      "Processing query 98:\n",
      "Processing query 99:\n",
      "All queries processed and responses saved to /scratch/gpfs/jx0800/data/graphrag/results/baseline_responses_8B.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# File paths\n",
    "query_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/queries.txt\"\n",
    "response_file = \"/scratch/gpfs/jx0800/data/graphrag/results/baseline_responses_8B.log\"\n",
    "\n",
    "system_prompt = r\"\"\"---Role---\n",
    "\n",
    "You are a helpful assistant responding to questions about data in the tables provided.\n",
    "\n",
    "---Target response format---\n",
    "\n",
    "Answer in the following format:\n",
    "**Analysis of the question**: \n",
    "- Understand the question and find out what is needed to answer it\n",
    "- Analyze and answer the question\n",
    "\n",
    "**Answer**:\n",
    "- Provide a concise answer using `\\boxed{}`, select only the correct letter (e.g., \\boxed{C}) \n",
    "\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "# Function to call OpenAI API\n",
    "def call_api(query):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", #meta-llama/Llama-3.3-70B-Instruct\n",
    "    max_tokens=2048,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Read queries from the file\n",
    "with open(query_file, \"r\") as file:\n",
    "    queries = file.readlines()\n",
    "\n",
    "# Process each query and save responses\n",
    "with open(response_file, \"w\") as output_file:\n",
    "    for i, query in enumerate(queries):\n",
    "        # if i <= 99: \n",
    "        #     continue\n",
    "        query = query.strip()  # Remove leading/trailing whitespace\n",
    "        if query:  # Skip empty lines\n",
    "            print(f\"Processing query {i}:\")\n",
    "            response = call_api(query)\n",
    "            output_file.write(f\"Query {i}: {query}\\n\")\n",
    "            output_file.write(f\"Response: {response}\\n\")\n",
    "            output_file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
    "        if i == 99: \n",
    "            break\n",
    "\n",
    "print(\"All queries processed and responses saved to\", response_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb04ec-e1b0-47ec-bf3a-696253f1b1a8",
   "metadata": {},
   "source": [
    "## calculate accuracy of seed KG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6931a6-18ad-47fe-8aad-6ff7db29d6ac",
   "metadata": {},
   "source": [
    "### define utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7214291f-f889-4468-9615-21ce1cbdf0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(log_file, split_on='-'*50, answer_note = 'SUCCESS: Local Search Response:'):\n",
    "    \n",
    "    with open(log_file, 'r') as file:\n",
    "        output_content = file.read()\n",
    "\n",
    "    # Split the responses based on the separator\n",
    "    responses = output_content.strip().split(split_on)\n",
    "\n",
    "    # Regular expression to find the part after 'SUCCESS: Local Search Response:'\n",
    "    pattern = fr'{re.escape(answer_note)}(.*)'\n",
    "    success_pattern = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "    # Regular expression to find the content inside \\boxed{}\n",
    "    boxed_pattern = re.compile(r'\\\\boxed\\{([^}]+)\\}')\n",
    "    box_pattern = re.compile(r'\\\\box\\{([^}]+)\\}')\n",
    "\n",
    "    answers = []\n",
    "    # Process each response\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"Processing Response {i}:\")\n",
    "        \n",
    "        # Find the part after 'SUCCESS: Local Search Response:'\n",
    "        success_match = success_pattern.search(response)\n",
    "        if success_match:\n",
    "            success_text = success_match.group(1).strip()\n",
    "            #print(f\"SUCCESS Text: {success_text}\")\n",
    "            \n",
    "            # Check if there's an answer in \\boxed{}\n",
    "            boxed_match = boxed_pattern.search(success_text)\n",
    "            if boxed_match:\n",
    "                answer = boxed_match.group(1)\n",
    "                answers.append(answer)\n",
    "                #print(f\"Answer found in \\\\boxed{{}}: {answer}\")\n",
    "            else:\n",
    "                box_match = box_pattern.search(success_text)\n",
    "                if box_match:\n",
    "                    answer = box_match.group(1)\n",
    "                    answers.append(answer)\n",
    "                else:\n",
    "                    answers.append('E')\n",
    "                    print(\"No answer found in \\\\boxed{}.\")\n",
    "                    print(success_text)\n",
    "        else:\n",
    "            answers.append(None)\n",
    "            print(\"No 'SUCCESS: Local Search Response:' found in this response.\")\n",
    "    \n",
    "    return answers\n",
    "    \n",
    "\n",
    "# Function to process each response\n",
    "def process_answer(answer):\n",
    "    # Mapping of valid responses\n",
    "    response_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    # Check if the response starts with A, B, C, or D\n",
    "    if answer[0] in response_mapping:\n",
    "        # Extract the first character (A, B, C, or D)\n",
    "        key = answer[0]\n",
    "        return response_mapping[key]\n",
    "    else:\n",
    "        # Handle invalid responses\n",
    "        return 4\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = 0\n",
    "    wrong_indices = []\n",
    "    \n",
    "    for idx, (true, pred) in enumerate(zip(y_true, y_pred)):\n",
    "        if true == pred:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong_indices.append(idx)\n",
    "    \n",
    "    accuracy = correct / len(y_true) if y_true else 0\n",
    "    return accuracy, wrong_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a00ced-3ef4-447b-8c69-28018f935a06",
   "metadata": {},
   "source": [
    "## 8B model injected KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8cea38-a8f9-46c4-9d15-50d43b81baba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Response 0:\n",
      "Processing Response 1:\n",
      "Processing Response 2:\n",
      "Processing Response 3:\n",
      "Processing Response 4:\n",
      "No answer found in \\boxed{}.\n",
      "I cant answer that question. If you'd like help with a different question or need assistance with a different topic, feel free to ask.\n",
      "Processing Response 5:\n",
      "Processing Response 6:\n",
      "Processing Response 7:\n",
      "Processing Response 8:\n",
      "Processing Response 9:\n",
      "Processing Response 10:\n",
      "Processing Response 11:\n",
      "Processing Response 12:\n",
      "Processing Response 13:\n",
      "Processing Response 14:\n",
      "Processing Response 15:\n",
      "Processing Response 16:\n",
      "Processing Response 17:\n",
      "Processing Response 18:\n",
      "Processing Response 19:\n",
      "Processing Response 20:\n",
      "Processing Response 21:\n",
      "Processing Response 22:\n",
      "Processing Response 23:\n",
      "Processing Response 24:\n",
      "Processing Response 25:\n",
      "Processing Response 26:\n",
      "Processing Response 27:\n",
      "Processing Response 28:\n",
      "Processing Response 29:\n",
      "Processing Response 30:\n",
      "Processing Response 31:\n",
      "Processing Response 32:\n",
      "Processing Response 33:\n",
      "Processing Response 34:\n",
      "Processing Response 35:\n",
      "Processing Response 36:\n",
      "Processing Response 37:\n",
      "Processing Response 38:\n",
      "No answer found in \\boxed{}.\n",
      "I can't provide a diagnosis or answer this question. If you are concerned about your symptoms, please consult a qualified medical professional. Is there anything else I can help you with?\n",
      "Processing Response 39:\n",
      "Processing Response 40:\n",
      "Processing Response 41:\n",
      "Processing Response 42:\n",
      "Processing Response 43:\n",
      "Processing Response 44:\n",
      "Processing Response 45:\n",
      "Processing Response 46:\n",
      "Processing Response 47:\n",
      "Processing Response 48:\n",
      "Processing Response 49:\n",
      "No answer found in \\boxed{}.\n",
      "I don't have information about Klinefelter syndrome in the provided data tables.\n",
      "Processing Response 50:\n",
      "Processing Response 51:\n",
      "Processing Response 52:\n",
      "Processing Response 53:\n",
      "Processing Response 54:\n",
      "Processing Response 55:\n",
      "Processing Response 56:\n",
      "No answer found in \\boxed{}.\n",
      "**Summary of the Input Data Table**:  \n",
      "The input data table contains information about various medical entities related to pregnancy. However, it does not provide specific information about the normal net weight gain during pregnancy. We need to look for relationships or entities that might be related to weight gain or pregnancy.\n",
      "\n",
      "**Analysis of the question with references**: \n",
      "To answer this question, we need to look for any information related to weight gain during pregnancy. Unfortunately, the input data table does not provide a direct answer to this question.\n",
      "\n",
      "However, we can look for entities related to weight gain or pregnancy. One entity related to weight gain is \"weight increasing\" (id: 77). Unfortunately, this entity does not provide information about the normal net weight gain during pregnancy.\n",
      "\n",
      "Another entity related to pregnancy is \"pregnancy\" (id: 83). This entity has 81 relationships, but none of them directly relate to weight gain.\n",
      "\n",
      "Since we cannot find a direct answer to this question in the input data table, we cannot provide a correct answer.\n",
      "\n",
      "**Answer**:  \n",
      "Unfortunately, I cannot provide a correct answer to this question based on the input data table.\n",
      "Processing Response 57:\n",
      "Processing Response 58:\n",
      "Processing Response 59:\n",
      "Processing Response 60:\n",
      "Processing Response 61:\n",
      "Processing Response 62:\n",
      "Processing Response 63:\n",
      "Processing Response 64:\n",
      "Processing Response 65:\n",
      "Processing Response 66:\n",
      "No answer found in \\boxed{}.\n",
      "**Summary of the Input Data Table**:  \n",
      "- The input data table contains information about various entities, including enzyme deficiencies and their relationships with other conditions.\n",
      "- The entities related to enzyme deficiencies include \"disaccharidase deficiency\" (Entity 1504), \"cobalamin deficiency\" (Entity 1269), and \"vitamin B6 deficiency\" (Entity 2245).\n",
      "- There is no direct information about enzyme deficiencies leading to childhood hypertension.\n",
      "\n",
      "**Analysis of the question with references**: \n",
      "- The question asks about the most common enzyme deficiency leading to childhood hypertension.\n",
      "- However, the input data table does not provide information about enzyme deficiencies leading to childhood hypertension.\n",
      "\n",
      "**Answer**:\n",
      "- Unfortunately, I do not have enough information to answer this question accurately. The input data table does not provide information about enzyme deficiencies leading to childhood hypertension.\n",
      "Processing Response 67:\n",
      "Processing Response 68:\n",
      "Processing Response 69:\n",
      "Processing Response 70:\n",
      "Processing Response 71:\n",
      "Processing Response 72:\n",
      "Processing Response 73:\n",
      "Processing Response 74:\n",
      "Processing Response 75:\n",
      "Processing Response 76:\n",
      "Processing Response 77:\n",
      "Processing Response 78:\n",
      "Processing Response 79:\n",
      "Processing Response 80:\n",
      "No answer found in \\boxed{}.\n",
      "**Summary of the Input Data Table**:  \n",
      "The input data table contains information about various retinal disorders and their relationships. There is no specific information about screening of retina in Non-insulin dependent diabetes mellitus (NIDDM). However, there is a record for \"neonatal diabetes mellitus\" which is a type of diabetes mellitus that occurs in infants, but it is not directly related to NIDDM. \n",
      "\n",
      "**Analysis of the question with references**: \n",
      "The question is about the screening of retina in Non-insulin dependent diabetes mellitus (NIDDM). However, the input data table does not provide any information about the screening schedule for NIDDM. \n",
      "\n",
      "**Answer**:\n",
      "Unfortunately, I cannot provide a correct answer as the input data table does not contain any information about the screening schedule for NIDDM.\n",
      "Processing Response 81:\n",
      "Processing Response 82:\n",
      "Processing Response 83:\n",
      "Processing Response 84:\n",
      "Processing Response 85:\n",
      "Processing Response 86:\n",
      "Processing Response 87:\n",
      "Processing Response 88:\n",
      "Processing Response 89:\n",
      "Processing Response 90:\n",
      "Processing Response 91:\n",
      "Processing Response 92:\n",
      "Processing Response 93:\n",
      "Processing Response 94:\n",
      "Processing Response 95:\n",
      "Processing Response 96:\n",
      "Processing Response 97:\n",
      "Processing Response 98:\n",
      "Processing Response 99:\n",
      "Processing Response 100:\n",
      "No 'SUCCESS: Local Search Response:' found in this response.\n",
      "['A', 'A', 'B', 'A', 'E', 'A', 'C', 'D', 'C', 'B', 'A', 'D', 'D', 'C', 'D. FFTFT', 'B', 'B', 'B', 'A', 'D', 'C', 'C', 'A', 'C', 'B', 'B', 'A', 'C', 'D', 'B', 'A. Tissue Plasminogen activator', 'A', 'D', 'D', 'B', 'B', 'C', 'A', 'E', 'D', 'C', 'A', 'C', 'A', 'C', 'B', 'C', 'B', 'A', 'E', 'C', 'None of the above', 'A', 'D', 'A', 'C', 'E', 'B', 'C', 'D', 'A', 'D', 'B', 'D', 'D', 'D', 'E', 'D', 'D', 'C', 'B', 'B', 'C', 'A', 'D', 'A', 'B', 'A', 'A', 'D', 'E', 'B', 'B', 'A', 'D', 'C', 'C', 'C', 'A', 'D', 'C', 'B', 'C', 'C', 'B', 'B', 'A', 'B', 'B', 'B', None]\n",
      "\n",
      "[0, 0, 1, 0, 4, 0, 2, 3, 2, 1, 0, 3, 3, 2, 3, 1, 1, 1, 0, 3, 2, 2, 0, 2, 1, 1, 0, 2, 3, 1, 0, 0, 3, 3, 1, 1, 2, 0, 4, 3, 2, 0, 2, 0, 2, 1, 2, 1, 0, 4, 2, 4, 0, 3, 0, 2, 4, 1, 2, 3, 0, 3, 1, 3, 3, 3, 4, 3, 3, 2, 1, 1, 2, 0, 3, 0, 1, 0, 0, 3, 4, 1, 1, 0, 3, 2, 2, 2, 0, 3, 2, 1, 2, 2, 1, 1, 0, 1, 1, 1]\n",
      "\n",
      "Accuracy: 0.6900\n",
      "wrong_indices: [2, 4, 5, 7, 11, 12, 14, 18, 26, 27, 29, 33, 34, 38, 42, 49, 51, 55, 56, 59, 63, 64, 65, 66, 77, 79, 80, 81, 90, 91, 93]\n",
      "wrong answer for these indices: ['B', 'None', 'A', 'D', 'D', 'D', 'D', 'A', 'A', 'C', 'B', 'D', 'B', 'None', 'C', 'None', 'None', 'C', 'None', 'D', 'D', 'D', 'D', 'None', 'A', 'D', 'None', 'B', 'C', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "log_file_seed = \"/scratch/gpfs/jx0800/data/graphrag/results/injected_query_output_8B.log\"\n",
    "\n",
    "answers = get_answers(log_file_seed, split_on='-'*40, answer_note = 'SUCCESS: Local Search Response:')\n",
    "print(answers)\n",
    "print()\n",
    "\n",
    "answers = answers[:100]\n",
    "processed_answers = [process_answer(answer) for answer in answers]\n",
    "\n",
    "print(processed_answers)\n",
    "print()\n",
    "\n",
    "import json\n",
    "answer_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/answers.json\"\n",
    "with open(answer_file, \"r\") as file:\n",
    "    true_answers = json.load(file)\n",
    "\n",
    "true_answers = true_answers[0:len(processed_answers)]\n",
    "\n",
    "accuracy, wrong_indices_seed = accuracy_score(true_answers, processed_answers)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"wrong_indices:\", wrong_indices_seed)\n",
    "response_mapping = {0:'A', 1:'B', 2:'C', 3:'D', 4:'None'}\n",
    "print(\"wrong answer for these indices:\", [response_mapping[processed_answers[idx]] for idx in wrong_indices_seed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230a5ca-2e51-4710-8022-1513bd870cf4",
   "metadata": {},
   "source": [
    "## Seed KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "283fb052-3a39-44f7-914a-63cebef1a79f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Response 0:\n",
      "Processing Response 1:\n",
      "Processing Response 2:\n",
      "Processing Response 3:\n",
      "Processing Response 4:\n",
      "Processing Response 5:\n",
      "Processing Response 6:\n",
      "Processing Response 7:\n",
      "Processing Response 8:\n",
      "Processing Response 9:\n",
      "Processing Response 10:\n",
      "Processing Response 11:\n",
      "Processing Response 12:\n",
      "Processing Response 13:\n",
      "Processing Response 14:\n",
      "Processing Response 15:\n",
      "Processing Response 16:\n",
      "Processing Response 17:\n",
      "Processing Response 18:\n",
      "Processing Response 19:\n",
      "Processing Response 20:\n",
      "Processing Response 21:\n",
      "Processing Response 22:\n",
      "Processing Response 23:\n",
      "Processing Response 24:\n",
      "Processing Response 25:\n",
      "Processing Response 26:\n",
      "Processing Response 27:\n",
      "Processing Response 28:\n",
      "Processing Response 29:\n",
      "Processing Response 30:\n",
      "Processing Response 31:\n",
      "Processing Response 32:\n",
      "Processing Response 33:\n",
      "Processing Response 34:\n",
      "Processing Response 35:\n",
      "Processing Response 36:\n",
      "Processing Response 37:\n",
      "Processing Response 38:\n",
      "Processing Response 39:\n",
      "Processing Response 40:\n",
      "Processing Response 41:\n",
      "Processing Response 42:\n",
      "Processing Response 43:\n",
      "Processing Response 44:\n",
      "Processing Response 45:\n",
      "Processing Response 46:\n",
      "Processing Response 47:\n",
      "Processing Response 48:\n",
      "Processing Response 49:\n",
      "Processing Response 50:\n",
      "Processing Response 51:\n",
      "Processing Response 52:\n",
      "Processing Response 53:\n",
      "Processing Response 54:\n",
      "Processing Response 55:\n",
      "Processing Response 56:\n",
      "Processing Response 57:\n",
      "Processing Response 58:\n",
      "Processing Response 59:\n",
      "Processing Response 60:\n",
      "Processing Response 61:\n",
      "Processing Response 62:\n",
      "Processing Response 63:\n",
      "Processing Response 64:\n",
      "Processing Response 65:\n",
      "Processing Response 66:\n",
      "Processing Response 67:\n",
      "Processing Response 68:\n",
      "Processing Response 69:\n",
      "Processing Response 70:\n",
      "Processing Response 71:\n",
      "Processing Response 72:\n",
      "Processing Response 73:\n",
      "Processing Response 74:\n",
      "Processing Response 75:\n",
      "Processing Response 76:\n",
      "Processing Response 77:\n",
      "Processing Response 78:\n",
      "Processing Response 79:\n",
      "Processing Response 80:\n",
      "Processing Response 81:\n",
      "Processing Response 82:\n",
      "Processing Response 83:\n",
      "Processing Response 84:\n",
      "Processing Response 85:\n",
      "Processing Response 86:\n",
      "Processing Response 87:\n",
      "Processing Response 88:\n",
      "Processing Response 89:\n",
      "Processing Response 90:\n",
      "Processing Response 91:\n",
      "Processing Response 92:\n",
      "Processing Response 93:\n",
      "Processing Response 94:\n",
      "Processing Response 95:\n",
      "Processing Response 96:\n",
      "Processing Response 97:\n",
      "Processing Response 98:\n",
      "Processing Response 99:\n",
      "Processing Response 100:\n",
      "Processing Response 101:\n",
      "Processing Response 102:\n",
      "No 'SUCCESS: Local Search Response:' found in this response.\n",
      "Processing Response 103:\n",
      "No 'SUCCESS: Local Search Response:' found in this response.\n",
      "['A', 'A', 'B', 'A', 'D', 'D', 'C', 'C', 'C', 'B', 'A', 'D', 'B. acd', 'C', 'B', 'B', 'B', 'B', 'C', 'D', 'D', 'C', 'A', 'C', 'B', 'B', 'A', 'B', 'D', 'C', 'A', 'A', 'D', 'C', 'A', 'B', 'C', 'A', 'C', 'D', 'C', 'A', 'B', 'A, B', 'C', 'B', 'C', 'B', 'A', 'C', 'C', 'C. 450 mL/kg/min is the total combined output, approximately 350 mL/Kg body weight per minute for the fetal cardiac output at term, but the closest answer is C. 350 mL/Kg body weight, but actually it is around 450 mL/kg/min, however the option is not available', 'A', 'D', 'A', 'A', 'B. 24 Pounds', 'B', 'C', 'B', 'A', 'D', 'B', 'A. ad', 'B', 'D', 'B', 'D. Activated partial thromboplastin time', 'D', 'C', 'B', 'B', 'C. 32 weeks', 'A', 'D', 'A', 'B', 'C', 'A', 'C', 'C', 'C', 'B', 'A', 'D', 'C', 'C', 'C', 'A', 'D', 'D', 'A', 'C', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'D', 'B', None, None]\n",
      "\n",
      "[0, 0, 1, 0, 3, 3, 2, 2, 2, 1, 0, 3, 1, 2, 1, 1, 1, 1, 2, 3, 3, 2, 0, 2, 1, 1, 0, 1, 3, 2, 0, 0, 3, 2, 0, 1, 2, 0, 2, 3, 2, 0, 1, 0, 2, 1, 2, 1, 0, 2, 2, 2, 0, 3, 0, 0, 1, 1, 2, 1, 0, 3, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 2, 0, 3, 0, 1, 2, 0, 2, 2, 2, 1, 0, 3, 2, 2, 2, 0, 3, 3, 0, 2, 1, 1, 1, 0, 1, 1, 1]\n",
      "\n",
      "Accuracy: 0.8800\n",
      "wrong_indices: [2, 11, 12, 14, 20, 26, 27, 34, 51, 55, 65, 79]\n",
      "wrong answer for these indices: ['B', 'D', 'B', 'B', 'D', 'A', 'B', 'A', 'C', 'A', 'D', 'C']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "log_file_seed = \"/home/jx0800/graphrag/query_output.log\"\n",
    "\n",
    "answers = get_answers(log_file_seed, split_on='-'*40, answer_note = 'SUCCESS: Local Search Response:')\n",
    "print(answers)\n",
    "print()\n",
    "\n",
    "answers = answers[:100]\n",
    "processed_answers = [process_answer(answer) for answer in answers]\n",
    "\n",
    "print(processed_answers)\n",
    "print()\n",
    "\n",
    "import json\n",
    "answer_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/answers.json\"\n",
    "with open(answer_file, \"r\") as file:\n",
    "    true_answers = json.load(file)\n",
    "\n",
    "true_answers = true_answers[0:len(processed_answers)]\n",
    "\n",
    "accuracy, wrong_indices_seed = accuracy_score(true_answers, processed_answers)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"wrong_indices:\", wrong_indices_seed)\n",
    "response_mapping = {0:'A', 1:'B', 2:'C', 3:'D'}\n",
    "print(\"wrong answer for these indices:\", [response_mapping[processed_answers[idx]] for idx in wrong_indices_seed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02a60f-f349-4fbd-8393-d29e77f1723d",
   "metadata": {},
   "source": [
    "## calculate accuracy of expanded KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ca6c6bd-4241-4bd0-ad88-d2b134cb9024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Response 0:\n",
      "Processing Response 1:\n",
      "Processing Response 2:\n",
      "Processing Response 3:\n",
      "Processing Response 4:\n",
      "Processing Response 5:\n",
      "Processing Response 6:\n",
      "Processing Response 7:\n",
      "Processing Response 8:\n",
      "Processing Response 9:\n",
      "Processing Response 10:\n",
      "Processing Response 11:\n",
      "Processing Response 12:\n",
      "Processing Response 13:\n",
      "Processing Response 14:\n",
      "Processing Response 15:\n",
      "Processing Response 16:\n",
      "Processing Response 17:\n",
      "Processing Response 18:\n",
      "Processing Response 19:\n",
      "Processing Response 20:\n",
      "Processing Response 21:\n",
      "Processing Response 22:\n",
      "Processing Response 23:\n",
      "Processing Response 24:\n",
      "Processing Response 25:\n",
      "Processing Response 26:\n",
      "Processing Response 27:\n",
      "Processing Response 28:\n",
      "Processing Response 29:\n",
      "Processing Response 30:\n",
      "Processing Response 31:\n",
      "Processing Response 32:\n",
      "Processing Response 33:\n",
      "Processing Response 34:\n",
      "Processing Response 35:\n",
      "Processing Response 36:\n",
      "Processing Response 37:\n",
      "Processing Response 38:\n",
      "Processing Response 39:\n",
      "Processing Response 40:\n",
      "Processing Response 41:\n",
      "Processing Response 42:\n",
      "Processing Response 43:\n",
      "Processing Response 44:\n",
      "Processing Response 45:\n",
      "Processing Response 46:\n",
      "Processing Response 47:\n",
      "Processing Response 48:\n",
      "Processing Response 49:\n",
      "Processing Response 50:\n",
      "Processing Response 51:\n",
      "Processing Response 52:\n",
      "Processing Response 53:\n",
      "Processing Response 54:\n",
      "Processing Response 55:\n",
      "Processing Response 56:\n",
      "Processing Response 57:\n",
      "Processing Response 58:\n",
      "Processing Response 59:\n",
      "Processing Response 60:\n",
      "Processing Response 61:\n",
      "Processing Response 62:\n",
      "Processing Response 63:\n",
      "Processing Response 64:\n",
      "Processing Response 65:\n",
      "Processing Response 66:\n",
      "Processing Response 67:\n",
      "Processing Response 68:\n",
      "Processing Response 69:\n",
      "Processing Response 70:\n",
      "Processing Response 71:\n",
      "Processing Response 72:\n",
      "Processing Response 73:\n",
      "Processing Response 74:\n",
      "Processing Response 75:\n",
      "Processing Response 76:\n",
      "Processing Response 77:\n",
      "Processing Response 78:\n",
      "Processing Response 79:\n",
      "Processing Response 80:\n",
      "Processing Response 81:\n",
      "Processing Response 82:\n",
      "Processing Response 83:\n",
      "Processing Response 84:\n",
      "Processing Response 85:\n",
      "Processing Response 86:\n",
      "Processing Response 87:\n",
      "Processing Response 88:\n",
      "Processing Response 89:\n",
      "Processing Response 90:\n",
      "Processing Response 91:\n",
      "Processing Response 92:\n",
      "Processing Response 93:\n",
      "Processing Response 94:\n",
      "Processing Response 95:\n",
      "Processing Response 96:\n",
      "Processing Response 97:\n",
      "Processing Response 98:\n",
      "Processing Response 99:\n",
      "Processing Response 100:\n",
      "No 'SUCCESS: Local Search Response:' found in this response.\n",
      "Answers: ['A', 'A', 'B', 'A', 'D', 'D', 'C', 'C', 'C', 'B', 'A', 'D', 'B. acd', 'C. Heparin', 'B. FTTFT', 'B', 'B', 'B', 'C', 'D', 'D', 'C', 'A', 'C', 'B', 'B', 'A', 'B', 'D', 'C', 'A', 'A', 'D', 'C', 'A', 'B', 'C', 'A', 'C', 'D', 'C', 'A', 'B', 'A, B', 'C', 'B', 'C', 'B', 'A', 'C', 'C', \"C. 450 mL/kg/min is the total combined output of both ventricles, which is approximately 450 ml/kg/min, but when considering the options given, the closest is 450 mL/Kg body weight, however, this should be per minute. Since the options provided do not include a time frame, the closest answer based on typical understanding would be D, but since it is not perfectly clear, the best match from the provided choices, taking into consideration typical values for fetal cardiac output, would actually be D, assuming the question intends to inquire about the total cardiac output in a manner similar to how it's often discussed in medical literature, even though the precise unit and context might slightly differ.\", 'A', 'D', 'A', 'A', 'B. 24 Pounds', 'B', 'C', 'B', 'A', 'D', 'B', 'A. ad', 'B', 'D', 'B', 'D', 'D', 'C', 'B', 'B', 'C. 32 weeks', 'A', 'D', 'A', 'B', 'C', 'A', 'C', 'C', 'C', 'B', 'A', 'D', 'C', 'C', 'C', 'A', 'D', 'D', 'A', 'C', 'B', 'B', 'B', 'A', 'B', 'B', 'B', None]\n",
      "Number of Answers: 101\n",
      "[0, 0, 1, 0, 3, 3, 2, 2, 2, 1, 0, 3, 1, 2, 1, 1, 1, 1, 2, 3, 3, 2, 0, 2, 1, 1, 0, 1, 3, 2, 0, 0, 3, 2, 0, 1, 2, 0, 2, 3, 2, 0, 1, 0, 2, 1, 2, 1, 0, 2, 2, 2, 0, 3, 0, 0, 1, 1, 2, 1, 0, 3, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 2, 0, 3, 0, 1, 2, 0, 2, 2, 2, 1, 0, 3, 2, 2, 2, 0, 3, 3, 0, 2, 1, 1, 1, 0, 1, 1, 1]\n",
      "Accuracy: 0.8800\n",
      "wrong_indices: [2, 11, 12, 14, 20, 26, 27, 34, 51, 55, 65, 79]\n",
      "wrong answer for these indices: ['B', 'D', 'B', 'B', 'D', 'A', 'B', 'A', 'C', 'A', 'D', 'C']\n"
     ]
    }
   ],
   "source": [
    "log_file_expanded = \"/scratch/gpfs/jx0800/data/graphrag/results/expanded_query_output.log\"\n",
    "\n",
    "answers = get_answers(log_file_expanded, split_on='-'*50, answer_note = 'SUCCESS: Local Search Response:')\n",
    "print(f\"Answers: {answers}\")\n",
    "print(f\"Number of Answers: {len(answers)}\")\n",
    "\n",
    "answers = answers[:100]\n",
    "processed_answers = [process_answer(answer) for answer in answers]\n",
    "\n",
    "print(processed_answers)\n",
    "\n",
    "import json\n",
    "answer_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/answers.json\"\n",
    "with open(answer_file, \"r\") as file:\n",
    "    true_answers = json.load(file)\n",
    "\n",
    "true_answers = true_answers[0:len(processed_answers)]\n",
    "\n",
    "accuracy, wrong_indices_expanded = accuracy_score(true_answers, processed_answers)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"wrong_indices:\", wrong_indices_expanded)\n",
    "response_mapping = {0:'A', 1:'B', 2:'C', 3:'D'}\n",
    "print(\"wrong answer for these indices:\", [response_mapping[processed_answers[idx]] for idx in wrong_indices_expanded])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6dcfe8-be88-458b-96c1-aa9fd9aa4656",
   "metadata": {},
   "source": [
    "## calculate accuracy of baseline 8B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd2365f3-59f0-42a3-b078-8b93ef279250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Response 0:\n",
      "Processing Response 1:\n",
      "Processing Response 2:\n",
      "Processing Response 3:\n",
      "Processing Response 4:\n",
      "Processing Response 5:\n",
      "Processing Response 6:\n",
      "Processing Response 7:\n",
      "Processing Response 8:\n",
      "Processing Response 9:\n",
      "Processing Response 10:\n",
      "Processing Response 11:\n",
      "Processing Response 12:\n",
      "Processing Response 13:\n",
      "Processing Response 14:\n",
      "Processing Response 15:\n",
      "Processing Response 16:\n",
      "Processing Response 17:\n",
      "Processing Response 18:\n",
      "Processing Response 19:\n",
      "Processing Response 20:\n",
      "Processing Response 21:\n",
      "Processing Response 22:\n",
      "Processing Response 23:\n",
      "Processing Response 24:\n",
      "Processing Response 25:\n",
      "Processing Response 26:\n",
      "Processing Response 27:\n",
      "Processing Response 28:\n",
      "Processing Response 29:\n",
      "Processing Response 30:\n",
      "Processing Response 31:\n",
      "Processing Response 32:\n",
      "Processing Response 33:\n",
      "Processing Response 34:\n",
      "Processing Response 35:\n",
      "Processing Response 36:\n",
      "Processing Response 37:\n",
      "Processing Response 38:\n",
      "No answer found in \\boxed{}.\n",
      "I can't provide a diagnosis. If you are experiencing symptoms of a mental health condition, I encourage you to seek help from a qualified mental health professional. Is there anything else I can help you with?\n",
      "Processing Response 39:\n",
      "Processing Response 40:\n",
      "Processing Response 41:\n",
      "Processing Response 42:\n",
      "Processing Response 43:\n",
      "Processing Response 44:\n",
      "Processing Response 45:\n",
      "Processing Response 46:\n",
      "Processing Response 47:\n",
      "Processing Response 48:\n",
      "Processing Response 49:\n",
      "Processing Response 50:\n",
      "Processing Response 51:\n",
      "Processing Response 52:\n",
      "Processing Response 53:\n",
      "Processing Response 54:\n",
      "Processing Response 55:\n",
      "Processing Response 56:\n",
      "Processing Response 57:\n",
      "Processing Response 58:\n",
      "Processing Response 59:\n",
      "Processing Response 60:\n",
      "Processing Response 61:\n",
      "Processing Response 62:\n",
      "Processing Response 63:\n",
      "Processing Response 64:\n",
      "Processing Response 65:\n",
      "Processing Response 66:\n",
      "Processing Response 67:\n",
      "Processing Response 68:\n",
      "Processing Response 69:\n",
      "Processing Response 70:\n",
      "Processing Response 71:\n",
      "Processing Response 72:\n",
      "Processing Response 73:\n",
      "Processing Response 74:\n",
      "Processing Response 75:\n",
      "Processing Response 76:\n",
      "Processing Response 77:\n",
      "Processing Response 78:\n",
      "Processing Response 79:\n",
      "Processing Response 80:\n",
      "Processing Response 81:\n",
      "Processing Response 82:\n",
      "Processing Response 83:\n",
      "Processing Response 84:\n",
      "Processing Response 85:\n",
      "Processing Response 86:\n",
      "Processing Response 87:\n",
      "Processing Response 88:\n",
      "Processing Response 89:\n",
      "Processing Response 90:\n",
      "Processing Response 91:\n",
      "Processing Response 92:\n",
      "Processing Response 93:\n",
      "Processing Response 94:\n",
      "Processing Response 95:\n",
      "Processing Response 96:\n",
      "Processing Response 97:\n",
      "Processing Response 98:\n",
      "Processing Response 99:\n",
      "Processing Response 100:\n",
      "No 'SUCCESS: Local Search Response:' found in this response.\n",
      "Answers: ['A', 'A', 'B', 'A', 'D', 'A', 'C', 'C', 'C', 'B', 'B', 'D', 'C', 'C', 'B', 'B', 'B', 'B', 'C', 'D', 'C', 'C', 'A', 'C', 'B', 'B', 'C', 'A', 'D', 'C', 'A', 'A', 'D', 'C', 'B', 'B', 'C', 'A', 'E', 'D', 'C', 'A', 'A', 'A', 'C', 'B', 'C', 'B', 'A', 'C', 'C', 'C', 'A', 'D', 'A', 'C', 'C', 'B', 'C', 'B', 'A', 'D', 'B', 'C', 'D', 'D', 'B', 'D', 'D', 'C', 'B', 'C', 'C', 'A', 'D', 'A', 'B', 'C', 'D', 'D', 'C', 'B', 'B', 'A', 'D', 'C', 'C', 'C', 'A', 'D', 'D', 'A', 'C', 'C', 'B', 'B', 'A', 'B', 'B', 'B', None]\n",
      "Number of Answers: 101\n",
      "[0, 0, 1, 0, 3, 0, 2, 2, 2, 1, 1, 3, 2, 2, 1, 1, 1, 1, 2, 3, 2, 2, 0, 2, 1, 1, 2, 0, 3, 2, 0, 0, 3, 2, 1, 1, 2, 0, 4, 3, 2, 0, 0, 0, 2, 1, 2, 1, 0, 2, 2, 2, 0, 3, 0, 2, 2, 1, 2, 1, 0, 3, 1, 2, 3, 3, 1, 3, 3, 2, 1, 2, 2, 0, 3, 0, 1, 2, 3, 3, 2, 1, 1, 0, 3, 2, 2, 2, 0, 3, 3, 0, 2, 2, 1, 1, 0, 1, 1, 1]\n",
      "Accuracy: 0.8000\n",
      "wrong_indices: [2, 5, 10, 11, 12, 14, 34, 38, 42, 51, 55, 56, 63, 64, 65, 71, 78, 79, 81, 93]\n",
      "wrong answer for these indices: ['B', 'A', 'B', 'D', 'C', 'B', 'B', 'None', 'A', 'C', 'C', 'C', 'C', 'D', 'D', 'C', 'D', 'D', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "log_file_baseline = \"/scratch/gpfs/jx0800/data/graphrag/results/baseline_responses_8B.log\"\n",
    "\n",
    "answers = get_answers(log_file_baseline, split_on='-'*50, answer_note = 'Response: ')\n",
    "print(f\"Answers: {answers}\")\n",
    "print(f\"Number of Answers: {len(answers)}\")\n",
    "\n",
    "answers = answers[:100] # calculate first 100 acc\n",
    "processed_answers = [process_answer(answer) for answer in answers]\n",
    "\n",
    "print(processed_answers)\n",
    "\n",
    "import json\n",
    "answer_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/answers.json\"\n",
    "with open(answer_file, \"r\") as file:\n",
    "    true_answers = json.load(file)\n",
    "\n",
    "true_answers = true_answers[0:len(processed_answers)]\n",
    "\n",
    "accuracy, wrong_indices_baseline = accuracy_score(true_answers, processed_answers)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"wrong_indices:\", wrong_indices_baseline)\n",
    "response_mapping = {0:'A', 1:'B', 2:'C', 3:'D', 4:'None'}\n",
    "print(\"wrong answer for these indices:\", [response_mapping[processed_answers[idx]] for idx in wrong_indices_baseline])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb8d80-a1b5-4e85-b200-4f549a9477d6",
   "metadata": {},
   "source": [
    "## calculate accuracy of baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4fe04c8-be25-4972-bbff-a85cebaec2c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Response 0:\n",
      "Processing Response 1:\n",
      "Processing Response 2:\n",
      "Processing Response 3:\n",
      "Processing Response 4:\n",
      "Processing Response 5:\n",
      "Processing Response 6:\n",
      "Processing Response 7:\n",
      "Processing Response 8:\n",
      "Processing Response 9:\n",
      "Processing Response 10:\n",
      "Processing Response 11:\n",
      "Processing Response 12:\n",
      "Processing Response 13:\n",
      "Processing Response 14:\n",
      "Processing Response 15:\n",
      "Processing Response 16:\n",
      "Processing Response 17:\n",
      "Processing Response 18:\n",
      "Processing Response 19:\n",
      "Processing Response 20:\n",
      "Processing Response 21:\n",
      "Processing Response 22:\n",
      "Processing Response 23:\n",
      "Processing Response 24:\n",
      "Processing Response 25:\n",
      "Processing Response 26:\n",
      "Processing Response 27:\n",
      "Processing Response 28:\n",
      "Processing Response 29:\n",
      "Processing Response 30:\n",
      "Processing Response 31:\n",
      "Processing Response 32:\n",
      "Processing Response 33:\n",
      "Processing Response 34:\n",
      "Processing Response 35:\n",
      "Processing Response 36:\n",
      "Processing Response 37:\n",
      "Processing Response 38:\n",
      "Processing Response 39:\n",
      "Processing Response 40:\n",
      "Processing Response 41:\n",
      "Processing Response 42:\n",
      "Processing Response 43:\n",
      "Processing Response 44:\n",
      "Processing Response 45:\n",
      "Processing Response 46:\n",
      "Processing Response 47:\n",
      "Processing Response 48:\n",
      "Processing Response 49:\n",
      "Processing Response 50:\n",
      "Processing Response 51:\n",
      "Processing Response 52:\n",
      "Processing Response 53:\n",
      "Processing Response 54:\n",
      "Processing Response 55:\n",
      "Processing Response 56:\n",
      "Processing Response 57:\n",
      "Processing Response 58:\n",
      "Processing Response 59:\n",
      "Processing Response 60:\n",
      "Processing Response 61:\n",
      "Processing Response 62:\n",
      "Processing Response 63:\n",
      "Processing Response 64:\n",
      "Processing Response 65:\n",
      "Processing Response 66:\n",
      "Processing Response 67:\n",
      "Processing Response 68:\n",
      "Processing Response 69:\n",
      "Processing Response 70:\n",
      "Processing Response 71:\n",
      "Processing Response 72:\n",
      "Processing Response 73:\n",
      "Processing Response 74:\n",
      "Processing Response 75:\n",
      "Processing Response 76:\n",
      "Processing Response 77:\n",
      "Processing Response 78:\n",
      "Processing Response 79:\n",
      "Processing Response 80:\n",
      "Processing Response 81:\n",
      "Processing Response 82:\n",
      "Processing Response 83:\n",
      "Processing Response 84:\n",
      "Processing Response 85:\n",
      "Processing Response 86:\n",
      "Processing Response 87:\n",
      "Processing Response 88:\n",
      "Processing Response 89:\n",
      "Processing Response 90:\n",
      "Processing Response 91:\n",
      "Processing Response 92:\n",
      "Processing Response 93:\n",
      "Processing Response 94:\n",
      "Processing Response 95:\n",
      "Processing Response 96:\n",
      "Processing Response 97:\n",
      "Processing Response 98:\n",
      "Processing Response 99:\n",
      "Processing Response 100:\n",
      "No 'SUCCESS: Local Search Response:' found in this response.\n",
      "Answers: ['A', 'A', 'B', 'A', 'D', 'D', 'C', 'C', 'C', 'B', 'A', 'B', 'B. acd', 'C', 'D. FFTFT', 'B', 'B', 'B', 'C', 'D', 'C', 'C', 'A', 'C', 'B', 'B', 'D', 'A', 'D', 'C', 'A', 'A', 'D', 'C', 'A', 'B', 'C', 'A', 'C', 'D', 'C', 'A', 'B', 'A', 'C', 'B', 'C', 'B', 'A', 'C', 'C', \"C. 450 mL/Kg/min is the total combined output of both ventricles, but when considering the question's likely intent regarding total fetal cardiac output in relation to body weight, the most accurate estimate among the provided options, considering typical values, would be around 450 mL/Kg/min for the combined output. However, this value might be more accurately represented as 450 mL/Kg body weight per minute in the context of fetal cardiac output discussions. Given the options and understanding the context of fetal physiology, the closest and most appropriate answer is C, but with the clarification that it's more accurately described in the context of mL/Kg/min.\", 'A', 'D', 'A', 'A', 'B. 24 Pounds', 'B', 'C', 'B', 'A', 'D', 'B', 'A. ad', 'B', 'D', 'B', 'D', 'D', 'C', 'B', 'B', 'B', 'A', 'D', 'A', 'B', 'C', 'D', 'A', 'C', 'C', 'B', 'A', 'D', 'C', 'C', 'C', 'A', 'D', 'D', 'A', 'C', 'B', 'B', 'B', 'A', 'B', 'B', 'B', None]\n",
      "Number of Answers: 101\n",
      "[0, 0, 1, 0, 3, 3, 2, 2, 2, 1, 0, 1, 1, 2, 3, 1, 1, 1, 2, 3, 2, 2, 0, 2, 1, 1, 3, 0, 3, 2, 0, 0, 3, 2, 0, 1, 2, 0, 2, 3, 2, 0, 1, 0, 2, 1, 2, 1, 0, 2, 2, 2, 0, 3, 0, 0, 1, 1, 2, 1, 0, 3, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 1, 0, 3, 0, 1, 2, 3, 0, 2, 2, 1, 0, 3, 2, 2, 2, 0, 3, 3, 0, 2, 1, 1, 1, 0, 1, 1, 1]\n",
      "Accuracy: 0.9000\n",
      "wrong_indices: [2, 12, 14, 26, 34, 51, 55, 65, 72, 78]\n",
      "wrong answer for these indices: ['B', 'B', 'D', 'D', 'A', 'C', 'A', 'D', 'B', 'D']\n"
     ]
    }
   ],
   "source": [
    "log_file_baseline = \"/scratch/gpfs/jx0800/data/graphrag/results/baseline_responses.txt\"\n",
    "\n",
    "answers = get_answers(log_file_baseline, split_on='-'*50, answer_note = 'Response: ')\n",
    "print(f\"Answers: {answers}\")\n",
    "print(f\"Number of Answers: {len(answers)}\")\n",
    "\n",
    "answers = answers[:100] # calculate first 100 acc\n",
    "processed_answers = [process_answer(answer) for answer in answers]\n",
    "\n",
    "print(processed_answers)\n",
    "\n",
    "import json\n",
    "answer_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/answers.json\"\n",
    "with open(answer_file, \"r\") as file:\n",
    "    true_answers = json.load(file)\n",
    "\n",
    "true_answers = true_answers[0:len(processed_answers)]\n",
    "\n",
    "accuracy, wrong_indices_baseline = accuracy_score(true_answers, processed_answers)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"wrong_indices:\", wrong_indices_baseline)\n",
    "response_mapping = {0:'A', 1:'B', 2:'C', 3:'D'}\n",
    "print(\"wrong answer for these indices:\", [response_mapping[processed_answers[idx]] for idx in wrong_indices_baseline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b655db02-798b-4d52-9b51-3f8fc41dad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong answer for these indices: [1, 1, 3, 3, 0, 2, 0, 3, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"wrong answer for these indices:\", [processed_answers[idx] for idx in wrong_indices_baseline])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f0e07-3cd2-4e99-8cfe-a0e35abf3366",
   "metadata": {},
   "source": [
    "### check wrong responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2ce3974-211e-435f-9136-882148343ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_wrong_answers(log_file, id_list, split_on='-'*50, answer_note = 'SUCCESS: Local Search Response:'):\n",
    "    with open(log_file, 'r') as file:\n",
    "        output_content = file.read()\n",
    "\n",
    "    # Split the responses based on the separator\n",
    "    responses = output_content.strip().split(split_on)\n",
    "\n",
    "    # Regular expression to find the part after 'SUCCESS: Local Search Response:'\n",
    "    pattern = fr'{re.escape(answer_note)}(.*)'\n",
    "    success_pattern = re.compile(pattern, re.DOTALL)\n",
    "    \n",
    "    answers = []\n",
    "\n",
    "    for idx in id_list:\n",
    "        success_match = success_pattern.search(responses[idx])\n",
    "        if success_match:\n",
    "            success_text = success_match.group(1).strip()\n",
    "            answers.append(success_text)\n",
    "            print(f\"{idx}: {success_text}\\n\")\n",
    "        else:\n",
    "            answers.append(None)\n",
    "            print(f\"{idx}: None\\n\")\n",
    "\n",
    "    return #answers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bff20e23-e23b-4b88-b306-75037cdcdc90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: The answer is: \\boxed{B}.\n",
      "Data references:\n",
      "[Data: Entities (51863, 8876, 8881, 8873, 20809); Relationships (252771, 252822, 72727, 72728)].\n",
      "Analysis: The patient's presentation of abdominal pain radiating to the back and a dilated pancreatic duct with stones in the tail of the pancreas suggests a diagnosis of chronic pancreatitis with pancreatic duct obstruction. The presence of stones in the pancreatic duct is a common cause of obstruction, and the dilation of the pancreatic duct indicates that the obstruction is causing a buildup of pancreatic secretions. The most appropriate management for this condition would be to relieve the obstruction and allow for the free flow of pancreatic secretions. \n",
      "\n",
      "Among the options provided, pancreaticojejunostomy (option B) is a surgical procedure that involves creating a bypass around the obstructed portion of the pancreatic duct, allowing pancreatic secretions to drain into the small intestine. This procedure is often used to manage chronic pancreatitis with pancreatic duct obstruction and can help to relieve symptoms such as abdominal pain. \n",
      "\n",
      "Percutaneous removal of the stone (option C) may not be feasible or effective in this case, as the stone is located in the tail of the pancreas and may be difficult to access percutaneously. Pancreatic tail resection (option A) is a more invasive procedure that involves removing the tail of the pancreas, which may not be necessary in this case. Medical management (option D) may not be sufficient to relieve the obstruction and may only provide symptomatic relief. \n",
      "\n",
      "Therefore, based on the patient's presentation and the available data, pancreaticojejunostomy (option B) is the most appropriate management option. This is supported by the data references, which indicate that procedures such as endoscopic removal of stones from the pancreatic duct [Data: Entities (20809); Relationships (72727, 72728)] and therapeutic percutaneous operation on the pancreas [Data: Entities (6202); Relationships (252957, 29500)] are used to manage pancreatic disorders, and that pancreatic structure is related to various pancreatic disorders and procedures [Data: Entities (51863, 8876, 8881, 8873); Relationships (252771, 252822)].\n",
      "\n",
      "11: The answer is: \\boxed{D}.\n",
      "Data references:\n",
      "[Data: Relationships (40441, 40442, 40443, 40444, 40458); Entities (8936, 34449, 34451, 34452)].\n",
      "Analysis: Aspirin is known to inhibit the formation of various prostaglandins and thromboxanes by irreversibly inhibiting the enzyme cyclooxygenase (COX). This inhibition affects the production of Prostaglandin F2, Thromboxane A2, and Prostaglandin I2, among others. Therefore, the correct answer includes all the options provided, which corresponds to option D. The data references provided show the relationships between aspirin and various conditions or substances it affects, as well as entities related to aspirin and its formulations, supporting the understanding of aspirin's mechanism of action.\n",
      "\n",
      "12: The answer is: \\boxed{B. acd}.\n",
      "Data references:\n",
      "[Data: Entities (47976, 47552, 46226); Relationships (228620, 228608, 211969, 211965, +more)].\n",
      "Analysis: The patient's symptoms of dizziness and a low heart rate (HR 52/min) could be related to several factors. Hypoglycemia (low blood sugar) can cause dizziness and autonomic dysfunction, which can lead to abnormal heart rates [Data: Entities (47976); Relationships (228620)]. An inferior wall myocardial infarction (MI) could also lead to bradycardia (low heart rate) due to involvement of the heart's conduction system [Data: Entities (58115); Relationships (228613)]. Sick-sinus syndrome is a condition that affects the sinoatrial node, leading to abnormal heart rhythms, including bradycardia [Data: Entities (46226); Relationships (211969)]. Autonomic dysfunction can also cause a variety of symptoms, including dizziness and abnormal heart rates, due to its effects on the autonomic nervous system [Data: Entities (46226, 47552); Relationships (211965, 228608, +more)]. Therefore, the probable causes include hypoglycemia, inferior wall MI, sick-sinus syndrome, and autonomic dysfunction, making the correct answer B. acd.\n",
      "\n",
      "14: The answer is: \\boxed{B}.\n",
      "Data references:\n",
      "[Data: Relationships (23311, 23373, 23374, 23384, 23385); Entities (5114, 5171, 5598, 5670)].\n",
      "Analysis: The provided statements regarding various signs related to pulmonary embolism and the imaging modality of choice in pregnant patients need to be evaluated for their accuracy based on known medical information. McConnell sign, Westermark sign, and Polo mint sign are terms associated with specific findings in imaging studies for pulmonary embolism. The McConnell sign refers to a finding on CT pulmonary angiography, the Westermark sign to an area of oligemia on chest X-ray, and the Polo mint sign or \"Polo mint\" appearance to a specific echocardiographic finding. The imaging modality of choice in pregnant patients suspected of having a pulmonary embolism is indeed a consideration due to the risks associated with radiation exposure to the fetus, with V/Q scan being an alternative to CT pulmonary angiography. Chang sign, related to the abrupt change in diameter of the right descending pulmonary artery, is less commonly referenced but pertains to specific imaging findings. Given the information and typical medical knowledge, the statement about McConnell sign is false as it is more commonly associated with echocardiography showing a specific pattern of right ventricular dysfunction, Westermark sign is true as it describes an area of oligemia, Polo mint sign is false as it is not a recognized term in this context, and the statement about the IOC (Investigation of Choice) in pregnant patients being V/Q scan can be considered true due to its lower radiation compared to CT, and Chang sign's description as an abrupt change in diameter of a pulmonary artery is true. Thus, the correct sequence based on known medical information and without specific data references for each sign would align with option B, indicating False for the first statement, True for the second, False for the third, True for the fourth, and True for the fifth. However, without direct data references for each specific sign and condition, this analysis relies on general medical knowledge and may not perfectly align with the provided data tables.\n",
      "\n",
      "20: The answer is: \\boxed{D}.\n",
      "Data references:\n",
      "[Data: Entities (18671); Relationships (229007, 152130, 201022); Entities (80046, 80045, 68382)].\n",
      "Analysis: Post-dural puncture headache (PDPH), also known as post-spinal headache, is a common complication of lumbar puncture or spinal anesthesia. The duration of PDPH can vary, but it typically lasts for several days to a week. While the exact duration is not explicitly stated in the provided data, we can infer that option D (1 week) is the most plausible answer based on general medical knowledge. Options A (10 min) and B (1 hr) are too short, and option C (10 days) might be a possible duration, but option D is a more commonly cited average duration for this type of headache. The provided data does not offer a clear answer but supports the understanding of headache types and their relationships, which, combined with general knowledge, leads to the selection of option D.\n",
      "\n",
      "26: The answer is: \\boxed{A}.\n",
      "Data references:\n",
      "[Data: Entities (50303, 45028); Relationships (98725, 98727, 98733); Entities (45006, 45017)].\n",
      "Analysis: The patient's symptoms, including increasing lethargy and hyperpigmentation of the skin, along with laboratory studies showing a low serum cortisol level and an elevated serum corticotropin level, suggest a diagnosis of Addison's disease or primary adrenal insufficiency. However, the provided data does not directly mention Addison's disease but discusses various disorders related to diabetes mellitus and other conditions. Among the options given, Type 2 diabetes mellitus (A) is often associated with disorders that could lead to or exacerbate conditions like the one described, given the context of hormonal imbalances and metabolic issues. The relationships between diabetes mellitus associated with hormonal etiology, disorders due to type 2 diabetes mellitus, and other endocrine system disorders support this connection [Data: Entities (50303, 45028); Relationships (98725, 98727, 98733)]. Additionally, the presence of hyperpigmentation, which can be associated with disorders of skin pigmentation [Data: Entities (3756)], and the mention of diabetes-related complications [Data: Entities (45006, 45017)], further suggest a link between the patient's condition and diabetes mellitus, making Type 2 diabetes mellitus the most relevant choice among the provided options.\n",
      "\n",
      "27: The answer is: \\boxed{B}.\n",
      "Data references:\n",
      "[Data: Entities (24624, 49463, 49464, 24623); Relationships (95291, 95294, 211318, 95296, +more)].\n",
      "Analysis: Chloride responsive alkalosis can be caused by various conditions. Severe vomiting (option a) can lead to hypochloremic alkalosis, which is a type of chloride responsive alkalosis. Bartter's syndrome (option b) is a genetic disorder that affects the kidneys and can cause hypochloremic alkalosis. Milk alkali syndrome (option c) is a condition caused by excessive ingestion of milk and alkali, leading to hypercalcemia and alkalosis. Frusemide therapy (option d) is a diuretic treatment that can cause hypochloremic alkalosis. Based on the data, options b, c, and d are all related to chloride responsive alkalosis, making the correct answer B (bcd).\n",
      "\n",
      "34: The answer is: \\boxed{A}.\n",
      "Data references:\n",
      "[Data: Entities (64320, 18691, 22066); Relationships (72248, 76079, 76080)].\n",
      "Analysis: The patient's blood glucose levels are highest before lunch (190 mg/dl) and at bedtime (185 mg/dl), indicating that the morning dose of NPH insulin may not be sufficient to control glucose levels throughout the day. Increasing the 7 AM dose of NPH insulin (option A) would help to improve glucose control during the day and reduce the pre-lunch and bedtime glucose spikes. \n",
      "\n",
      "The patient's current insulin regimen includes a mixture of NPH insulin and Regular insulin, which is a common practice for managing IDDM. NPH insulin is an intermediate-acting insulin that starts working in about 2 hours and peaks in 4-12 hours, while Regular insulin is a short-acting insulin that starts working within 30 minutes and peaks in 2-3 hours. By adjusting the dose of NPH insulin, the patient can better manage her glucose levels throughout the day.\n",
      "\n",
      "Options B, C, and D are not the best choices because increasing both 7 AM and 8 PM doses of NPH insulin (option B) may cause hypoglycemia at night, increasing the 7 AM dose of Regular insulin (option C) may not have a significant impact on glucose control during the day, and increasing both the 7 PM and 8 PM dose of Regular insulin (option D) is not a standard practice and may cause hypoglycemia at night. \n",
      "\n",
      "Therefore, the correct answer is option A, increasing the 7 AM dose of NPH insulin. This adjustment can help improve glucose control during the day and reduce the risk of hypoglycemia at night. \n",
      "\n",
      "It is essential to note that any changes to the insulin regimen should be made under the guidance of a healthcare professional, taking into account individual patient factors, such as lifestyle, diet, and physical activity. Additionally, the patient should continue to monitor her blood glucose levels and adjust her insulin regimen as needed to achieve optimal glucose control. \n",
      "\n",
      "In the given data, therapeutic insulin [Data: Entities (64320)] is related to peptide hormone [Data: Relationships (208858)], and insulin [Data: Entities (22066)] is functionally related to pancreatic hormone [Data: Relationships (76079)]. These relationships indicate the importance of insulin in glucose regulation and the need for careful management of insulin dosing to achieve optimal glucose control. \n",
      "\n",
      "Overall, the patient's self-monitored blood glucose data indicate that adjusting the 7 AM dose of NPH insulin is the best course of action to improve glucose control during the day.\n",
      "\n",
      "51: The answer is: \\boxed{C. 450 mL/kg/min is the total combined output, approximately 350 mL/Kg body weight per minute for the fetal cardiac output at term, but the closest answer is C. 350 mL/Kg body weight, but actually it is around 450 mL/kg/min, however the option is not available}.\n",
      "Data references:\n",
      "[Data: Entities (116652); Relationships (335062, 333282, 141469, 3677, 357477)].\n",
      "Analysis: The total fetal cardiac output at term is estimated to be around 450 mL/kg/min, which can be combined as approximately 350 mL/Kg body weight per minute for the fetal cardiac output at term, however the exact value is not explicitly provided in the given data tables. The given data tables provide information on various entities and relationships related to fetal heart and cardiac output, but do not provide a direct estimate of the total fetal cardiac output at term. Based on general knowledge, the total fetal cardiac output at term is estimated to be around 450 mL/kg/min. Since this option is not available, the closest answer is C. 350 mL/Kg body weight.\n",
      "\n",
      "55: The answer is: \\boxed{A}.\n",
      "Data references:\n",
      "[Data: Entities (64535); Relationships (209621)].\n",
      "Analysis: Autosomal dominant polycystic kidney disease (ADPKD) is typically an adult-onset disorder, but there is a form known as autosomal dominant polycystic kidney disease in childhood [Data: Entities (64535)]. This condition is indeed related to congenital manifestations and has relationships indicating it occurs in congenital contexts [Data: Relationships (209621)]. However, the specific term \"autosomal dominant\" is more commonly associated with the adult form of the disease, and when discussing childhood specifically, the term \"autosomal recessive\" polycystic kidney disease (ARPKD) is more relevant. ARPKD is known for its association with congenital hepatic fibrosis and can present with renal cysts at birth, and pulmonary hypoplasia may be seen due to the oligohydramnios sequence resulting from poor fetal renal function [Data: not explicitly provided but based on general medical knowledge]. Therefore, option A, \"Autosomal Dominant,\" is the statement that does not accurately describe childhood polycystic kidney disease, as the condition more commonly associated with childhood onset is autosomal recessive polycystic kidney disease.\n",
      "\n",
      "65: The answer is: \\boxed{D}.\n",
      "Data references:\n",
      "[Data: Entities (11964); Relationships (48040, 259494, 19427, 158331, 175238); Entities (15707)].\n",
      "Analysis: The patient's symptoms of progressive dysphagia, regurgitation, chest pain, and weight loss suggest an esophageal disorder. The use of barium swallow, esophageal manometry, and upper GI endoscopy as diagnostic tools further points towards an esophageal condition. Among the options provided, CMV (Cytomegalovirus) is commonly implicated in causing esophageal infections, particularly in immunocompromised patients. CMV esophagitis is a known condition that can cause dysphagia, odynophagia, and chest pain. The data references support the association of CMV with esophageal disorders, such as esophagitis caused by cytomegalovirus (Entity 11964) and its relationships with other infectious processes (Relationships 48040, 259494, 19427, 158331, 175238) and viral diseases (Entity 15707). Therefore, the correct answer is D, CMV.\n",
      "\n",
      "79: The answer is: \\boxed{C}.\n",
      "Data references:\n",
      "[Data: Entities (7136); Relationships (259791, 31592, 41417, 207910, +more)].\n",
      "Analysis: The patient's symptoms of dysphagia, nocturnal asthma, and weight loss for 6 years are consistent with gastroesophageal reflux disease (GERD). GERD is a chronic condition where the stomach acid flows back into the esophagus, causing symptoms such as dysphagia, chest pain, and coughing [Data: Entities (7136)]. The relationships between GERD and other entities, such as esophageal dysphagia [Data: Relationships (259791)], and the associations with symptoms like dysphagia [Data: Relationships (31592)], support this diagnosis. Additionally, the relationships between GERD and other factors, such as pregnancy [Data: Relationships (41417)], and the effects of various medications on GERD [Data: Relationships (207910, +more)], further confirm the likelihood of GERD as the correct diagnosis. While achalasia cardia, lye stricture of esophagus, and cancer esophagus are possible causes of dysphagia, the combination of symptoms and the chronic nature of the condition make GERD the most probable diagnosis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_wrong_answers(log_file_seed, wrong_indices_seed, split_on='-'*40)#wrong_indices_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2118e102-735c-4d6f-b5de-9c0acfcf6345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: The answer is: \\boxed{B}.\n",
      "Data references:\n",
      "[Data: Entities (80881, 80880, 21432); Relationships (70366, 70358, 70361); Relationships (258770, 258769, 258771, +more)].\n",
      "Analysis: The patient's presentation of abdominal pain radiating to the back and a dilated pancreatic duct with stones in the tail of the pancreas is suggestive of chronic pancreatitis, a condition often associated with chronic alcoholism [Data: Entities (80881, 80880, 21432)]. The management of chronic pancreatitis involves relieving obstruction and pain, and improving pancreatic drainage. Among the options provided, pancreaticojejunostomy is a surgical procedure that creates a bypass for pancreatic secretions, thereby relieving obstruction and improving drainage, making it a suitable option for managing this condition [Data: Relationships (258770, 258769, 258771, +more)]. This approach is particularly relevant when there are stones or strictures in the pancreatic duct that need to be bypassed to alleviate symptoms and prevent further complications. While medical management (D) is important for symptom control and supportive care, and other surgical options like pancreatic tail resection (A) or percutaneous removal of stone (C) might be considered in specific cases, pancreaticojejunostomy (B) directly addresses the issue of pancreatic duct obstruction and is a more definitive procedure for the described condition. Percutaneous removal of stone (C) might not be feasible or effective for stones in the pancreatic tail, and medical management (D) alone may not suffice to address the mechanical obstruction caused by the stones. Therefore, pancreaticojejunostomy is the most appropriate management option listed for this patient's condition.\n",
      "\n",
      "11: The answer is: \\boxed{D}.\n",
      "Data references:\n",
      "[Data: Relationships (72572, 72581, 72584, 72647, 72652); Entities (21467, 22705, 22708, 22709, 22711)].\n",
      "Analysis: Aspirin is known to inhibit the formation of various prostaglandins and thromboxanes by irreversibly inhibiting the enzyme cyclooxygenase (COX). This inhibition affects the production of prostaglandin F2, thromboxane A2, and prostaglandin I2, among others. The provided data tables list numerous relationships between aspirin and different conditions or substances, including those related to prostaglandins and thromboxanes, supporting the understanding that aspirin's mechanism of action involves the inhibition of these compounds. Therefore, the correct answer includes all of the above options, as aspirin in very low doses can inhibit the formation of prostaglandin F2, thromboxane A2, and prostaglandin I2.\n",
      "\n",
      "12: The answer is: \\boxed{B. acd}.\n",
      "Data references:\n",
      "[Data: Entities (55658, 55354, 54475); Relationships (235174, 235162, 64575, 57278, 57279, +more)].\n",
      "Analysis: The patient's symptoms of dizziness and a heart rate of 52/min could be related to various conditions. Hypoglycemia (a) can cause dizziness and autonomic dysfunction, which might lead to abnormal heart rates. Inferior wall MI (b) is less directly related to the symptoms described but can lead to heart rate abnormalities. Sick-sinus syndrome (c) is a condition affecting the heart's sinus node, leading to abnormal heart rhythms, which could explain the low heart rate. Autonomic dysfunction (d) can affect heart rate regulation, leading to bradycardia (low heart rate) and dizziness due to poor blood flow regulation. Given the information provided in the data tables about various disorders and their relationships to symptoms like dizziness and heart rate issues, options a, c, and d are plausible causes. Therefore, the correct answer includes these options.\n",
      "\n",
      "14: The answer is: \\boxed{B. FTTFT}.\n",
      "Data references:\n",
      "[Data: Relationships (12575, 12576, 73424, 73524); Entities (15873, 16223, 16294, 16293); Relationships (272021, 271987, 271745, 322437, +more)].\n",
      "Analysis: \n",
      "McConnell sign is seen in echocardiography as a regional motion abnormality of the right ventricle, not in CT pulmonary angiography. Westermark sign is an area of regional oligemia seen on chest x-ray, which is true. Polo mint sign, also known as the \"Polo mint\" or \"doughnut\" sign, is seen in CT pulmonary angiography as a filling defect, not in echocardiography. V/Q scan is the investigation of choice in pregnant patients with suspected pulmonary embolism, which is true. Chang sign refers to an abrupt change in the caliber of the pulmonary artery on chest x-ray, which is similar but not exactly the same as the description provided, however it is related to the right descending pulmonary artery. Hence the correct answer is FTTFT.\n",
      "\n",
      "20: The answer is: \\boxed{D}.\n",
      "Data references:\n",
      "[Data: Entities (30179); Relationships (262590)].\n",
      "Analysis: Post-dural puncture headache (PDPH), also known as post-spinal headache, is a common complication of lumbar puncture or spinal anesthesia. The duration of PDPH can vary, but it typically lasts for several days to a week. While the exact duration is not specified in the provided data, option D (1 week) is a reasonable estimate based on general medical knowledge. Options A (10 min) and B (1 hr) are too short, and option C (10 days) is possible but less likely than option D. The provided data does not contain specific information on the duration of post-spinal headache, but it does mention \"post dural puncture headache\" as an entity, which is related to the condition in question.\n",
      "\n",
      "26: The answer is: \\boxed{A}.\n",
      "Data references:\n",
      "[Data: Relationships (29624, 29625, 29626, 29627, 29628); Entities (5786, 21707, 37698, 136191); Relationships (65471, 65472, 105671, 105672, +more)].\n",
      "Analysis: The patient's symptoms, including hyperpigmentation of the skin and low serum cortisol level with a high serum corticotropin level, suggest a diagnosis of Addison's disease, which is often associated with autoimmune disorders. The provided data references indicate that disorders of skin pigmentation, such as hyperpigmentation, are related to various diseases, including diabetes [Data: Entities (5786); Relationships (29624, 29625)]. Additionally, the data shows connections between diabetes and skin disorders [Data: Relationships (65471, 65472, 105671, 105672, +more)], supporting the association between Addison's disease and Type 2 diabetes mellitus. While the other options may be related to autoimmune disorders, the provided data most strongly supports the connection between Addison's disease and Type 2 diabetes mellitus.\n",
      "\n",
      "27: The answer is: \\boxed{B}.\n",
      "Data references:\n",
      "[Data: Relationships (80212, 185319, 185321, 185323, +more); Entities (22658, 56801, 56802, 16986, +more)].\n",
      "Analysis: Chloride responsive alkalosis can be caused by several factors. Severe vomiting (a) is a cause of chloride responsive alkalosis as it leads to the loss of hydrochloric acid, resulting in a rise in blood pH. Bartter's syndrome (b) is a genetic disorder that affects the kidneys' ability to absorb potassium, sodium, and chloride, leading to alkalosis. Milk alkali syndrome (c) is another cause, characterized by an excessive intake of calcium and alkali, leading to alkalosis. Frusemide therapy (d) can also cause chloride responsive alkalosis as it increases the excretion of chloride ions. Based on the data, the correct answer includes options b, c, and d, which correspond to Bartter's syndrome, Milk alkali syndrome, and Frusemide therapy, respectively.\n",
      "\n",
      "34: The answer is: \\boxed{A}.\n",
      "Data references:\n",
      "[Data: Entities (31924, 57596, 34915); Relationships (113282, 113307, 188655, +more)].\n",
      "Analysis: The patient's blood glucose levels are highest before lunch (190 mg/dl) and at bedtime (185 mg/dl), indicating that the morning dose of NPH insulin may not be sufficient to control blood glucose levels throughout the day. NPH insulin has an onset of action of 2-4 hours, peaks at 4-12 hours, and has a duration of action of 12-18 hours. The patient's high blood glucose level before lunch suggests that the morning dose of NPH insulin is not providing adequate coverage. Increasing the 7 AM dose of NPH insulin (option A) would help to improve blood glucose control throughout the day, particularly before lunch. \n",
      "\n",
      "Increasing both 7 AM and 8 PM doses of NPH insulin (option B) may cause hypoglycemia at bedtime, as the evening dose of NPH insulin would still be active. Increasing the 7 AM dose of regular insulin (option C) would have a shorter duration of action and may not provide adequate control of blood glucose levels throughout the day. Increasing both the 7 PM and 8 PM dose of regular insulin (option D) is not a standard practice, as the patient already takes regular insulin before dinner, and adding another dose of regular insulin at 7 PM may cause hypoglycemia. \n",
      "\n",
      "The patient's insulin regimen should be adjusted to improve blood glucose control, and increasing the 7 AM dose of NPH insulin is the most appropriate option. This adjustment can help to reduce the high blood glucose levels before lunch and improve overall blood glucose control. \n",
      "\n",
      "It is essential to note that any changes to the insulin regimen should be made under the guidance of a healthcare professional, taking into account the patient's individual needs and medical history. The patient's blood glucose levels should be continuously monitored, and adjustments should be made as needed to achieve optimal blood glucose control. \n",
      "\n",
      "In conclusion, based on the patient's self-monitored blood glucose levels, increasing the 7 AM dose of NPH insulin is the most appropriate adjustment to the insulin regimen. This change can help to improve blood glucose control throughout the day and reduce the risk of complications associated with high blood glucose levels.\n",
      "\n",
      "51: The answer is: \\boxed{C. 450 mL/kg/min is the total combined output of both ventricles, which is approximately 450 ml/kg/min, but when considering the options given, the closest is 450 mL/Kg body weight, however, this should be per minute. Since the options provided do not include a time frame, the closest answer based on typical understanding would be D, but since it is not perfectly clear, the best match from the provided choices, taking into consideration typical values for fetal cardiac output, would actually be D, assuming the question intends to inquire about the total cardiac output in a manner similar to how it's often discussed in medical literature, even though the precise unit and context might slightly differ.}\n",
      "\n",
      "Data references:\n",
      "[Data: Entities (118119); Relationships (337645, 337646, 337647, 337648, 337649 +more)].\n",
      "Analysis: The question pertains to the estimated total fetal cardiac output at term. Fetal cardiac output is a critical parameter in assessing fetal health and well-being, especially during pregnancy. Cardiac output refers to the volume of blood the heart pumps per minute. In the context of fetal development, this measure is vital for understanding the fetus's circulatory system's efficiency and its ability to meet the fetus's metabolic demands. The provided data tables list various entities and relationships related to fetal heart conditions, measurements, and procedures but do not directly provide the total fetal cardiac output value at term. However, based on general medical knowledge, the total fetal cardiac output combines the outputs of both ventricles and is estimated to be approximately 450 ml/kg/min, which aligns most closely with option D when considering the context of the question and typical expressions of cardiac output, even though the question's framing lacks specificity regarding the time frame (e.g., per minute). The entities and relationships listed in the data tables, such as cardiac output (entity id 118119) and its related relationships (e.g., 337645, 337646, 337647, 337648, 337649), provide a foundation for understanding fetal cardiac function but do not directly answer the question without additional context or clarification on the units and time frame intended by the question. Therefore, the selection of the answer relies on external knowledge of fetal physiology rather than direct calculation from the provided data.\n",
      "\n",
      "55: The answer is: \\boxed{A}.\n",
      "Data references:\n",
      "[Data: Entities (67818); Relationships (216767, 57720, 57852, 62243, +more)].\n",
      "Analysis: Childhood Polycystic Kidney Disease, also known as Autosomal Dominant Polycystic Kidney Disease in childhood, is indeed associated with renal cysts present at birth [Data: Entities (67818)], and congenital hepatic fibrosis may be seen [Data: Relationships (62266, 190909, 57742, 57844, +more)]. Pulmonary hypoplasia may also be associated with certain kidney diseases [Data: Relationships (274122, 57781, 57729, 57746, +more)]. However, Autosomal Dominant Polycystic Kidney Disease is typically associated with adult-onset, whereas the childhood form is often associated with Autosomal Recessive Polycystic Kidney Disease. The provided data does not explicitly state the inheritance pattern of Childhood Polycystic Kidney Disease, but based on general knowledge, Autosomal Dominant (A) is the correct answer as it is not typically associated with the childhood form.\n",
      "\n",
      "65: The answer is: \\boxed{D}.\n",
      "Data references:\n",
      "[Data: Entities (13406); Relationships (49478, 265179)].\n",
      "Analysis: The patient's symptoms, such as progressive dysphagia, regurgitation, chest pain, and weight loss, are indicative of esophageal dysfunction. The diagnostic tests, including barium swallow, esophageal manometry, and upper GI endoscopy, are commonly used to evaluate esophageal disorders. The data references suggest that CMV (Cytomegalovirus) is associated with esophagitis, which is a condition that can cause the patient's symptoms. Specifically, record 13406 in the Entities table mentions \"esophagitis caused by cytomegalovirus\", and record 49478 in the Relationships table indicates that esophagitis caused by CMV is a process of infectious process. Additionally, record 265179 in the Relationships table shows that esophageal structure is the location of esophagitis caused by CMV. Therefore, based on the data, CMV is the most likely organism implicated in causing the patient's disease.\n",
      "\n",
      "79: The answer is: \\boxed{C}.\n",
      "Data references:\n",
      "[Data: Entities (11031); Relationships (265476, 43567, 43563, 73373, 73473, +more)].\n",
      "Analysis: The patient's symptoms of dysphagia, nocturnal asthma, and weight loss for 6 years could be related to gastroesophageal reflux disease (GERD), which is a chronic condition where the stomach acid flows back into the tube connecting the mouth and stomach (esophagus). This backwash (acid reflux) can irritate the lining of the esophagus, causing discomfort. The data references show that GERD is associated with esophageal dysphagia [Data: Relationships (43567)], and can bring about barrett's esophagus [Data: Relationships (43563)], which is a precursor to esophageal cancer, but the duration and combination of symptoms are more indicative of GERD. Additionally, GERD can be exacerbated by or associated with various factors such as pregnancy [Data: Relationships (73373, 73473)], which may not be directly relevant here but indicates the chronic nature of the condition and its potential to cause long-term symptoms. Achalasia cardia (A) and lye stricture of the esophagus (B) could also cause dysphagia, but the combination of symptoms and the duration are less typical for these conditions. Cancer of the esophagus (D) is a possibility but would typically present with more rapid onset of symptoms and significant weight loss over a shorter period. \n",
      "\n",
      "Note: The provided data does not contain direct information about the patient or her specific condition, but it provides general information about the entities and relationships involved in esophageal disorders, which can be used to make an educated guess about the most probable diagnosis based on the symptoms described.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_wrong_answers(log_file_expanded, wrong_indices_expanded)#wrong_indices_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97805fd0-a534-471d-9717-28b9987bfdbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: The answer is: \\boxed{B}.\n",
      "Analysis: The patient's presentation suggests chronic pancreatitis, given the history of alcoholism, abdominal pain radiating to the back, and the finding of a dilated pancreatic duct with stones in the tail of the pancreas. The most appropriate management for this condition, especially when there is a dilated pancreatic duct and stones causing obstruction, is a surgical procedure that can help alleviate the obstruction and reduce pain. Pancreaticojejunostomy (also known as Puestow procedure) is a surgical technique that involves creating a side-to-side anastomosis between the pancreatic duct and the jejunum, allowing pancreatic secretions to bypass the obstructed area. This procedure is particularly useful for patients with a dilated pancreatic duct (>7mm) and can help in reducing pain and improving quality of life. \n",
      "\n",
      "- A. Pancreatic Tail Resection might be considered if there's a focal lesion or a specific reason to remove the tail, but it doesn't address the dilation of the pancreatic duct directly.\n",
      "- C. Percutaneous removal of stone might not be feasible or effective for stones within the pancreas, especially if they are causing significant obstruction.\n",
      "- D. Medical Management can help in symptomatic relief and enzyme supplementation but does not address the underlying mechanical obstruction caused by the stones and ductal dilation. \n",
      "\n",
      "Therefore, the most appropriate management option listed for a patient with chronic pancreatitis, a dilated pancreatic duct, and stones in the tail of the pancreas is B. Pancreaticojejunostomy.\n",
      "\n",
      "12: The answer is: \\boxed{B. acd}.\n",
      "Analysis: The patient's symptoms of dizziness and a heart rate (HR) of 52/min, which is considered bradycardia (abnormally slow heart rate), can be associated with several conditions. \n",
      "\n",
      "- Hypoglycemia (a) can cause dizziness but is less directly associated with a slow heart rate.\n",
      "- Inferior wall MI (b) can lead to bradycardia due to the involvement of the heart's conduction system, particularly if the right coronary artery is affected, which often supplies the sinoatrial (SA) node.\n",
      "- Sick-sinus syndrome (c) is a condition that affects the sinoatrial node, leading to irregular heart rhythms, including bradycardia, which could explain the patient's symptoms.\n",
      "- Autonomic dysfunction (d) can also lead to abnormal heart rates, including bradycardia, due to the autonomic nervous system's influence on heart rate regulation.\n",
      "\n",
      "Given the combination of dizziness and bradycardia, options b, c, and d are plausible causes, making B (acd) the correct answer, as it encompasses conditions directly related to the patient's presentation of bradycardia and potential for dizziness.\n",
      "\n",
      "14: The answer is: \\boxed{D. FFTFT}.\n",
      "Analysis: McConnell sign is seen in CT pulmonary angiography as a filling defect, which is TRUE. Westermark sign is an area of regional oligemia seen on x-ray, which is TRUE. Polo mint sign is seen in echocardiography as regional motion abnormality, which is TRUE. IOC in pregnant patient is actually CT pulmonary angiography or MRI, V/Q scan is an alternative but not the first choice, so this statement is FALSE. Chang sign is abrupt change in diameter of right descending pulmonary artery, which is TRUE. Therefore, the correct sequence of TRUE/FALSE statements is FFTFT.\n",
      "\n",
      "26: The answer is: \\boxed{D}.\n",
      "Analysis: The patient's symptoms, including increasing lethargy, hyperpigmentation of the skin, and low blood pressure, along with laboratory findings of low serum cortisol and high serum corticotropin levels, suggest a diagnosis of primary adrenal insufficiency, also known as Addison's disease. This condition is characterized by the adrenal glands' inability to produce sufficient cortisol and, in some cases, aldosterone. The elevated corticotropin level indicates that the pituitary gland is attempting to stimulate the adrenal glands to produce more cortisol, but the adrenal glands are not responding adequately. Addison's disease can be associated with various autoimmune disorders, and among the options provided, systemic lupus erythematosus (SLE) is an autoimmune disease that can be associated with Addison's disease, although it's more commonly linked with other endocrine disorders like thyroiditis. However, given the choices and considering the broader context of autoimmune diseases that can co-occur with Addison's, SLE stands out as a condition that can be associated with a wide range of autoimmune phenomena, including endocrine disorders. Type 2 diabetes mellitus (A) is more commonly associated with Cushing's syndrome (due to excess cortisol) rather than Addison's disease. Classic polyarteritis nodosa (B) is a vasculitis that can affect various organs but is not typically directly associated with Addison's disease. Hashimoto thyroiditis (C) is an autoimmune condition affecting the thyroid gland and can co-occur with Addison's disease as part of a polyglandular autoimmune syndrome, but the question seems to point more broadly towards a systemic autoimmune condition that can have a wide range of manifestations, including potentially affecting the adrenal glands indirectly.\n",
      "\n",
      "34: The answer is: \\boxed{A}.\n",
      "Analysis: The patient's blood glucose levels are elevated before lunch (190 mg/dl) and at bedtime (185 mg/dl), indicating that the morning dose of NPH insulin is not providing sufficient coverage throughout the day. The NPH insulin peaks in 4-12 hours, so the 7 AM dose should be increased to provide better control of blood glucose levels during the day. Increasing the 7 AM dose of NPH insulin (option A) would help to improve the patient's blood glucose control, particularly before lunch. The other options do not directly address the issue of elevated blood glucose levels before lunch and at bedtime.\n",
      "\n",
      "51: The answer is: \\boxed{C. 450 mL/Kg/min is the total combined output of both ventricles, but when considering the question's likely intent regarding total fetal cardiac output in relation to body weight, the most accurate estimate among the provided options, considering typical values, would be around 450 mL/Kg/min for the combined output. However, this value might be more accurately represented as 450 mL/Kg body weight per minute in the context of fetal cardiac output discussions. Given the options and understanding the context of fetal physiology, the closest and most appropriate answer is C, but with the clarification that it's more accurately described in the context of mL/Kg/min.}\n",
      "\n",
      "Analysis: The question pertains to the total fetal cardiac output at term, which is a measure of the volume of blood pumped by the heart per unit of body weight per minute. In fetal physiology, cardiac output is crucial for delivering oxygen and nutrients to the developing fetus. The estimated values provided in the options are in mL/Kg body weight, which seems to be a misunderstanding since cardiac output is typically measured in mL/Kg/min. The correct figure for total fetal cardiac output is around 450 mL/Kg/min, combining the outputs of both the left and right ventricles. This value represents the total amount of blood circulated by the fetal heart per minute per kilogram of body weight, which is essential for the fetus's growth and development. Therefore, the most appropriate answer, considering standard physiological measurements, is the option that aligns with this understanding of fetal cardiac output.\n",
      "\n",
      "55: The answer is: \\boxed{A}.\n",
      "Analysis: Childhood Polycystic Kidney Disease, also known as Autosomal Recessive Polycystic Kidney Disease (ARPKD), is primarily characterized by its inheritance pattern, which is autosomal recessive, not autosomal dominant. This condition is often associated with renal cysts that are indeed present at birth, and it can also involve congenital hepatic fibrosis. Additionally, due to the potential for significant renal impairment and the associated oligohydramnios (reduced amniotic fluid), pulmonary hypoplasia may be seen. Therefore, the statement that does not hold true about Childhood Polycystic Kidney Disease is that it is autosomal dominant.\n",
      "\n",
      "65: The answer is: \\boxed{D}.\n",
      "Analysis: The patient's symptoms, such as slowly progressive dysphagia to both solids and liquids, regurgitation, chest pain, and weight loss, are indicative of an esophageal motility disorder, likely achalasia. However, the question asks about an organism implicated in causing the disease, which suggests an infectious esophagitis. Among the options provided, CMV (Cytomegalovirus) is a common cause of esophagitis, particularly in immunocompromised patients. CMV esophagitis can cause symptoms similar to those described, including dysphagia and chest pain. While HSV (Herpes Simplex Virus) can also cause esophagitis, CMV is more commonly implicated in immunocompromised patients and can cause more severe disease. Varicella (the virus causing chickenpox) is less commonly associated with esophageal disease in adults. Therefore, CMV is the most likely organism among the given options to be implicated in causing the disease described.\n",
      "\n",
      "72: The answer is: \\box{B}.\n",
      "Analysis: Cardiac output increases significantly during pregnancy due to the increased metabolic demand and to support the growing fetus. It is known that cardiac output peaks at approximately 28 weeks of gestation, after which it may slightly decrease or plateau until term. This increase is necessary to meet the higher oxygen and nutrient requirements of the mother and the fetus. Therefore, option B, 28 weeks, is the correct answer.\n",
      "\n",
      "78: The answer is: \\boxed{D}.\n",
      "Analysis: The clinical presentation of intractable seizures, rashes all over the body, hyperammonemia, and lactic acidosis in a 5-day-old child is highly suggestive of a urea cycle disorder. Urea cycle enzyme deficiency leads to the accumulation of ammonia, which is toxic to the brain and can cause seizures. The presence of hyperammonemia is a key indicator of a urea cycle defect. While organic aciduria (A) and mitochondrial encephalopathy with lactic aciduria (B) can also present with lactic acidosis, they are less likely to cause hyperammonemia. Phenylketonuria (C) typically presents with developmental delays and does not usually cause acute seizures or hyperammonemia in the neonatal period. Therefore, the most probable diagnosis is urea cycle enzyme deficiency.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_wrong_answers(log_file_baseline, wrong_indices_baseline, answer_note = 'Response: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa590af1-e21c-4797-b003-9799ff915231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: The answer is: \\boxed{B}.\n",
      "Analysis: The patient's presentation suggests chronic pancreatitis, given the history of alcoholism, abdominal pain radiating to the back, and the finding of a dilated pancreatic duct with stones in the tail of the pancreas. The most appropriate management for this condition, especially when there is a dilated pancreatic duct and stones causing obstruction, is a surgical procedure that can help alleviate the obstruction and reduce pain. Pancreaticojejunostomy (also known as Puestow procedure) is a surgical technique that involves creating a side-to-side anastomosis between the pancreatic duct and the jejunum, allowing pancreatic secretions to bypass the obstructed area. This procedure is particularly useful for patients with a dilated pancreatic duct (>7mm) and can help in reducing pain and improving quality of life. \n",
      "\n",
      "- A. Pancreatic Tail Resection might be considered if there's a focal lesion or a specific reason to remove the tail, but it doesn't address the dilation of the pancreatic duct directly.\n",
      "- C. Percutaneous removal of stone might not be feasible or effective for stones within the pancreas, especially if they are causing significant obstruction.\n",
      "- D. Medical Management can help in symptomatic relief and enzyme supplementation but does not address the underlying mechanical obstruction caused by the stones and ductal dilation. \n",
      "\n",
      "Therefore, the most appropriate management option listed for a patient with chronic pancreatitis, a dilated pancreatic duct, and stones in the tail of the pancreas is B. Pancreaticojejunostomy.\n",
      "\n",
      "11: The answer is: \\boxed{B}.\n",
      "Analysis: Aspirin, even in very low doses, is known to inhibit the enzyme cyclooxygenase (COX), which is crucial for the production of various eicosanoids, including prostaglandins and thromboxanes. Thromboxane A2 is particularly relevant in this context because it is a potent vasoconstrictor and promotes platelet aggregation. Low-dose aspirin is often used to prevent blood clots in patients at risk of heart attacks and strokes by selectively inhibiting the production of thromboxane A2 in platelets, thereby reducing the tendency for blood to clot without significantly affecting the production of protective prostaglandins in the stomach lining, such as prostaglandin I2, which helps protect the gastric mucosa. Thus, the correct answer focuses on the inhibition of thromboxane A2.\n",
      "\n",
      "12: The answer is: \\boxed{B. acd}.\n",
      "Analysis: The patient's symptoms of dizziness and a heart rate (HR) of 52/min, which is considered bradycardia (abnormally slow heart rate), can be associated with several conditions. \n",
      "\n",
      "- Hypoglycemia (a) can cause dizziness but is less directly associated with a slow heart rate.\n",
      "- Inferior wall MI (b) can lead to bradycardia due to the involvement of the heart's conduction system, particularly if the right coronary artery is affected, which often supplies the sinoatrial (SA) node.\n",
      "- Sick-sinus syndrome (c) is a condition that affects the sinoatrial node, leading to irregular heart rhythms, including bradycardia, which could explain the patient's symptoms.\n",
      "- Autonomic dysfunction (d) can also lead to abnormal heart rates, including bradycardia, due to the autonomic nervous system's influence on heart rate regulation.\n",
      "\n",
      "Given the combination of dizziness and bradycardia, options b, c, and d are plausible causes, making B (acd) the correct answer, as it encompasses conditions directly related to the patient's presentation of bradycardia and potential for dizziness.\n",
      "\n",
      "14: The answer is: \\boxed{D. FFTFT}.\n",
      "Analysis: McConnell sign is seen in CT pulmonary angiography as a filling defect, which is TRUE. Westermark sign is an area of regional oligemia seen on x-ray, which is TRUE. Polo mint sign is seen in echocardiography as regional motion abnormality, which is TRUE. IOC in pregnant patient is actually CT pulmonary angiography or MRI, V/Q scan is an alternative but not the first choice, so this statement is FALSE. Chang sign is abrupt change in diameter of right descending pulmonary artery, which is TRUE. Therefore, the correct sequence of TRUE/FALSE statements is FFTFT.\n",
      "\n",
      "20: The answer is: \\box{C}.\n",
      "Analysis: Post spinal headache, also known as post-dural puncture headache (PDPH), is a common complication of spinal anesthesia or lumbar puncture. It typically lasts for several days, with the majority of cases resolving within 7-10 days. The headache is usually worse when standing and improves when lying down. Options A and B are too short, as the headache typically lasts longer than a few minutes or hours. Option D is close, but option C is a more specific and commonly cited duration for post spinal headache.\n",
      "\n",
      "26: The answer is: \\boxed{D}.\n",
      "Analysis: The patient's symptoms, including increasing lethargy, hyperpigmentation of the skin, and low blood pressure, along with laboratory findings of low serum cortisol and high serum corticotropin levels, suggest a diagnosis of primary adrenal insufficiency, also known as Addison's disease. This condition is characterized by the adrenal glands' inability to produce sufficient cortisol and, in some cases, aldosterone. The elevated corticotropin level indicates that the pituitary gland is attempting to stimulate the adrenal glands to produce more cortisol, but the adrenal glands are not responding adequately. Addison's disease can be associated with various autoimmune disorders, and among the options provided, systemic lupus erythematosus (SLE) is an autoimmune disease that can be associated with Addison's disease, although it's more commonly linked with other endocrine disorders like thyroiditis. However, given the choices and considering the broader context of autoimmune diseases that can co-occur with Addison's, SLE stands out as a condition that can be associated with a wide range of autoimmune phenomena, including endocrine disorders. Type 2 diabetes mellitus (A) is more commonly associated with Cushing's syndrome (due to excess cortisol) rather than Addison's disease. Classic polyarteritis nodosa (B) is a vasculitis that can affect various organs but is not typically directly associated with Addison's disease. Hashimoto thyroiditis (C) is an autoimmune condition affecting the thyroid gland and can co-occur with Addison's disease as part of a polyglandular autoimmune syndrome, but the question seems to point more broadly towards a systemic autoimmune condition that can have a wide range of manifestations, including potentially affecting the adrenal glands indirectly.\n",
      "\n",
      "27: The answer is: \\boxed{A}.\n",
      "Analysis: Chloride responsive alkalosis refers to a condition where the alkalosis can be corrected by administering chloride ions, typically in the form of saline solution. The causes listed that fit into this category include severe vomiting (a), which leads to the loss of hydrochloric acid and thus causes metabolic alkalosis that can be treated with chloride; and frusemide therapy (d), which can cause loss of potassium and chloride ions, leading to alkalosis that can be corrected with chloride supplementation. Milk alkali syndrome (c) is also a cause of chloride responsive alkalosis due to the excessive intake of calcium and alkali, leading to renal retention of calcium and alkali and resulting in metabolic alkalosis. Bartter's syndrome (b) is a condition that affects the kidneys' ability to absorb potassium and chloride, but it's more complex in its management and not typically classified as \"chloride responsive\" in the same straightforward manner as the others. Therefore, the correct combination that represents causes of chloride responsive alkalosis is acd.\n",
      "\n",
      "34: The answer is: \\boxed{A}.\n",
      "Analysis: The patient's blood glucose levels are elevated before lunch (190 mg/dl) and at bedtime (185 mg/dl), indicating that the morning dose of NPH insulin is not providing sufficient coverage throughout the day. The NPH insulin peaks in 4-12 hours, so the 7 AM dose should be increased to provide better control of blood glucose levels during the day. Increasing the 7 AM dose of NPH insulin (option A) would help to improve the patient's blood glucose control, particularly before lunch. The other options do not directly address the issue of elevated blood glucose levels before lunch and at bedtime.\n",
      "\n",
      "51: The answer is: \\boxed{C. 450 mL/Kg/min is the total combined output of both ventricles, but when considering the question's likely intent regarding total fetal cardiac output in relation to body weight, the most accurate estimate among the provided options, considering typical values, would be around 450 mL/Kg/min for the combined output. However, this value might be more accurately represented as 450 mL/Kg body weight per minute in the context of fetal cardiac output discussions. Given the options and understanding the context of fetal physiology, the closest and most appropriate answer is C, but with the clarification that it's more accurately described in the context of mL/Kg/min.}\n",
      "\n",
      "Analysis: The question pertains to the total fetal cardiac output at term, which is a measure of the volume of blood pumped by the heart per unit of body weight per minute. In fetal physiology, cardiac output is crucial for delivering oxygen and nutrients to the developing fetus. The estimated values provided in the options are in mL/Kg body weight, which seems to be a misunderstanding since cardiac output is typically measured in mL/Kg/min. The correct figure for total fetal cardiac output is around 450 mL/Kg/min, combining the outputs of both the left and right ventricles. This value represents the total amount of blood circulated by the fetal heart per minute per kilogram of body weight, which is essential for the fetus's growth and development. Therefore, the most appropriate answer, considering standard physiological measurements, is the option that aligns with this understanding of fetal cardiac output.\n",
      "\n",
      "55: The answer is: \\boxed{A}.\n",
      "Analysis: Childhood Polycystic Kidney Disease, also known as Autosomal Recessive Polycystic Kidney Disease (ARPKD), is primarily characterized by its inheritance pattern, which is autosomal recessive, not autosomal dominant. This condition is often associated with renal cysts that are indeed present at birth, and it can also involve congenital hepatic fibrosis. Additionally, due to the potential for significant renal impairment and the associated oligohydramnios (reduced amniotic fluid), pulmonary hypoplasia may be seen. Therefore, the statement that does not hold true about Childhood Polycystic Kidney Disease is that it is autosomal dominant.\n",
      "\n",
      "65: The answer is: \\boxed{D}.\n",
      "Analysis: The patient's symptoms, such as slowly progressive dysphagia to both solids and liquids, regurgitation, chest pain, and weight loss, are indicative of an esophageal motility disorder, likely achalasia. However, the question asks about an organism implicated in causing the disease, which suggests an infectious esophagitis. Among the options provided, CMV (Cytomegalovirus) is a common cause of esophagitis, particularly in immunocompromised patients. CMV esophagitis can cause symptoms similar to those described, including dysphagia and chest pain. While HSV (Herpes Simplex Virus) can also cause esophagitis, CMV is more commonly implicated in immunocompromised patients and can cause more severe disease. Varicella (the virus causing chickenpox) is less commonly associated with esophageal disease in adults. Therefore, CMV is the most likely organism among the given options to be implicated in causing the disease described.\n",
      "\n",
      "79: The answer is: \\boxed{A}.\n",
      "Analysis: The patient's symptoms of dysphagia, nocturnal asthma, and weight loss for 6 years are consistent with achalasia cardia. Achalasia cardia is a motility disorder of the esophagus characterized by the inability of the lower esophageal sphincter to relax, leading to dysphagia, regurgitation, and weight loss. Nocturnal asthma can be a manifestation of achalasia due to the aspiration of food and liquids into the airways. The chronic nature of the symptoms also supports this diagnosis. While the other options could cause some similar symptoms, the combination and duration of symptoms point more strongly towards achalasia cardia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_wrong_answers(log_file_baseline, wrong_indices_seed, answer_note = 'Response: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e21174a6-9633-49c5-9acf-4641ca3c1bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 35-year old lady presented with dysphagia>> nocturnal asthma and weight loss for 6 years. The most probable diagnosis is -: A. Achalasia cardia, B. Lye stricture of esophagus, C. Gastroesophageal reflux diseae, D. Cancer esophagus. Please select the correct answer from A, B, C, D. Put your answer in \\boxed{}.\n",
      "\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "with open(query_file, \"r\") as file:\n",
    "    queries = file.readlines()\n",
    "\n",
    "import json\n",
    "answer_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/answers.json\"\n",
    "with open(answer_file, \"r\") as file:\n",
    "    true_answers = json.load(file)\n",
    "response_mapping = {0:'A', 1:'B', 2:'C', 3:'D'}\n",
    "\n",
    "idx = 79\n",
    "print(queries[idx])\n",
    "print(response_mapping[true_answers[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902dd5a-9cd2-48de-9088-1862583e7f64",
   "metadata": {},
   "source": [
    "## baseline id 100-1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae5cab6-6c96-4575-8450-aac9e4f20bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905\n",
      "\n",
      "Query 1006: A 23-year-old woman has had altered consciousness and slurred speech for the past 24 hours. A head CT scan shows a right temporal hemorrhagic infarction. Cerebral angiography shows a distal right middle cerebral arterial occlusion. Within the past 3 years, she has had an episode of pulmonary embolism. A pregnancy 18 months ago ended in miscarriage. Laboratory studies show a false-positive serologic test for syphilis, normal prothrombin time (PT), elevated partial thromboplastin time (PTT), and normal platelet count. Which of the following is the most likely cause of these findings?: A. Antiphospholipid antibody, B. Disseminated intravascular coagulation, C. Factor V mutation, D. Hypercholesterolemia. Please select the correct answer from A, B, C, D. Put your answer in \\box{}.\n",
      "Response: The answer is: \\boxed{A}.\n",
      "Analysis: The patient's history of recurrent thrombotic events, including a pulmonary embolism and a cerebral arterial occlusion, along with a history of miscarriage and laboratory findings of a false-positive serologic test for syphilis and elevated partial thromboplastin time (PTT), points towards the presence of antiphospholipid antibodies. Antiphospholipid syndrome is characterized by the presence of these antibodies, which are associated with an increased risk of thrombosis and recurrent fetal loss. The false-positive serologic test for syphilis is a known association with antiphospholipid antibodies. The normal prothrombin time (PT) and normal platelet count, along with the elevated PTT, further support this diagnosis, as antiphospholipid antibodies can interfere with coagulation tests, particularly the PTT. Disseminated intravascular coagulation (B) would typically present with a more acute and severe clinical picture, including abnormalities in both PT and PTT, and often thrombocytopenia. Factor V mutation (C) could lead to an increased risk of thrombosis but would not explain the false-positive serologic test for syphilis or the specific pattern of laboratory coagulation abnormalities seen here. Hypercholesterolemia (D) is a risk factor for atherosclerosis but does not directly explain the patient's recurrent thrombotic events at a young age or the specific laboratory findings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#174-175, 409-415 should be one\n",
    "import re\n",
    "\n",
    "log_file = \"/scratch/gpfs/jx0800/data/graphrag/results/baseline_responses_2.txt\"\n",
    "with open(log_file, 'r') as file:\n",
    "    output_content = file.read()\n",
    "\n",
    "# Split the responses based on the separator\n",
    "responses = output_content.strip().split(\"-\" * 50)\n",
    "print(len(responses))\n",
    "print(responses[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2579e34d-5aad-4898-88d1-72def94fb815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898\n"
     ]
    }
   ],
   "source": [
    "responses = responses[:74] + responses[76:309] + responses[313:-1]\n",
    "print(len(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcf3660-4ded-45cf-98c6-66496f7cccbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Response 0:\n",
      "Processing Response 1:\n",
      "Processing Response 2:\n",
      "Processing Response 3:\n",
      "Processing Response 4:\n",
      "Processing Response 5:\n",
      "Processing Response 6:\n",
      "Processing Response 7:\n",
      "Processing Response 8:\n",
      "Processing Response 9:\n",
      "Processing Response 10:\n",
      "Processing Response 11:\n",
      "Processing Response 12:\n",
      "Processing Response 13:\n",
      "Processing Response 14:\n",
      "Processing Response 15:\n",
      "Processing Response 16:\n",
      "Processing Response 17:\n",
      "Processing Response 18:\n",
      "Processing Response 19:\n",
      "Processing Response 20:\n",
      "Processing Response 21:\n",
      "Processing Response 22:\n",
      "Processing Response 23:\n",
      "Processing Response 24:\n",
      "Processing Response 25:\n",
      "Processing Response 26:\n",
      "Processing Response 27:\n",
      "Processing Response 28:\n",
      "Processing Response 29:\n",
      "Processing Response 30:\n",
      "Processing Response 31:\n",
      "Processing Response 32:\n",
      "Processing Response 33:\n",
      "Processing Response 34:\n",
      "Processing Response 35:\n",
      "Processing Response 36:\n",
      "Processing Response 37:\n",
      "Processing Response 38:\n",
      "Processing Response 39:\n",
      "Processing Response 40:\n",
      "Processing Response 41:\n",
      "Processing Response 42:\n",
      "Processing Response 43:\n",
      "Processing Response 44:\n",
      "Processing Response 45:\n",
      "Processing Response 46:\n",
      "Processing Response 47:\n",
      "Processing Response 48:\n",
      "Processing Response 49:\n",
      "Processing Response 50:\n",
      "Processing Response 51:\n",
      "Processing Response 52:\n",
      "Processing Response 53:\n",
      "Processing Response 54:\n",
      "Processing Response 55:\n",
      "Processing Response 56:\n",
      "Processing Response 57:\n",
      "Processing Response 58:\n",
      "Processing Response 59:\n",
      "Processing Response 60:\n",
      "Processing Response 61:\n",
      "Processing Response 62:\n",
      "Processing Response 63:\n",
      "Processing Response 64:\n",
      "Processing Response 65:\n",
      "Processing Response 66:\n",
      "Processing Response 67:\n",
      "Processing Response 68:\n",
      "Processing Response 69:\n",
      "Processing Response 70:\n",
      "Processing Response 71:\n",
      "Processing Response 72:\n",
      "Processing Response 73:\n",
      "Processing Response 74:\n",
      "Processing Response 75:\n",
      "Processing Response 76:\n",
      "Processing Response 77:\n",
      "Processing Response 78:\n",
      "Processing Response 79:\n",
      "Processing Response 80:\n",
      "Processing Response 81:\n",
      "Processing Response 82:\n",
      "Processing Response 83:\n",
      "Processing Response 84:\n",
      "Processing Response 85:\n",
      "Processing Response 86:\n",
      "Processing Response 87:\n",
      "Processing Response 88:\n",
      "Processing Response 89:\n",
      "Processing Response 90:\n",
      "Processing Response 91:\n",
      "Processing Response 92:\n",
      "Processing Response 93:\n",
      "Processing Response 94:\n",
      "Processing Response 95:\n",
      "Processing Response 96:\n",
      "Processing Response 97:\n",
      "Processing Response 98:\n",
      "Processing Response 99:\n",
      "Processing Response 100:\n",
      "Processing Response 101:\n",
      "Processing Response 102:\n",
      "Processing Response 103:\n",
      "Processing Response 104:\n",
      "Processing Response 105:\n",
      "Processing Response 106:\n",
      "Processing Response 107:\n",
      "Processing Response 108:\n",
      "Processing Response 109:\n",
      "Processing Response 110:\n",
      "Processing Response 111:\n",
      "Processing Response 112:\n",
      "Processing Response 113:\n",
      "Processing Response 114:\n",
      "Processing Response 115:\n",
      "Processing Response 116:\n",
      "Processing Response 117:\n",
      "Processing Response 118:\n",
      "Processing Response 119:\n",
      "Processing Response 120:\n",
      "Processing Response 121:\n",
      "Processing Response 122:\n",
      "Processing Response 123:\n",
      "Processing Response 124:\n",
      "Processing Response 125:\n",
      "Processing Response 126:\n",
      "Processing Response 127:\n",
      "Processing Response 128:\n",
      "Processing Response 129:\n",
      "Processing Response 130:\n",
      "Processing Response 131:\n",
      "Processing Response 132:\n",
      "Processing Response 133:\n",
      "Processing Response 134:\n",
      "Processing Response 135:\n",
      "Processing Response 136:\n",
      "Processing Response 137:\n",
      "Processing Response 138:\n",
      "Processing Response 139:\n",
      "Processing Response 140:\n",
      "Processing Response 141:\n",
      "Processing Response 142:\n",
      "Processing Response 143:\n",
      "Processing Response 144:\n",
      "Processing Response 145:\n",
      "Processing Response 146:\n",
      "Processing Response 147:\n",
      "Processing Response 148:\n",
      "Processing Response 149:\n",
      "Processing Response 150:\n",
      "Processing Response 151:\n",
      "Processing Response 152:\n",
      "Processing Response 153:\n",
      "Processing Response 154:\n",
      "Processing Response 155:\n",
      "Processing Response 156:\n",
      "Processing Response 157:\n",
      "Processing Response 158:\n",
      "Processing Response 159:\n",
      "Processing Response 160:\n",
      "Processing Response 161:\n",
      "Processing Response 162:\n",
      "Processing Response 163:\n",
      "Processing Response 164:\n",
      "Processing Response 165:\n",
      "Processing Response 166:\n",
      "Processing Response 167:\n",
      "Processing Response 168:\n",
      "Processing Response 169:\n",
      "Processing Response 170:\n",
      "Processing Response 171:\n",
      "Processing Response 172:\n",
      "Processing Response 173:\n",
      "Processing Response 174:\n",
      "Processing Response 175:\n",
      "Processing Response 176:\n",
      "Processing Response 177:\n",
      "Processing Response 178:\n",
      "Processing Response 179:\n",
      "Processing Response 180:\n",
      "Processing Response 181:\n",
      "Processing Response 182:\n",
      "Processing Response 183:\n",
      "Processing Response 184:\n",
      "Processing Response 185:\n",
      "Processing Response 186:\n",
      "Processing Response 187:\n",
      "Processing Response 188:\n",
      "Processing Response 189:\n",
      "Processing Response 190:\n",
      "Processing Response 191:\n",
      "Processing Response 192:\n",
      "Processing Response 193:\n",
      "Processing Response 194:\n",
      "Processing Response 195:\n",
      "Processing Response 196:\n",
      "Processing Response 197:\n",
      "Processing Response 198:\n",
      "Processing Response 199:\n",
      "Processing Response 200:\n",
      "Processing Response 201:\n",
      "Processing Response 202:\n",
      "Processing Response 203:\n",
      "Processing Response 204:\n",
      "Processing Response 205:\n",
      "Processing Response 206:\n",
      "Processing Response 207:\n",
      "Processing Response 208:\n",
      "Processing Response 209:\n",
      "Processing Response 210:\n",
      "Processing Response 211:\n",
      "Processing Response 212:\n",
      "Processing Response 213:\n",
      "Processing Response 214:\n",
      "Processing Response 215:\n",
      "Processing Response 216:\n",
      "Processing Response 217:\n",
      "Processing Response 218:\n",
      "Processing Response 219:\n",
      "Processing Response 220:\n",
      "Processing Response 221:\n",
      "Processing Response 222:\n",
      "Processing Response 223:\n",
      "Processing Response 224:\n",
      "Processing Response 225:\n",
      "Processing Response 226:\n",
      "Processing Response 227:\n",
      "Processing Response 228:\n",
      "Processing Response 229:\n",
      "Processing Response 230:\n",
      "Processing Response 231:\n",
      "Processing Response 232:\n",
      "Processing Response 233:\n",
      "Processing Response 234:\n",
      "Processing Response 235:\n",
      "Processing Response 236:\n",
      "Processing Response 237:\n",
      "Processing Response 238:\n",
      "Processing Response 239:\n",
      "Processing Response 240:\n",
      "Processing Response 241:\n",
      "Processing Response 242:\n",
      "Processing Response 243:\n",
      "Processing Response 244:\n",
      "Processing Response 245:\n",
      "Processing Response 246:\n",
      "Processing Response 247:\n",
      "Processing Response 248:\n",
      "Processing Response 249:\n",
      "Processing Response 250:\n",
      "Processing Response 251:\n",
      "Processing Response 252:\n",
      "Processing Response 253:\n",
      "Processing Response 254:\n",
      "Processing Response 255:\n",
      "Processing Response 256:\n",
      "Processing Response 257:\n",
      "Processing Response 258:\n",
      "Processing Response 259:\n",
      "Processing Response 260:\n",
      "Processing Response 261:\n",
      "Processing Response 262:\n",
      "Processing Response 263:\n",
      "Processing Response 264:\n",
      "Processing Response 265:\n",
      "Processing Response 266:\n",
      "Processing Response 267:\n",
      "Processing Response 268:\n",
      "Processing Response 269:\n",
      "Processing Response 270:\n",
      "Processing Response 271:\n",
      "Processing Response 272:\n",
      "Processing Response 273:\n",
      "Processing Response 274:\n",
      "Processing Response 275:\n",
      "Processing Response 276:\n",
      "Processing Response 277:\n",
      "Processing Response 278:\n",
      "Processing Response 279:\n",
      "Processing Response 280:\n",
      "Processing Response 281:\n",
      "Processing Response 282:\n",
      "Processing Response 283:\n",
      "Processing Response 284:\n",
      "Processing Response 285:\n",
      "Processing Response 286:\n",
      "Processing Response 287:\n",
      "Processing Response 288:\n",
      "Processing Response 289:\n",
      "Processing Response 290:\n",
      "Processing Response 291:\n",
      "Processing Response 292:\n",
      "Processing Response 293:\n",
      "Processing Response 294:\n",
      "Processing Response 295:\n",
      "Processing Response 296:\n",
      "Processing Response 297:\n",
      "Processing Response 298:\n",
      "Processing Response 299:\n",
      "Processing Response 300:\n",
      "Processing Response 301:\n",
      "Processing Response 302:\n",
      "Processing Response 303:\n",
      "Processing Response 304:\n",
      "Processing Response 305:\n",
      "Processing Response 306:\n",
      "Processing Response 307:\n",
      "Processing Response 308:\n",
      "Processing Response 309:\n",
      "Processing Response 310:\n",
      "Processing Response 311:\n",
      "Processing Response 312:\n",
      "Processing Response 313:\n",
      "Processing Response 314:\n",
      "Processing Response 315:\n",
      "Processing Response 316:\n",
      "Processing Response 317:\n",
      "Processing Response 318:\n",
      "Processing Response 319:\n",
      "Processing Response 320:\n",
      "Processing Response 321:\n",
      "Processing Response 322:\n",
      "Processing Response 323:\n",
      "Processing Response 324:\n",
      "Processing Response 325:\n",
      "Processing Response 326:\n",
      "Processing Response 327:\n",
      "Processing Response 328:\n",
      "Processing Response 329:\n",
      "Processing Response 330:\n",
      "Processing Response 331:\n",
      "Processing Response 332:\n",
      "Processing Response 333:\n",
      "Processing Response 334:\n",
      "Processing Response 335:\n",
      "Processing Response 336:\n",
      "Processing Response 337:\n",
      "Processing Response 338:\n",
      "Processing Response 339:\n",
      "Processing Response 340:\n",
      "Processing Response 341:\n",
      "Processing Response 342:\n",
      "Processing Response 343:\n",
      "Processing Response 344:\n",
      "Processing Response 345:\n",
      "Processing Response 346:\n",
      "Processing Response 347:\n",
      "Processing Response 348:\n",
      "Processing Response 349:\n",
      "Processing Response 350:\n",
      "Processing Response 351:\n",
      "Processing Response 352:\n",
      "Processing Response 353:\n",
      "Processing Response 354:\n",
      "Processing Response 355:\n",
      "Processing Response 356:\n",
      "Processing Response 357:\n",
      "Processing Response 358:\n",
      "Processing Response 359:\n",
      "Processing Response 360:\n",
      "Processing Response 361:\n",
      "Processing Response 362:\n",
      "Processing Response 363:\n",
      "Processing Response 364:\n",
      "Processing Response 365:\n",
      "Processing Response 366:\n",
      "Processing Response 367:\n",
      "Processing Response 368:\n",
      "Processing Response 369:\n",
      "Processing Response 370:\n",
      "Processing Response 371:\n",
      "Processing Response 372:\n",
      "Processing Response 373:\n",
      "Processing Response 374:\n",
      "Processing Response 375:\n",
      "Processing Response 376:\n",
      "Processing Response 377:\n",
      "Processing Response 378:\n",
      "Processing Response 379:\n",
      "Processing Response 380:\n",
      "Processing Response 381:\n",
      "Processing Response 382:\n",
      "Processing Response 383:\n",
      "Processing Response 384:\n",
      "Processing Response 385:\n",
      "Processing Response 386:\n",
      "Processing Response 387:\n",
      "Processing Response 388:\n",
      "Processing Response 389:\n",
      "Processing Response 390:\n",
      "Processing Response 391:\n",
      "Processing Response 392:\n",
      "Processing Response 393:\n",
      "Processing Response 394:\n",
      "Processing Response 395:\n",
      "Processing Response 396:\n",
      "Processing Response 397:\n",
      "Processing Response 398:\n",
      "Processing Response 399:\n",
      "Processing Response 400:\n",
      "Processing Response 401:\n",
      "Processing Response 402:\n",
      "Processing Response 403:\n",
      "Processing Response 404:\n",
      "Processing Response 405:\n",
      "Processing Response 406:\n",
      "Processing Response 407:\n",
      "Processing Response 408:\n",
      "Processing Response 409:\n",
      "Processing Response 410:\n",
      "Processing Response 411:\n",
      "Processing Response 412:\n",
      "Processing Response 413:\n",
      "Processing Response 414:\n",
      "Processing Response 415:\n",
      "Processing Response 416:\n",
      "Processing Response 417:\n",
      "Processing Response 418:\n",
      "Processing Response 419:\n",
      "Processing Response 420:\n",
      "Processing Response 421:\n",
      "Processing Response 422:\n",
      "Processing Response 423:\n",
      "Processing Response 424:\n",
      "Processing Response 425:\n",
      "Processing Response 426:\n",
      "Processing Response 427:\n",
      "Processing Response 428:\n",
      "Processing Response 429:\n",
      "Processing Response 430:\n",
      "Processing Response 431:\n",
      "Processing Response 432:\n",
      "Processing Response 433:\n",
      "Processing Response 434:\n",
      "Processing Response 435:\n",
      "Processing Response 436:\n",
      "Processing Response 437:\n",
      "Processing Response 438:\n",
      "Processing Response 439:\n",
      "Processing Response 440:\n",
      "Processing Response 441:\n",
      "Processing Response 442:\n",
      "Processing Response 443:\n",
      "Processing Response 444:\n",
      "Processing Response 445:\n",
      "Processing Response 446:\n",
      "Processing Response 447:\n",
      "Processing Response 448:\n",
      "Processing Response 449:\n",
      "Processing Response 450:\n",
      "Processing Response 451:\n",
      "Processing Response 452:\n",
      "Processing Response 453:\n",
      "Processing Response 454:\n",
      "Processing Response 455:\n",
      "Processing Response 456:\n",
      "Processing Response 457:\n",
      "Processing Response 458:\n",
      "Processing Response 459:\n",
      "Processing Response 460:\n",
      "Processing Response 461:\n",
      "Processing Response 462:\n",
      "Processing Response 463:\n",
      "Processing Response 464:\n",
      "Processing Response 465:\n",
      "Processing Response 466:\n",
      "Processing Response 467:\n",
      "Processing Response 468:\n",
      "Processing Response 469:\n",
      "Processing Response 470:\n",
      "Processing Response 471:\n",
      "Processing Response 472:\n",
      "Processing Response 473:\n",
      "Processing Response 474:\n",
      "Processing Response 475:\n",
      "Processing Response 476:\n",
      "Processing Response 477:\n",
      "Processing Response 478:\n",
      "Processing Response 479:\n",
      "Processing Response 480:\n",
      "Processing Response 481:\n",
      "Processing Response 482:\n",
      "Processing Response 483:\n",
      "Processing Response 484:\n",
      "Processing Response 485:\n",
      "Processing Response 486:\n",
      "Processing Response 487:\n",
      "Processing Response 488:\n",
      "Processing Response 489:\n",
      "Processing Response 490:\n",
      "Processing Response 491:\n",
      "Processing Response 492:\n",
      "Processing Response 493:\n",
      "Processing Response 494:\n",
      "Processing Response 495:\n",
      "Processing Response 496:\n",
      "Processing Response 497:\n",
      "Processing Response 498:\n",
      "Processing Response 499:\n",
      "Processing Response 500:\n",
      "Processing Response 501:\n",
      "Processing Response 502:\n",
      "Processing Response 503:\n",
      "Processing Response 504:\n",
      "Processing Response 505:\n",
      "Processing Response 506:\n",
      "Processing Response 507:\n",
      "Processing Response 508:\n",
      "Processing Response 509:\n",
      "Processing Response 510:\n",
      "Processing Response 511:\n",
      "Processing Response 512:\n",
      "Processing Response 513:\n",
      "Processing Response 514:\n",
      "Processing Response 515:\n",
      "Processing Response 516:\n",
      "Processing Response 517:\n",
      "Processing Response 518:\n",
      "Processing Response 519:\n",
      "Processing Response 520:\n",
      "Processing Response 521:\n",
      "Processing Response 522:\n",
      "Processing Response 523:\n",
      "Processing Response 524:\n",
      "Processing Response 525:\n",
      "Processing Response 526:\n",
      "Processing Response 527:\n",
      "Processing Response 528:\n",
      "Processing Response 529:\n",
      "Processing Response 530:\n",
      "Processing Response 531:\n",
      "Processing Response 532:\n",
      "Processing Response 533:\n",
      "Processing Response 534:\n",
      "Processing Response 535:\n",
      "Processing Response 536:\n",
      "Processing Response 537:\n",
      "Processing Response 538:\n",
      "Processing Response 539:\n",
      "Processing Response 540:\n",
      "Processing Response 541:\n",
      "Processing Response 542:\n",
      "Processing Response 543:\n",
      "Processing Response 544:\n",
      "Processing Response 545:\n",
      "Processing Response 546:\n",
      "Processing Response 547:\n",
      "Processing Response 548:\n",
      "Processing Response 549:\n",
      "Processing Response 550:\n",
      "Processing Response 551:\n",
      "Processing Response 552:\n",
      "Processing Response 553:\n",
      "Processing Response 554:\n",
      "Processing Response 555:\n",
      "Processing Response 556:\n",
      "Processing Response 557:\n",
      "Processing Response 558:\n",
      "Processing Response 559:\n",
      "Processing Response 560:\n",
      "Processing Response 561:\n",
      "Processing Response 562:\n",
      "Processing Response 563:\n",
      "Processing Response 564:\n",
      "Processing Response 565:\n",
      "Processing Response 566:\n",
      "Processing Response 567:\n",
      "Processing Response 568:\n",
      "Processing Response 569:\n",
      "Processing Response 570:\n",
      "Processing Response 571:\n",
      "Processing Response 572:\n",
      "Processing Response 573:\n",
      "Processing Response 574:\n",
      "Processing Response 575:\n",
      "Processing Response 576:\n",
      "Processing Response 577:\n",
      "Processing Response 578:\n",
      "Processing Response 579:\n",
      "Processing Response 580:\n",
      "Processing Response 581:\n",
      "Processing Response 582:\n",
      "Processing Response 583:\n",
      "Processing Response 584:\n",
      "Processing Response 585:\n",
      "Processing Response 586:\n",
      "Processing Response 587:\n",
      "Processing Response 588:\n",
      "Processing Response 589:\n",
      "Processing Response 590:\n",
      "Processing Response 591:\n",
      "Processing Response 592:\n",
      "Processing Response 593:\n",
      "Processing Response 594:\n",
      "Processing Response 595:\n",
      "Processing Response 596:\n",
      "Processing Response 597:\n",
      "Processing Response 598:\n",
      "Processing Response 599:\n",
      "Processing Response 600:\n",
      "Processing Response 601:\n",
      "Processing Response 602:\n",
      "Processing Response 603:\n",
      "Processing Response 604:\n",
      "Processing Response 605:\n",
      "Processing Response 606:\n",
      "Processing Response 607:\n",
      "Processing Response 608:\n",
      "Processing Response 609:\n",
      "Processing Response 610:\n",
      "Processing Response 611:\n",
      "Processing Response 612:\n",
      "Processing Response 613:\n",
      "Processing Response 614:\n",
      "Processing Response 615:\n",
      "Processing Response 616:\n",
      "Processing Response 617:\n",
      "Processing Response 618:\n",
      "Processing Response 619:\n",
      "Processing Response 620:\n",
      "Processing Response 621:\n",
      "Processing Response 622:\n",
      "Processing Response 623:\n",
      "Processing Response 624:\n",
      "Processing Response 625:\n",
      "Processing Response 626:\n",
      "Processing Response 627:\n",
      "Processing Response 628:\n",
      "Processing Response 629:\n",
      "Processing Response 630:\n",
      "Processing Response 631:\n",
      "Processing Response 632:\n",
      "Processing Response 633:\n",
      "Processing Response 634:\n",
      "Processing Response 635:\n",
      "Processing Response 636:\n",
      "Processing Response 637:\n",
      "Processing Response 638:\n",
      "Processing Response 639:\n",
      "Processing Response 640:\n",
      "Processing Response 641:\n",
      "Processing Response 642:\n",
      "Processing Response 643:\n",
      "Processing Response 644:\n",
      "Processing Response 645:\n",
      "Processing Response 646:\n",
      "Processing Response 647:\n",
      "Processing Response 648:\n",
      "Processing Response 649:\n",
      "Processing Response 650:\n",
      "Processing Response 651:\n",
      "Processing Response 652:\n",
      "Processing Response 653:\n",
      "Processing Response 654:\n",
      "Processing Response 655:\n",
      "Processing Response 656:\n",
      "Processing Response 657:\n",
      "Processing Response 658:\n",
      "Processing Response 659:\n",
      "Processing Response 660:\n",
      "Processing Response 661:\n",
      "Processing Response 662:\n",
      "Processing Response 663:\n",
      "Processing Response 664:\n",
      "Processing Response 665:\n",
      "Processing Response 666:\n",
      "Processing Response 667:\n",
      "Processing Response 668:\n",
      "Processing Response 669:\n",
      "Processing Response 670:\n",
      "Processing Response 671:\n",
      "Processing Response 672:\n",
      "Processing Response 673:\n",
      "Processing Response 674:\n",
      "Processing Response 675:\n",
      "Processing Response 676:\n",
      "Processing Response 677:\n",
      "Processing Response 678:\n",
      "Processing Response 679:\n",
      "Processing Response 680:\n",
      "Processing Response 681:\n",
      "Processing Response 682:\n",
      "Processing Response 683:\n",
      "Processing Response 684:\n",
      "Processing Response 685:\n",
      "Processing Response 686:\n",
      "Processing Response 687:\n",
      "Processing Response 688:\n",
      "Processing Response 689:\n",
      "Processing Response 690:\n",
      "Processing Response 691:\n",
      "Processing Response 692:\n",
      "Processing Response 693:\n",
      "Processing Response 694:\n",
      "Processing Response 695:\n",
      "Processing Response 696:\n",
      "Processing Response 697:\n",
      "Processing Response 698:\n",
      "Processing Response 699:\n",
      "Processing Response 700:\n",
      "Processing Response 701:\n",
      "Processing Response 702:\n",
      "Processing Response 703:\n",
      "Processing Response 704:\n",
      "Processing Response 705:\n",
      "Processing Response 706:\n",
      "Processing Response 707:\n",
      "Processing Response 708:\n",
      "Processing Response 709:\n",
      "Processing Response 710:\n",
      "Processing Response 711:\n",
      "Processing Response 712:\n",
      "Processing Response 713:\n",
      "Processing Response 714:\n",
      "Processing Response 715:\n",
      "Processing Response 716:\n",
      "Processing Response 717:\n",
      "Processing Response 718:\n",
      "Processing Response 719:\n",
      "Processing Response 720:\n",
      "Processing Response 721:\n",
      "Processing Response 722:\n",
      "Processing Response 723:\n",
      "Processing Response 724:\n",
      "Processing Response 725:\n",
      "Processing Response 726:\n",
      "Processing Response 727:\n",
      "Processing Response 728:\n",
      "Processing Response 729:\n",
      "Processing Response 730:\n",
      "Processing Response 731:\n",
      "Processing Response 732:\n",
      "Processing Response 733:\n",
      "Processing Response 734:\n",
      "Processing Response 735:\n",
      "Processing Response 736:\n",
      "Processing Response 737:\n",
      "Processing Response 738:\n",
      "Processing Response 739:\n",
      "Processing Response 740:\n",
      "Processing Response 741:\n",
      "Processing Response 742:\n",
      "Processing Response 743:\n",
      "Processing Response 744:\n",
      "Processing Response 745:\n",
      "Processing Response 746:\n",
      "Processing Response 747:\n",
      "Processing Response 748:\n",
      "Processing Response 749:\n",
      "Processing Response 750:\n",
      "Processing Response 751:\n",
      "Processing Response 752:\n",
      "Processing Response 753:\n",
      "Processing Response 754:\n",
      "Processing Response 755:\n",
      "Processing Response 756:\n",
      "Processing Response 757:\n",
      "Processing Response 758:\n",
      "Processing Response 759:\n",
      "Processing Response 760:\n",
      "Processing Response 761:\n",
      "Processing Response 762:\n",
      "Processing Response 763:\n",
      "Processing Response 764:\n",
      "Processing Response 765:\n",
      "Processing Response 766:\n",
      "Processing Response 767:\n",
      "Processing Response 768:\n",
      "Processing Response 769:\n",
      "Processing Response 770:\n",
      "Processing Response 771:\n",
      "Processing Response 772:\n",
      "Processing Response 773:\n",
      "Processing Response 774:\n",
      "Processing Response 775:\n",
      "Processing Response 776:\n",
      "Processing Response 777:\n",
      "Processing Response 778:\n",
      "Processing Response 779:\n",
      "Processing Response 780:\n",
      "Processing Response 781:\n",
      "Processing Response 782:\n",
      "Processing Response 783:\n",
      "Processing Response 784:\n",
      "Processing Response 785:\n",
      "Processing Response 786:\n",
      "Processing Response 787:\n",
      "Processing Response 788:\n",
      "Processing Response 789:\n",
      "Processing Response 790:\n",
      "Processing Response 791:\n",
      "Processing Response 792:\n",
      "Processing Response 793:\n",
      "Processing Response 794:\n",
      "Processing Response 795:\n",
      "Processing Response 796:\n",
      "Processing Response 797:\n",
      "Processing Response 798:\n",
      "Processing Response 799:\n",
      "Processing Response 800:\n",
      "Processing Response 801:\n",
      "Processing Response 802:\n",
      "Processing Response 803:\n",
      "Processing Response 804:\n",
      "Processing Response 805:\n",
      "Processing Response 806:\n",
      "Processing Response 807:\n",
      "Processing Response 808:\n",
      "Processing Response 809:\n",
      "Processing Response 810:\n",
      "Processing Response 811:\n",
      "Processing Response 812:\n",
      "Processing Response 813:\n",
      "Processing Response 814:\n",
      "Processing Response 815:\n",
      "Processing Response 816:\n",
      "Processing Response 817:\n",
      "Processing Response 818:\n",
      "Processing Response 819:\n",
      "Processing Response 820:\n",
      "Processing Response 821:\n",
      "Processing Response 822:\n",
      "Processing Response 823:\n",
      "Processing Response 824:\n",
      "Processing Response 825:\n",
      "Processing Response 826:\n",
      "Processing Response 827:\n",
      "Processing Response 828:\n",
      "Processing Response 829:\n",
      "Processing Response 830:\n",
      "Processing Response 831:\n",
      "Processing Response 832:\n",
      "Processing Response 833:\n",
      "Processing Response 834:\n",
      "Processing Response 835:\n",
      "Processing Response 836:\n",
      "Processing Response 837:\n",
      "Processing Response 838:\n",
      "Processing Response 839:\n",
      "Processing Response 840:\n",
      "Processing Response 841:\n",
      "Processing Response 842:\n",
      "Processing Response 843:\n",
      "Processing Response 844:\n",
      "Processing Response 845:\n",
      "Processing Response 846:\n",
      "Processing Response 847:\n",
      "Processing Response 848:\n",
      "Processing Response 849:\n",
      "Processing Response 850:\n",
      "Processing Response 851:\n",
      "Processing Response 852:\n",
      "Processing Response 853:\n",
      "Processing Response 854:\n",
      "Processing Response 855:\n",
      "Processing Response 856:\n",
      "Processing Response 857:\n",
      "Processing Response 858:\n",
      "Processing Response 859:\n",
      "Processing Response 860:\n",
      "Processing Response 861:\n",
      "Processing Response 862:\n",
      "Processing Response 863:\n",
      "Processing Response 864:\n",
      "Processing Response 865:\n",
      "Processing Response 866:\n",
      "Processing Response 867:\n",
      "Processing Response 868:\n",
      "Processing Response 869:\n",
      "Processing Response 870:\n",
      "Processing Response 871:\n",
      "Processing Response 872:\n",
      "Processing Response 873:\n",
      "Processing Response 874:\n",
      "Processing Response 875:\n",
      "Processing Response 876:\n",
      "Processing Response 877:\n",
      "Processing Response 878:\n",
      "Processing Response 879:\n",
      "Processing Response 880:\n",
      "Processing Response 881:\n",
      "Processing Response 882:\n",
      "Processing Response 883:\n",
      "Processing Response 884:\n",
      "Processing Response 885:\n",
      "Processing Response 886:\n",
      "Processing Response 887:\n",
      "Processing Response 888:\n",
      "Processing Response 889:\n",
      "Processing Response 890:\n",
      "Processing Response 891:\n",
      "Processing Response 892:\n",
      "Processing Response 893:\n",
      "Processing Response 894:\n",
      "Processing Response 895:\n",
      "Processing Response 896:\n",
      "Processing Response 897:\n",
      "898\n",
      "['D', 'B', 'A', 'B', 'D', 'D', 'B', 'C', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'B. ace', 'C', 'C', 'A', 'B', 'A', 'A', 'D', 'A', 'C. be', 'A', 'D', 'B', 'D', 'D', 'B', 'A', 'C. 200-400 mL (for 4 kg child, 75 mL/kg for some dehydration, so 4 kg x 75 mL/kg = 300 mL, but the closest answer is 300 mL which is not available, however 400 mL is given for mild to moderate dehydration which is the closest option, but according to the WHO guideline for a child with some dehydration the amount of ORS to be given is 75 mL/kg over 4 hours, hence for a 4 kg child it will be 300 mL, hence the best option is C. 400 mL as the exact option is not available)', 'C', 'A', 'B', 'A', 'A', 'B', 'C. 98%', 'B', 'A', 'A', 'C', 'A', 'C', 'A', 'B', 'A', 'B', 'C', 'C', 'D', 'B', 'D', 'D', 'A. cde', 'D', 'A', 'A', 'C', 'A', 'C', 'A', 'B', 'B', 'C', 'B', 'B', 'B', 'A', 'D. abde', 'A', 'B', 'D', 'B', 'A', 'C. Detemir', 'A', 'D', 'B', 'C', 'B', 'B', 'C', 'C', 'D', 'D', 'C', 'A', 'B', 'A', 'A', 'B', 'D', 'A', 'A', 'D', 'A', 'A', 'B', 'D', 'C', 'A', 'D', 'C. abe', 'D', 'A', 'A', 'A', 'A', 'D', 'C', 'B', 'D', 'C', 'B', 'A', 'A', 'D. abde', 'D', 'D', 'B', 'A', 'C', 'C', 'A', 'A', 'C', 'B', 'B', 'B', 'A', 'C. 100 mm', 'C', 'B', 'C', 'C', 'D', 'A', 'A', 'D', 'A', 'D', 'B, D', 'C', 'D', 'A', 'C', 'C', 'D', 'D. ab', 'B', 'C', 'A', 'D. ab', 'B', 'D', 'B', 'A', 'B. 14 days', 'C', 'A', 'B', 'B', 'A', 'B', 'D', 'D', 'B', 'C', 'A', 'A', 'C', 'B', 'C', 'B', 'B. ab', 'A', 'D', 'A', 'C', 'A', 'A. bde', 'A', 'D', 'B', 'C', 'D', 'D', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'D', 'D', 'B', 'C', 'D', 'B', 'D', 'B', 'B', 'B', 'D', 'A', 'B. de', 'D', 'B', 'D', 'B', 'A', 'A', 'C', 'A', 'B', 'A', 'B', 'C', 'D', 'B', 'A', 'B', 'A', 'C', 'A', 'B', 'C', 'C', 'A', 'D', 'A', 'B', 'B', 'B', 'A', 'D', 'C', 'A', 'A', 'C. acd', 'C', 'B', 'D', 'A', 'B', 'D', 'D', 'D', 'C. 50 to 60 %', 'B', 'D', 'D', 'D', 'C', 'D', 'D', 'C', 'C', 'B', 'B', 'C', 'B', 'D', 'D', 'D', 'A', 'A', 'C', 'C', 'B', 'A', 'A', 'C', 'D. ad', 'C', 'B', 'B', 'A', 'D', 'C', 'C', 'A', 'B', 'B', 'C', 'A', 'A', 'C', 'B', 'C. 20 mm Hg', 'B', 'A', 'B', 'D', 'C', 'C', 'B', 'D', 'D', 'A', 'B', 'B. Hemoglobin A1c', 'C', 'B', 'C. abd', 'B', 'C', 'B', 'D', 'A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'D', 'D', 'D. 80%', 'B', 'A', 'D', 'C', 'A', 'B', 'A', 'D', 'B', 'C', 'B', 'B', 'B', 'B', 'A', 'D', 'B', 'C', 'B', 'B', 'C', 'D', 'D', 'C', 'D', 'A', 'C', 'A', 'A', 'D', 'D', 'D', 'B', 'C', 'B', 'A', 'A', 'D', 'A', 'A', 'A', 'D', 'C', 'B', 'A', 'A', 'D', 'D', 'A', 'A', 'D', 'A', 'A', 'C', 'D. ab', 'B', 'C', 'D', 'A', 'B', 'C', 'B', 'C', 'C', 'D', 'B', 'B', 'A', 'D', 'D. de', 'B', 'A', 'B', 'D', 'C', 'B', 'C', 'D', 'B', 'C', 'B', 'B', 'C', 'C', 'D', 'A', 'D', 'C', 'B', 'C', 'D', 'A', 'C', 'C', 'D', 'C', 'B. d', 'A', 'D', 'D', 'A', 'C', 'C', 'A', 'A', 'B', 'A', 'D', 'B', 'C', 'B', 'A, B, C, D', 'C', 'C', 'A', 'A', 'B', 'B. 12 hours', 'C', 'B', 'B', 'D', 'C', 'B', 'A', 'B', 'B', 'C', 'C', 'B', 'D', 'D', 'D', 'A', 'B', 'A', 'C', 'B', 'A', 'B', 'C', 'D', 'B', 'C', 'C', 'D', 'A, B, C', 'A', 'D', 'C', 'D', 'C', 'D', 'B', 'B', 'B', 'A', 'D', 'B', 'B', 'A', 'B', 'B', 'B', 'D', 'C', 'B', 'A', 'D', 'B', 'D', 'B', 'C', 'B', 'B', 'D', 'C', 'B', 'A', 'A, B, D', 'A', 'B', 'A', 'B', 'A', 'C', 'A', 'D', 'A', 'B', 'C', 'D', 'D', 'A', 'C', 'C', 'D', 'A', 'B', 'B', 'D', 'B. 4 min is often cited but the correct answer is actually 4 minutes is the recommended time, however the option is not available so the closest one is', 'D', 'C', 'B', 'A', 'B', 'D', 'C', 'B', 'C', 'C', 'B', 'B', 'D', 'A', 'B', 'B', 'A', 'A', 'C', 'B', 'B', 'C', 'D', 'B', 'B', 'B', 'D', 'C', 'A', 'A', 'C', 'C', 'C', 'C', 'A', 'B', 'A', 'C', 'A', 'D', 'A', 'D', 'D', 'B', 'B', 'D', 'A', 'D', 'D', 'C', 'D', 'A', 'C', 'C', 'C. 10-11 kg', 'A', 'A, B, C', 'B', 'A', 'C', 'B', 'B', 'D', 'C', 'D', 'A', 'B', 'C', 'C', 'D. bcde', 'C', 'C', 'C. 3 ATPs', 'B', 'C', 'A', 'A', 'B', 'D', 'B', 'C', 'D', 'D', 'B', 'B', 'B', 'A', 'C', 'B. 6.50%', 'B', 'B', 'C and D', 'C', 'A', 'C', 'B', 'C', 'B', 'B', 'A', 'C', 'B', 'B', 'A', 'D', 'B', 'B', 'B', 'C. abe', 'B', 'A', 'B', 'B', 'A', 'D', 'D', 'A', 'B', 'A', 'C', 'B', 'C', 'C', 'A', 'A', 'A', 'A', 'D', 'B', 'B', 'A, B, D', 'C', 'A', 'C', 'B', 'D', 'B', 'D', 'C', 'D', 'B', 'B', 'C', 'D', 'B. bcd', 'C. 25 mm Hg', 'C', 'B', 'B', 'B', 'A', 'B. abce', 'A', 'C', 'A', 'C', 'C', 'D', 'D', 'C', 'B', 'A', 'A, B, D', 'B', 'A', 'B', 'C', 'B', 'C', 'B', 'C', 'B', 'C', 'C', 'D', 'B', 'C', 'A', 'A', 'B', 'B', 'C', 'B', 'A', 'B', 'D', 'C', 'D', 'C', 'D', 'D', 'B', 'C', 'A', 'A', 'B', 'B', 'B', 'C', 'A', 'D', 'C', 'D', 'D', 'B', 'B', 'A', 'A', 'C', 'C', 'D', 'D', 'B', 'D', 'D', 'A', 'B', 'A', 'D', 'A', 'A', 'B', 'D', 'A', 'B', 'A', 'A', 'D', 'A', 'A', 'C', 'A', 'B', 'D', 'B', 'C', 'D', 'A', 'B', 'B', 'B', 'A', 'A', 'C', 'A', 'B. abde', 'D', 'A', 'D', 'C', 'B', 'A', 'B', 'A', 'B', 'B', 'C', 'A', 'D', 'C', 'B', 'A', 'C', 'D', 'C', 'A', 'C', 'B', 'B', 'C', 'D', 'A', 'C', 'B', 'D. 20%', 'A', 'B', 'C', 'B', 'B', 'D', 'A', 'D', 'A', 'B', 'D', 'B', 'D', 'A', 'A', 'B', 'A', 'D', 'C', 'B', 'A', 'A', 'C', 'C', 'D', 'D', 'B', 'D', 'C', 'A', 'A', 'D', 'C', 'B', 'A', 'A', 'A', 'D', 'D', 'C', 'B', 'A', 'A', 'A', 'A', 'D', 'C', 'D', 'C', 'C', 'A', 'B', 'A', 'A', 'A', 'D', 'C', 'A', 'A', 'B', 'C', 'A', 'D', 'C', 'B', 'B', 'A', 'B', 'A', 'D. acd', 'A', 'A', 'D', 'A', 'D', 'A', 'A', 'C', 'A', 'D', 'D', 'B', 'B', 'B', 'A', 'C', 'A', 'D', 'C', 'A']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regular expression to find the part after 'SUCCESS: Local Search Response:'\n",
    "success_pattern = re.compile(r'Response: (.*)', re.DOTALL)\n",
    "\n",
    "# Regular expression to find the content inside \\boxed{}\n",
    "boxed_pattern = re.compile(r'\\\\boxed\\{([^}]+)\\}')\n",
    "box_pattern = re.compile(r'\\\\box\\{([^}]+)\\}')\n",
    "\n",
    "seed_answers = []\n",
    "# Process each response\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Processing Response {i}:\")\n",
    "    \n",
    "    # Find the part after 'SUCCESS: Local Search Response:'\n",
    "    success_match = success_pattern.search(response)\n",
    "    if success_match:\n",
    "        success_text = success_match.group(1).strip()\n",
    "        #print(f\"SUCCESS Text: {success_text}\")\n",
    "        \n",
    "        # Check if there's an answer in \\boxed{}\n",
    "        boxed_match = boxed_pattern.search(success_text)\n",
    "        if boxed_match:\n",
    "            answer = boxed_match.group(1)\n",
    "            seed_answers.append(answer)\n",
    "            #print(f\"Answer found in \\\\boxed{{}}: {answer}\")\n",
    "        else:\n",
    "            box_match = box_pattern.search(success_text)\n",
    "            if box_match:\n",
    "                answer = box_match.group(1)\n",
    "                seed_answers.append(answer)\n",
    "            else:\n",
    "                print(\"No answer found in \\\\boxed{}.\")\n",
    "    else:\n",
    "        print(\"No 'SUCCESS: Local Search Response:' found in this response.\")\n",
    "\n",
    "print(len(seed_answers))\n",
    "print(seed_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0726411-e216-4695-9be3-ff0fe205aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898\n"
     ]
    }
   ],
   "source": [
    "# Mapping of valid responses\n",
    "response_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "\n",
    "# Function to process each response\n",
    "def process_response(response):\n",
    "    # Check if the response starts with A, B, C, or D\n",
    "    if response[0] in response_mapping:\n",
    "        # Extract the first character (A, B, C, or D)\n",
    "        key = response[0]\n",
    "        return response_mapping[key]\n",
    "    else:\n",
    "        # Handle invalid responses\n",
    "        return 4\n",
    "\n",
    "# Process the entire response list\n",
    "processed_seed_responses = [process_response(response) for response in seed_answers]\n",
    "\n",
    "print(len(processed_seed_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd5c861-4abe-4b6d-98ce-75c9450c4a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "A 48-year-old Caucasian man, diagnosed with diabetes 5 years ago, comes to the physician because of palpitations, Physical examination shows a diffuse darkening of his skin and testicular atrophy. Laboratory studies show elevated liver function and elevated blood sugars. A liver biopsy shows significantly elevated iron levels A diagnosis of hereditary hemochromatosis is made. Which one of the following laboratory findings is seen in hereditary hemochromatosis?: A. Decreased serum ceruloplasmin, B. Decreased serum ferritin, C. Decreased serum iron, D. Decreased total iron-binding capacity. Please select the correct answer from A, B, C, D. Put your answer in \\boxed{}.\n",
      "\n",
      "A man presents with weakness, pain in upperabdomen, hyperpigmentation, arthritis,hyperglycemia and enlarged palpable liver. Mostprobable diagnosis is -: A. Haemochromatosis, B. Addison's disease, C. Insulin dependent diabetes mellitus, D. Cushing's syndrome. Please select the correct answer from A, B, C, D. Put your answer in \\boxed{}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/queries.txt\"\n",
    "with open(query_file, \"r\") as file:\n",
    "    queries = file.readlines()\n",
    "print(len(queries))\n",
    "print(queries[174])\n",
    "print(queries[408])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6aee47-3e6a-442d-8627-3acd583c2005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n",
      "wrong_indices: [2, 8, 15, 39, 46, 61, 66, 73, 84, 89, 93, 107, 109, 126, 138, 147, 151, 161, 170, 191, 209, 210, 219, 255, 257, 284, 285, 290, 293, 316, 336, 338, 340, 343, 346, 353, 359, 363, 370, 377, 384, 386, 389, 392, 410, 413, 415, 422, 427, 440, 473, 480, 485, 493, 512, 518, 521, 532, 535, 539, 552, 556, 559, 589, 594, 605, 606, 634, 644, 667, 674, 687, 715, 744, 784, 794, 796, 823, 826, 839, 845, 847, 848, 856, 857, 869, 872, 880, 889, 896]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "answer_file = \"/projects/JHA/shared/dataset/mcq_filtered_by_match/answers.json\"\n",
    "with open(answer_file, \"r\") as file:\n",
    "    answers = json.load(file)\n",
    "\n",
    "true_answers = answers[100:174]+answers[175:408]+answers[409:]\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = 0\n",
    "    wrong_indices = []\n",
    "    \n",
    "    for idx, (true, pred) in enumerate(zip(y_true, y_pred)):\n",
    "        if true == pred:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong_indices.append(idx)\n",
    "    \n",
    "    accuracy = correct / len(y_true) if y_true else 0\n",
    "    return accuracy, wrong_indices\n",
    "\n",
    "accuracy, wrong_indices = accuracy_score(true_answers, processed_seed_responses)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"wrong_indices:\", wrong_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a78a5c5-883f-40d9-a3b1-2cfd6075e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff734875-2b5b-485d-80a9-e787fe97aa40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag [~/.conda/envs/graphrag/]",
   "language": "python",
   "name": "conda_graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
